{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQKrwc1N9yINH/PAWncnLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpandanaKalakonda/LLMS/blob/main/pdf_query_langchain_cassandra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Neccessary Libraries\n",
        "!pip install -q langchain_openai cassio datasets tiktoken langchain_community langchain PyPDF2"
      ],
      "metadata": {
        "id": "p4nUQIHFVXnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "54f624b8-5e5d-4a1b-c06c-63670dd5f4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlEm5YOBS0jw"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "from langchain_openai import OpenAI, OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from datasets import load_dataset\n",
        "from PyPDF2 import PdfReader\n",
        "import cassio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the secret keys\n",
        "from google.colab import userdata\n",
        "ASTRA_DB_APPLICATION_TOKEN = userdata.get('ASTRA_DB_APPLICATION_TOKEN')\n",
        "ASTRA_DB_ID = userdata.get('ASTRA_DB_ID')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "DJc_flAlj7lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a Pdfreader object to read the PDF file\n",
        "pdfreader = PdfReader(\"/content/[THE] A Course in Machine Learning (2013).pdf\")"
      ],
      "metadata": {
        "id": "eAsqvqSUrbXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the contents of the pdf file\n",
        "from typing_extensions import Concatenate\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  text = page.extract_text()\n",
        "  if text:\n",
        "    raw_text += text"
      ],
      "metadata": {
        "id": "IdDiV64ErjRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "LTdZwlxdr4Lt",
        "outputId": "3ac6c73d-89a1-4288-9bc0-e2977f501981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Course in\\nMachine Learning\\nHal Daumé IIICopyright © 2013 –2017 Hal Daumé III\\nSelf-published\\nhttp://ciml.info/\\nTODO. . . .\\nSecond printing, January 2017For my students and teachers.\\nOften the same.TABLE OF CONTENTS\\nAbout this Book 6\\n1 Decision Trees 8\\n2 Limits of Learning 19\\n3 Geometry and Nearest Neighbors 29\\n4 The Perceptron 41\\n5 Practical Issues 55\\n6 Beyond Binary Classification 73\\n7 Linear Models 87\\n8 Bias and Fairness 104\\n9 Probabilistic Modeling 116\\n10 Neural Networks 1295\\n11 Kernel Methods 141\\n12 Learning Theory 154\\n13 Ensemble Methods 164\\n14 Efficient Learning 171\\n15 Unsupervised Learning 178\\n16 Expectation Maximization 186\\n17 Structured Prediction 195\\n18 Imitation Learning 212\\nCode and Datasets 222\\nBibliography 223\\nIndex 225ABOUT THIS BOOK\\nMachine learning is a broad and fascinating field . Even\\ntoday, machine learning technology runs a substantial part of your\\nlife, often without you knowing it. Any plausible approach to artiﬁ-\\ncial intelligence must involve learning, at some level, if for no other\\nreason than it’s hard to call a system intelligent if it cannot learn.\\nMachine learning is also fascinating in its own right for the philo-\\nsophical questions it raises about what it means to learn and succeed\\nat tasks.\\nMachine learning is also a very broad ﬁeld, and attempting to\\ncover everything would be a pedagogical disaster. It is also so quickly\\nmoving that any book that attempts to cover the latest developments\\nwill be outdated before it gets online. Thus, this book has two goals.\\nFirst, to be a gentle introduction to what is a very deep ﬁeld. Second,\\nto provide readers with the skills necessary to pick up new technol-\\nogy as it is developed.\\n0.1How to Use this Book\\nThis book is designed to be read linearly, since it’s goal is not to be\\na generic reference. That said, once you get through chapter 5, you\\ncan pretty much jump anywhere. When I teach a one-semester un-\\ndergraduate course, I typically cover the chapter 1-13, sometimes\\nskipping 7or9or10or12depending on time and interest. For a\\ngraduate course for students with no prior machine learning back-\\nground, I would very quickly blaze through 1-4, then cover the rest,\\naugmented with some additional reading.\\n0.2Why Another Textbook?\\nThe purpose of this book is to provide a gentle and pedagogically orga-\\nnized introduction to the ﬁeld. This is in contrast to most existing ma-\\nchine learning texts, which tend to organize things topically, rather7\\nthan pedagogically (an exception is Mitchell’s book1, but unfortu-1Mitchell 1997\\nnately that is getting more and more outdated). This makes sense for\\nresearchers in the ﬁeld, but less sense for learners. A second goal of\\nthis book is to provide a view of machine learning that focuses on\\nideas and models, not on math. It is not possible (or even advisable)\\nto avoid math. But math should be there to aidunderstanding, not\\nhinder it. Finally, this book attempts to have minimal dependencies,\\nso that one can fairly easily pick and choose chapters to read. When\\ndependencies exist, they are listed at the start of the chapter.\\nThe audience of this book is anyone who knows differential calcu-\\nlus and discrete math, and can program reasonably well. (A little bit\\nof linear algebra and probability will not hurt.) An undergraduate in\\ntheir fourth or ﬁfth semester should be fully capable of understand-\\ning this material. However, it should also be suitable for ﬁrst year\\ngraduate students, perhaps at a slightly faster pace.\\n0.3Organization and Auxilary Material\\nThere is an associated web page, http://ciml.info/ , which contains\\nan online copy of this book, as well as associated code and data. It\\nalso contains errata. Please submit bug reports on github: github.com/\\nhal3/ciml .\\n0.4Acknowledgements\\nAcknowledgements: I am indebted to many people for this book.\\nMy teachers, especially Rami Grossberg (from whom the title of this\\nbook was borrowed) and Stefan Schaal. Students who have taken\\nmachine learning from me over the past ten years, including those\\nwho suffered through the initial versions of the class before I ﬁgured\\nout how to teach it. Especially Scott Alfeld, Josh de Bever, Cecily\\nHeiner, Jeffrey Ferraro, Seth Juarez, John Moeller, JT Olds, Piyush\\nRai. People who have helped me edit, and who have submitted bug\\nreports, including TODO. . . , but also check github for the latest list\\nof contributors!1 | D ECISION TREES\\nDependencies: None.At a basic level , machine learning is about predicting the future\\nbased on the past. For instance, you might wish to predict how much\\na user Alice will like a movie that she hasn’t seen, based on her rat-\\nings of movies that she has seen. This prediction could be based on\\nmany factors of the movies: their category (drama, documentary,\\netc.), the language, the director and actors, the production company,\\netc. In general, this means making informed guesses about some un-\\nobserved property of some object, based on observed properties of\\nthat object.\\nThe ﬁrst question we’ll ask is: what does it mean to learn? In\\norder to develop learning machines, we must know what learning\\nactually means, and how to determine success (or failure). You’ll see\\nthis question answered in a very limited learning setting, which will\\nbe progressively loosened and adapted throughout the rest of this\\nbook. For concreteness, our focus will be on a very simple model of\\nlearning called a decision tree.\\n1.1What Does it Mean to Learn?\\nAlice has just begun taking a course on machine learning. She knows\\nthat at the end of the course, she will be expected to have “learned”\\nall about this topic. A common way of gauging whether or not she\\nhas learned is for her teacher, Bob, to give her a exam. She has done\\nwell at learning if she does well on the exam.\\nBut what makes a reasonable exam? If Bob spends the entire\\nsemester talking about machine learning, and then gives Alice an\\nexam on History of Pottery, then Alice’s performance on this exam\\nwill notbe representative of her learning. On the other hand, if the\\nexam only asks questions that Bob has answered exactly during lec-\\ntures, then this is also a bad test of Alice’s learning, especially if it’s\\nan “open notes” exam. What is desired is that Alice observes speciﬁc\\nexamples from the course, and then has to answer new, but related\\nquestions on the exam. This tests whether Alice has the ability toLearning Objectives:\\n• Explain the difference between\\nmemorization and generalization.\\n• Implement a decision tree classiﬁer.\\n• Take a concrete task and cast it as a\\nlearning problem, with a formal no-\\ntion of input space, features, output\\nspace, generating distribution and\\nloss function.The words printed here are concepts.\\nY ou must go through the experiences. – Carl Frederickdecision trees 9\\ngeneralize. Generalization is perhaps the most central concept in\\nmachine learning.\\nAs a concrete example, consider a course recommendation system\\nfor undergraduate computer science students. We have a collection\\nof students and a collection of courses. Each student has taken, and\\nevaluated, a subset of the courses. The evaluation is simply a score\\nfrom\\x002 (terrible) to +2 (awesome). The job of the recommender\\nsystem is to predict how much a particular student (say, Alice) will\\nlike a particular course (say, Algorithms).\\nGiven historical data from course ratings (i.e., the past) we are\\ntrying to predict unseen ratings (i.e., the future). Now, we could\\nbe unfair to this system as well. We could ask it whether Alice is\\nlikely to enjoy the History of Pottery course. This is unfair because\\nthe system has no idea what History of Pottery even is, and has no\\nprior experience with this course. On the other hand, we could ask\\nit how much Alice will like Artiﬁcial Intelligence, which she took\\nlast year and rated as +2 (awesome). We would expect the system to\\npredict that she would really like it, but this isn’t demonstrating that\\nthe system has learned: it’s simply recalling its past experience. In\\nthe former case, we’re expecting the system to generalize beyond its\\nexperience, which is unfair. In the latter case, we’re not expecting it\\nto generalize at all.\\nThis general set up of predicting the future based on the past is\\nat the core of most machine learning. The objects that our algorithm\\nwill make predictions about are examples . In the recommender sys-\\ntem setting, an example would be some particular Student/Course\\npair (such as Alice/Algorithms). The desired prediction would be the\\nrating that Alice would give to Algorithms.\\nknown labelstraining\\ndatalearning\\nalgorithm\\nf ?test\\nexample\\npredicted\\nlabel\\nFigure 1.1: The general supervised ap-\\nproach to machine learning: a learning\\nalgorithm reads in training data and\\ncomputes a learned function f. This\\nfunction can then automatically label\\nfuture text examples.To make this concrete, Figure 1.1shows the general framework of\\ninduction. We are given train ingdata on which our algorithm is ex-\\npected to learn. This training data is the examples that Alice observes\\nin her machine learning course, or the historical ratings data for\\nthe recommender system. Based on this training data, our learning\\nalgorithm induces a function fthat will map a new example to a cor-\\nresponding prediction. For example, our function might guess that\\nf(Alice/Machine Learning )might be high because our training data\\nsaid that Alice liked Artiﬁcial Intelligence. We want our algorithm\\nto be able to make lots of predictions, so we refer to the collection\\nof examples on which we will evaluate our algorithm as the test set.\\nThe test set is a closely guarded secret: it is the ﬁnal exam on which\\nour learning algorithm is being tested. If our algorithm gets to peek\\nat it ahead of time, it’s going to cheat and do better than it should. Why is it bad if the learning algo-\\nrithm gets to peek at the test data??The goal of inductive machine learning is to take some training\\ndata and use it to induce a function f. This function fwill be evalu-10 a course in machine learning\\nated on the test data. The machine learning algorithm has succeeded\\nif its performance on the test data is high.\\n1.2Some Canonical Learning Problems\\nThere are a large number of typical inductive learning problems.\\nThe primary difference between them is in what type of thing they’re\\ntrying to predict. Here are some examples:\\nRegression: trying to predict a real value. For instance, predict the\\nvalue of a stock tomorrow given its past performance. Or predict\\nAlice’s score on the machine learning ﬁnal exam based on her\\nhomework scores.\\nBinary Classiﬁcation: trying to predict a simple yes/no response.\\nFor instance, predict whether Alice will enjoy a course or not.\\nOr predict whether a user review of the newest Apple product is\\npositive or negative about the product.\\nMulticlass Classiﬁcation: trying to put an example into one of a num-\\nber of classes. For instance, predict whether a news story is about\\nentertainment, sports, politics, religion, etc. Or predict whether a\\nCS course is Systems, Theory, AI or Other.\\nRanking: trying to put a set of objects in order of relevance. For in-\\nstance, predicting what order to put web pages in, in response to a\\nuser query. Or predict Alice’s ranked preferences over courses she\\nhasn’t taken.\\nFor each of these types of canon-\\nical machine learning problems,\\ncome up with one or two concrete\\nexamples.?The reason that it is convenient to break machine learning prob-\\nlems down by the type of object that they’re trying to predict has to\\ndo with measuring error. Recall that our goal is to build a system\\nthat can make “good predictions.” This begs the question: what does\\nit mean for a prediction to be “good?” The different types of learning\\nproblems differ in how they deﬁne goodness. For instance, in regres-\\nsion, predicting a stock price that is off by $0.05 is perhaps much\\nbetter than being off by $200.00. The same does not hold of multi-\\nclass classiﬁcation. There, accidentally predicting “entertainment”\\ninstead of “sports” is no better or worse than predicting “politics.”\\n1.3The Decision Tree Model of Learning\\nThe decision tree is a classic and natural model of learning. It is\\nclosely related to the fundamental computer science notion of “di-\\nvide and conquer.” Although decision trees can be applied to manydecision trees 11\\nlearning problems, we will begin with the simplest case: binary clas-\\nsiﬁcation.\\nSuppose that your goal is to predict whether some unknown user\\nwill enjoy some unknown course. You must simply answer “yes” or\\n“no.” In order to make a guess, you’re allowed to ask binary ques-\\ntions about the user/course under consideration. For example:\\nYou: Is the course under consideration in Systems?\\nMe: Yes\\nYou: Has this student taken any other Systems courses?\\nMe: Yes\\nYou: Has this student liked most previous Systems courses?\\nMe: No\\nYou: I predict this student will not like this course.\\nThe goal in learning is to ﬁgure out what questions to ask, in what\\norder to ask them, and what answer to predict once you have asked\\nenough questions.\\nisSystems?\\ntakenOtherSys?\\nmorning? likedOtherSys?\\nlike nah nah likelikeno\\nno\\nno noyes\\nyes\\nyes yesFigure 1.2: A decision tree for a course\\nrecommender system, from which the\\nin-text “dialog” is drawn.\\nThe decision tree is so-called because we can write our set of ques-\\ntions and guesses in a tree format, such as that in Figure 1.2. In this\\nﬁgure, the questions are written in the internal tree nodes (rectangles)\\nand the guesses are written in the leaves (ovals). Each non-terminal\\nnode has two children: the left child speciﬁes what to do if the an-\\nswer to the question is “no” and the right child speciﬁes what to do if\\nit is “yes.”\\nIn order to learn, I will give you training data. This data consists\\nof a set of user/course examples, paired with the correct answer for\\nthese examples (did the given user enjoy the given course?). From\\nthis, you must construct your questions. For concreteness, there is a\\nsmall data set in Table 1in the Appendix of this book. This training\\ndata consists of 20course rating examples, with course ratings and\\nanswers to questions that you might ask about this pair. We will\\ninterpret ratings of 0, +1 and +2 as “liked” and ratings of \\x002 and\\x001\\nas “hated.”\\nIn what follows, we will refer to the questions that you can ask as\\nfeatures and the responses to these questions as feature values. The\\nrating is called the label. An example is just a set of feature values.\\nAnd our training data is a set of examples, paired with labels.\\nThere are a lot of logically possible trees that you could build,\\neven over just this small number of features (the number is in the\\nmillions). It is computationally infeasible to consider all of these to\\ntry to choose the “best” one. Instead, we will build our decision tree\\ngreedily. We will begin by asking:\\nIf I could only ask one question, what question would I ask?\\noverall:\\neasy:\\nAI:\\nsystems:\\ntheory:60%\\n40%like\\nnah\\n60%\\n40%\\n60%\\n40%yes\\nno\\n82%\\n18%\\n33%\\n67%yes\\nno\\n20%\\n80%\\n100%\\n0%yes\\nno\\n80%\\n20%\\n40%\\n60%yes\\nno\\nFigure 1.3: A histogram of labels for (a)\\nthe entire data set; (b-e) the examples\\nin the data set for each value of the ﬁrst\\nfour features.You want to ﬁnd a feature that is most useful in helping you guess\\nwhether this student will enjoy this course. A useful way to think12 a course in machine learning\\nabout this is to look at the histogram of labels for each feature.1 1A colleague related the story of\\ngetting his 8-year old nephew to\\nguess a number between 1and100.\\nHis nephew’s ﬁrst four questions\\nwere: Is it bigger than 20? (YES) Is\\nit even? (YES) Does it have a 7in it?\\n(NO) Is it 80? (NO). It took 20more\\nquestions to get it, even though 10\\nshould have been sufﬁcient. At 8,\\nthe nephew hadn’t quite ﬁgured out\\nhow to divide and conquer. http:\\n//blog.computationalcomplexity.\\norg/2007/04/\\ngetting-8-year-old-interested-in.\\nhtml .This is shown for the ﬁrst four features in Figure 1.3. Each histogram\\nshows the frequency of “like”/“hate” labels for each possible value\\nof an associated feature. From this ﬁgure, you can see that asking\\nthe ﬁrst feature is not useful: if the value is “no” then it’s hard to\\nguess the label; similarly if the answer is “yes.” On the other hand,\\nasking the second feature isuseful: if the value is “no,” you can be\\npretty conﬁdent that this student will hate this course; if the answer\\nis “yes,” you can be pretty conﬁdent that this student will like this\\ncourse.\\nMore formally, you will consider each feature in turn. You might\\nconsider the feature “Is this a System’s course?” This feature has two\\npossible value: no and yes. Some of the training examples have an\\nanswer of “no” – let’s call that the “NO” set. Some of the training\\nexamples have an answer of “yes” – let’s call that the “YES” set. For\\neach set (NO and YES) we will build a histogram over the labels.\\nThis is the second histogram in Figure 1.3. Now, suppose you were\\nto ask this question on a random example and observe a value of\\n“no.” Further suppose that you must immediately guess the label for\\nthis example. You will guess “like,” because that’s the more preva-\\nlent label in the NO set (actually, it’s the only label in the NO set).\\nAlternatively, if you receive an answer of “yes,” you will guess “hate”\\nbecause that is more prevalent in the YES set.\\nSo, for this single feature, you know what you would guess if you\\nhad to. Now you can ask yourself: if I made that guess on the train-\\ning data, how well would I have done? In particular, how many ex-\\namples would I classify correctly? In the NO set (where you guessed\\n“like”) you would classify all 10 of them correctly. In the YES set\\n(where you guessed “hate”) you would classify 8 (out of 10) of them\\ncorrectly. So overall you would classify 18 (out of 20) correctly. Thus,\\nwe’ll say that the score of the “Is this a System’s course?” question is\\n18/20. How many training examples\\nwould you classify correctly for\\neach of the other three features\\nfrom Figure 1.3??You will then repeat this computation for each of the available\\nfeatures to us, compute the scores for each of them. When you must\\nchoose which feature consider ﬁrst, you will want to choose the one\\nwith the highest score.\\nBut this only lets you choose the ﬁrst feature to ask about. This\\nis the feature that goes at the root of the decision tree. How do we\\nchoose subsequent features? This is where the notion of divide and\\nconquer comes in. You’ve already decided on your ﬁrst feature: “Is\\nthis a Systems course?” You can now partition the data into two parts:\\nthe NO part and the YES part. The NO part is the subset of the data\\non which value for this feature is “no”; the YES half is the rest. This\\nis the divide step.decision trees 13\\nAlgorithm 1Decision TreeTrain (data,remaining features )\\n1:guess most frequent answer in data // default answer for this data\\n2:ifthe labels in data are unambiguous then\\n3:return Leaf(guess ) // base case: no need to split further\\n4:else if remaining features is empty then\\n5:return Leaf(guess ) // base case: cannot split further\\n6:else // we need to query more features\\n7:for all f2remaining features do\\n8: NO the subset of data on which f=no\\n9: YES the subset of data on which f=yes\\n10: score [f] # of majority vote answers in NO\\n11: + # of majority vote answers in YES\\n// the accuracy we would get if we only queried on f\\n12:end for\\n13:f the feature with maximal score (f)\\n14:NO the subset of data on which f=no\\n15:YES the subset of data on which f=yes\\n16:left Decision TreeTrain (NO,remaining features nffg)\\n17:right Decision TreeTrain (YES,remaining features nffg)\\n18:return Node(f,left,right )\\n19:end if\\nAlgorithm 2Decision TreeTest(tree,test point )\\n1:iftreeis of the form L eaf(guess )then\\n2:return guess\\n3:else if treeis of the form N ode(f,left,right )then\\n4:iff=nointest point then\\n5: return Decision TreeTest(left,test point )\\n6:else\\n7: return Decision TreeTest(right ,test point )\\n8:end if\\n9:end if\\nThe conquer step is to recurse, and run the same routine (choosing\\nthe feature with the highest score) on the NO set (to get the left half\\nof the tree) and then separately on the YES set (to get the right half of\\nthe tree).\\nAt some point it will become useless to query on additional fea-\\ntures. For instance, once you know that this is a Systems course,\\nyou know that everyone will hate it. So you can immediately predict\\n“hate” without asking any additional questions. Similarly, at some\\npoint you might have already queried every available feature and still\\nnot whittled down to a single answer. In both cases, you will need to\\ncreate a leaf node and guess the most prevalent answer in the current\\npiece of the training data that you are looking at.\\nPutting this all together, we arrive at the algorithm shown in Al-\\ngorithm 1.3.2This function, D ecision TreeTrain takes two argu-2There are more nuanced algorithms\\nfor building decision trees, some of\\nwhich are discussed in later chapters of\\nthis book. They primarily differ in how\\nthey compute the score function.14 a course in machine learning\\nments: our data, and the set of as-yet unused features. It has two\\nbase cases: either the data is unambiguous, or there are no remaining\\nfeatures. In either case, it returns a L eaf node containing the most\\nlikely guess at this point. Otherwise, it loops over all remaining fea-\\ntures to ﬁnd the one with the highest score. It then partitions the data\\ninto a NO/YES split based on the best feature. It constructs its left\\nand right subtrees by recursing on itself. In each recursive call, it uses\\none of the partitions of the data, and removes the just-selected feature\\nfrom consideration. Is Algorithm 1.3guaranteed to\\nterminate??The corresponding prediction algorithm is shown in Algorithm 1.3.\\nThis function recurses down the decision tree, following the edges\\nspeciﬁed by the feature values in some test point . When it reaches a\\nleaf, it returns the guess associated with that leaf.\\n1.4Formalizing the Learning Problem\\nAs you’ve seen, there are several issues that we must take into ac-\\ncount when formalizing the notion of learning.\\n• The performance of the learning algorithm should be measured on\\nunseen “test” data.\\n• The way in which we measure performance should depend on the\\nproblem we are trying to solve.\\n• There should be a strong relationship between the data that our\\nalgorithm sees at training time and the data it sees at test time.\\nIn order to accomplish this, let’s assume that someone gives us a\\nloss func tion,`(\\x01,\\x01), of two arguments. The job of `is to tell us how\\n“bad” a system’s prediction is in comparison to the truth. In particu-\\nlar, if yis the truth and ˆyis the system’s prediction, then `(y,ˆy)is a\\nmeasure of error.\\nFor three of the canonical tasks discussed above, we might use the\\nfollowing loss functions:\\nRegression: squared loss`(y,ˆy) = ( y\\x00ˆy)2\\norabsolute loss`(y,ˆy) =jy\\x00ˆyj.\\nBinary Classiﬁcation: zero/one loss`(y,ˆy) =(\\n0 if y=ˆy\\n1 otherwiseThis notation means that the loss is zero\\nif the prediction is correct and is one\\notherwise.\\nMulticlass Classiﬁcation: also zero/one loss.\\nWhy might it be a bad idea to use\\nzero/one loss to measure perfor-\\nmance for a regression problem??Note that the loss function is something that youmust decide on\\nbased on the goals of learning.\\nNow that we have deﬁned our loss function, we need to consider\\nwhere the data (training andtest) comes from. The model that wedecision trees 15\\nWe write E(x,y)\\x18D[`(y,f(x))]for the expected loss. Expectation means “average.” This is saying “if you\\ndrew a bunch of (x,y)pairs independently at random from D, what would your average loss be?More\\nformally, ifDis a discrete probability distribution, then this expectation can be expanded as:\\nE(x,y)\\x18D[`(y,f(x))] =å\\n(x,y)2D[D(x,y)`(y,f(x))] (1.1)\\nThis is exactly the weighted average loss over the all (x,y)pairs inD, weighted by their probability,\\nD(x,y). IfDis aﬁnite discrete distribution , for instance deﬁned by a ﬁnite data set f(x1,y1), . . . ,(xN,yN)\\nthat puts equal weight on each example (probability 1/ N), then we get:\\nE(x,y)\\x18D[`(y,f(x))] =å\\n(x,y)2D[D(x,y)`(y,f(x))] deﬁnition of expectation (1.2)\\n=N\\nå\\nn=1[D(xn,yn)`(yn,f(xn))] Dis discrete and ﬁnite (1.3)\\n=N\\nå\\nn=1[1\\nN`(yn,f(xn))] deﬁnition of D (1.4)\\n=1\\nNN\\nå\\nn=1[`(yn,f(xn))] rearranging terms (1.5)\\nWhich is exactly the average loss on that dataset.\\nThe most important thing to remember is that there are two equivalent ways to think about expections:\\n(1) The expectation of some function gis the weighted average value of g , where the weights are given by\\nthe underlying probability distribution. ( 2) The expectation of some function gis your best guess of the\\nvalue of g if you were to draw a single item from the underlying probability distribution.MATHREVIEW | EXPECTATED VALUES\\nFigure 1.4:\\nwill use is the probabilistic model of learning. Namely, there is a prob-\\nability distribution Dover input/output pairs. This is often called\\nthedata generatingdistribution. If we write xfor the input (the\\nuser/course pair) and yfor the output (the rating), then Dis a distri-\\nbution over (x,y)pairs.\\nA useful way to think about Dis that it gives high probability to\\nreasonable (x,y)pairs, and low probability to unreasonable (x,y)\\npairs. A (x,y)pair can be unreasonable in two ways. First, xmight\\nbe an unusual input. For example, a xrelated to an “Intro to Java”\\ncourse might be highly probable; a xrelated to a “Geometric and\\nSolid Modeling” course might be less probable. Second, ymight\\nbe an unusual rating for the paired x. For instance, if Alice were to\\ntake AI 100 times (without remembering that she took it before!),\\nshe would give the course a +2 almost every time. Perhaps some16 a course in machine learning\\nsemesters she might give a slightly lower score, but it would be un-\\nlikely to see x=Alice/AI paired with y=\\x002.\\nIt is important to remember that we are not making anyassump-\\ntions about what the distribution Dlooks like. (For instance, we’re\\nnot assuming it looks like a Gaussian or some other, common distri-\\nbution.) We are also not assuming that we know what Dis. In fact,\\nif you know a priori what your data generating distribution is, your\\nlearning problem becomes signiﬁcantly easier. Perhaps the hardest\\nthing about machine learning is that we don’t know whatDis: all we\\nget is a random sample from it. This random sample is our training\\ndata.\\nOur learning problem, then, is deﬁned by two quantities: Consider the following prediction\\ntask. Given a paragraph written\\nabout a course, we have to predict\\nwhether the paragraph is a positive\\nornegative review of the course.\\n(This is the sentiment analysis prob-\\nlem.) What is a reasonable loss\\nfunction? How would you deﬁne\\nthe data generating distribution??1. The loss function `, which captures our notion of what is important\\nto learn.\\n2. The data generating distribution D, which deﬁnes what sort of\\ndata we expect to see.\\nWe are given access to train ingdata , which is a random sample of\\ninput/output pairs drawn from D. Based on this training data, we\\nneed to induce a function fthat maps new inputs ˆ xto corresponding\\nprediction ˆy. The key property that fshould obey is that it should do\\nwell (as measured by `) on future examples that are alsodrawn from\\nD. Formally, it’s expected loss eoverDwith repsect to `should be\\nas small as possible:\\ne,E(x,y)\\x18D\\x02`(y,f(x))\\x03=å\\n(x,y)D(x,y)`(y,f(x)) (1.6)\\nThe difﬁculty in minimizing our expected loss from Eq ( 1.6) is\\nthat we don’t know whatDis!All we have access to is some training\\ndata sampled from it! Suppose that we denote our training data\\nset by D. The training data consists of N-many input/output pairs,\\n(x1,y1),(x2,y2), . . . ,(xN,yN). Given a learned function f, we can\\ncompute our train ingerror,ˆe:\\nˆe,1\\nNN\\nå\\nn=1`(yn,f(xn)) (1.7)\\nThat is, our training error is simply our average error over the train-\\ning data. Verify by calculation that we\\ncan write our training error as\\nE(x,y)\\x18D\\x02`(y,f(x))\\x03\\n, by thinking\\nofDas a distribution that places\\nprobability 1/ Nto each example in\\nDand probability 0 on everything\\nelse.?Of course, we can drive ˆeto zero by simply memorizing our train-\\ning data. But as Alice might ﬁnd in memorizing past exams, this\\nmight not generalize well to a new exam!\\nThis is the fundamental difﬁculty in machine learning: the thing\\nwe have access to is our training error, ˆe. But the thing we care aboutdecision trees 17\\nminimizing is our expected error e. In order to get the expected error\\ndown, our learned function needs to generalizebeyond the training\\ndata to some future data that it might not have seen yet!\\nSo, putting it all together, we get a formal deﬁnition of induction\\nmachine learning: Given (i) a loss function `and (ii) a sample D\\nfrom some unknown distribution D, you must compute a function\\nfthat has low expected error eoverDwith respect to `.\\nA very important comment is that we should never expect a ma-\\nchine learning algorithm to generalize beyond the data distribution\\nit has seen at training time. In a famous—if posssibly apocryphal—\\nexample from the 1970 s, the US Government wanted to train a clas-\\nsiﬁer to distinguish between US tanks and Russian tanks. They col-\\nlected a training and test set, and managed to build a classiﬁer with\\nnearly 100% accuracy on that data. But when this classiﬁer was run\\nin the “real world”, it failed miserably. It had not, in fact, learned\\nto distinguish between US tanks and Russian tanks, but rather just\\nbetween clear photos and blurry photos. In this case, there was a bias\\nin the training data (due to how the training data was collected) that\\ncaused the learning algorithm to learn something other than what we\\nwere hoping for. We will return to this issue in Chapter 8; for now,\\nsimply remember that the distribution Dfor training data must match\\nthe distribution Dfor the test data.\\n1.5Chapter Summary and Outlook\\nAt this point, you should be able to use decision trees to do machine\\nlearning. Someone will give you data. You’ll split it into training,\\ndevelopment and test portions. Using the training and development\\ndata, you’ll ﬁnd a good value for maximum depth that trades off\\nbetween underﬁtting and overﬁtting. You’ll then run the resulting\\ndecision tree model on the test data to get an estimate of how well\\nyou are likely to do in the future.\\nYou might think: why should I read the rest of this book? Aside\\nfrom the fact that machine learning is just an awesome fun ﬁeld to\\nlearn about, there’s a lot left to cover. In the next two chapters, you’ll\\nlearn about two models that have very different inductive biases than\\ndecision trees. You’ll also get to see a very useful way of thinking\\nabout learning: the geometric view of data. This will guide much of\\nwhat follows. After that, you’ll learn how to solve problems more\\ncomplicated that simple binary classiﬁcation. (Machine learning\\npeople like binary classiﬁcation a lot because it’s one of the simplest\\nnon-trivial problems that we can work on.) After that, things will\\ndiverge: you’ll learn about ways to think about learning as a formal\\noptimization problem, ways to speed up learning, ways to learn18 a course in machine learning\\nwithout labeled data (or with very little labeled data) and all sorts of\\nother fun topics.\\nBut throughout, we will focus on the view of machine learning\\nthat you’ve seen here. You select a model (and its associated induc-\\ntive biases). You use data to ﬁnd parameters of that model that work\\nwell on the training data. You use development data to avoid under-\\nﬁtting and overﬁtting. And you use test data (which you’ll never look\\nat or touch, right?) to estimate future model performance. Then you\\nconquer the world.\\n1.6Further Reading\\nIn our discussion of decision trees, we used misclassiﬁcation rate for\\nselecting features. While simple and intuitive, misclassiﬁcation rate\\nhas problems. There has been a signiﬁcant amount of work that\\nconsiders more advanced splitting criteria; the most popular is ID 3,\\nbased on the mutual information quantity from information the-\\nory. We have also only considered a very simple mechanism for\\ncontrolling inductive bias: limiting the depth of the decision tree.\\nAgain, there are more advanced “tree pruning” techniques that typ-\\nically operate by growing deep trees and then pruning back some\\nof the branches. These approaches have the advantage that differ-\\nent branches can have different depths, accounting for the fact that\\nthe amount of data that gets passed down each branch might differ\\ndramatically3.3Quinlan 19862 | L IMITS OF LEARNING\\nDependencies: None.Machine learning is a very general and useful framework,\\nbut it is not “magic” and will not always work. In order to better\\nunderstand when it will and when it will not work, it is useful to\\nformalize the learning problem more. This will also help us develop\\ndebugging strategies for learning algorithms.\\n2.1Data Generating Distributions\\nOur underlying assumption for the majority of this book is that\\nlearning problems are characterized by some unknown probability\\ndistributionDover input/output pairs (x,y)2X\\x02Y . Suppose that\\nsomeone toldyou whatDwas. In particular, they gave you a Python\\nfunction compute D that took two inputs, xand y, and returned the\\nprobability of that x,ypair underD. If you had access to such a func-\\ntion, classiﬁcation becomes simple. We can deﬁne the Bayes optimal\\nclassiﬁeras the classiﬁer that, for any test input ˆx, simply returns the\\nˆythat maximizes compute D(ˆx,ˆy), or, more formally:\\nf(BO)(ˆx) =arg max\\nˆy2YD(ˆx,ˆy) (2.1)\\nThis classiﬁer is optimal in one speciﬁc sense: of all possible classiﬁers,\\nit achieves the smallest zero/one error.\\nTheorem 1(Bayes Optimal Classiﬁer) .The Bayes Optimal Classiﬁer\\nf(BO)achieves minimal zero/one error of any deterministic classiﬁer.\\nThis theorem assumes that you are comparing against deterministic\\nclassiﬁers. You can actually prove a stronger result that f(BO)is opti-\\nmal for randomized classiﬁers as well, but the proof is a bit messier.\\nHowever, the intuition is the same: for a given x,f(BO)chooses the\\nlabel with highest probability, thus minimizing the probability that it\\nmakes an error.\\nProof of Theorem 1.Consider some other classiﬁer gthat claims to\\nbe better than f(BO). Then, there must be some xon which g(x)6=Learning Objectives:\\n• Deﬁne “inductive bias” and recog-\\nnize the role of inductive bias in\\nlearning.\\n• Illustrate how regularization trades\\noff between underﬁtting and overﬁt-\\nting.\\n• Evaluate whether a use of test data\\nis “cheating” or not.Our lives sometimes depend on computers performing as pre-\\ndicted. – Philip Emeagwali20 a course in machine learning\\nf(BO)(x). Fix such an x. Now, the probability that f(BO)makes an error\\non this particular xis 1\\x00D(x,f(BO)(x))and the probability that g\\nmakes an error on this xis 1\\x00D(x,g(x)). But f(BO)was chosen in\\nsuch a way to maximizeD(x,f(BO)(x)), so this must be greater than\\nD(x,g(x)). Thus, the probability that f(BO)errs on this particular xis\\nsmaller than the probability that gerrs on it. This applies to any xfor\\nwhich f(BO)(x)6=g(x)and therefore f(BO)achieves smaller zero/one\\nerror than any g.\\nThe Bayes errorrate (orBayes optimal errorrate) is the error\\nrate of the Bayes optimal classiﬁer. It is the best error rate you can\\never hope to achieve on this classiﬁcation problem (under zero/one\\nloss). The take-home message is that if someone gave you access to\\nthe data distribution, forming an optimal classiﬁer would be trivial.\\nUnfortunately, no one gave you this distribution, so we need to ﬁgure\\nout ways of learning the mapping from xtoygiven only access to a\\ntraining set sampled fromD, rather thanDitself.\\n2.2Inductive Bias: What We Know Before the Data Arrives\\nclass A\\nclass B\\nFigure 2.1: Training data for a binary\\nclassiﬁcation problem.\\nFigure 2.2: Test data for the same\\nclassiﬁcation problem.In Figure 2.1you’ll ﬁnd training data for a binary classiﬁcation prob-\\nlem. The two labels are “A” and “B” and you can see four examples\\nfor each label. Below, in Figure 2.2, you will see some test data. These\\nimages are left unlabeled. Go through quickly and, based on the\\ntraining data, label these images. (Really do it before you read fur-\\nther! I’ll wait!)\\nMost likely you produced one of two labelings: either ABBA or\\nAABB. Which of these solutions is right? The answer is that you can-\\nnot tell based on the training data. If you give this same example\\nto 100 people, 60\\x0070 of them come up with the ABBA prediction\\nand 30\\x0040 come up with the AABB prediction. Why? Presumably\\nbecause the ﬁrst group believes that the relevant distinction is be-\\ntween “bird” and “non-bird” while the second group believes that\\nthe relevant distinction is between “ﬂy” and “no-ﬂy.”\\nThis preference for one distinction (bird/non-bird) over another\\n(ﬂy/no-ﬂy) is a bias that different human learners have. In the con-\\ntext of machine learning, it is called inductive bias : in the absense of\\ndata that narrow down the relevant concept, what type of solutions\\nare we more likely to prefer? Two thirds of people seem to have an\\ninductive bias in favor of bird/non-bird, and one third seem to have\\nan inductive bias in favor of ﬂy/no-ﬂy.\\nIt is also possible that the correct\\nclassiﬁcation on the test data is\\nABAB. This corresponds to the bias\\n“is the background in focus.” Some-\\nhow no one seems to come up with\\nthis classiﬁcation rule.?Throughout this book you will learn about several approaches to\\nmachine learning. The decision tree model is the ﬁrst such approach.\\nThese approaches differ primarily in the sort of inductive bias thatlimits of learning 21\\nthey exhibit.\\nConsider a variant of the decision tree learning algorithm. In this\\nvariant, we will not allow the trees to grow beyond some pre-deﬁned\\nmaximum depth, d. That is, once we have queried on d-many fea-\\ntures, we cannot query on any more and must just make the best\\nguess we can at that point. This variant is called a shal low decision\\ntree.\\nThe key question is: What is the inductive bias of shallow decision\\ntrees? Roughly, their bias is that decisions can be made by only look-\\ning at a small number of features. For instance, a shallow decision\\ntree would be very good at learning a function like “students only\\nlike AI courses.” It would be very bad at learning a function like “if\\nthis student has liked an odd number of their past courses, they will\\nlike the next one; otherwise they will not.” This latter is the parity\\nfunction, which requires you to inspect every feature to make a pre-\\ndiction. The inductive bias of a decision tree is that the sorts of things\\nwe want to learn to predict are more like the ﬁrst example and less\\nlike the second example.\\n2.3Not Everything is Learnable\\nAlthough machine learning works well—perhaps astonishingly\\nwell—in many cases, it is important to keep in mind that it is not\\nmagical. There are many reasons why a machine learning algorithm\\nmight fail on some learning task.\\nThere could be noise in the training data. Noise can occur both\\nat the feature level and at the label level. Some features might corre-\\nspond to measurements taken by sensors. For instance, a robot might\\nuse a laser range ﬁnder to compute its distance to a wall. However,\\nthis sensor might fail and return an incorrect value. In a sentiment\\nclassiﬁcation problem, someone might have a typo in their review of\\na course. These would lead to noise at the feature level. There might\\nalso be noise at the label level. A student might write a scathingly\\nnegative review of a course, but then accidentally click the wrong\\nbutton for the course rating.\\nThe features available for learning might simply be insufﬁcient.\\nFor example, in a medical context, you might wish to diagnose\\nwhether a patient has cancer or not. You may be able to collect a\\nlarge amount of data about this patient, such as gene expressions,\\nX-rays, family histories, etc. But, even knowing all of this information\\nexactly, it might still be impossible to judge for sure whether this pa-\\ntient has cancer or not. As a more contrived example, you might try\\nto classify course reviews as positive or negative. But you may have\\nerred when downloading the data and only gotten the ﬁrst ﬁve char-22 a course in machine learning\\nacters of each review. If you had the rest of the features you might\\nbe able to do well. But with this limited feature set, there’s not much\\nyou can do.\\nSome examples may not have a single correct answer. You might\\nbe building a system for “safe web search,” which removes offen-\\nsive web pages from search results. To build this system, you would\\ncollect a set of web pages and ask people to classify them as “offen-\\nsive” or not. However, what one person considers offensive might be\\ncompletely reasonable for another person. It is common to consider\\nthis as a form of label noise. Nevertheless, since you, as the designer\\nof the learning system, have some control over this problem, it is\\nsometimes helpful to isolate it as a source of difﬁculty.\\nFinally, learning might fail because the inductive bias of the learn-\\ning algorithm is too far away from the concept that is being learned.\\nIn the bird/non-bird data, you might think that if you had gotten\\na few more training examples, you might have been able to tell\\nwhether this was intended to be a bird/non-bird classiﬁcation or a\\nﬂy/no-ﬂy classiﬁcation. However, no one I’ve talked to has ever come\\nup with the “background is in focus” classiﬁcation. Even with many\\nmore training points, this is such an unusual distinction that it may\\nbe hard for anyone to ﬁgure out it. In this case, the inductive bias of\\nthe learner is simply too misaligned with the target classiﬁcation to\\nlearn.\\nNote that the inductive bias source of error is fundamentally dif-\\nferent than the other three sources of error. In the inductive bias case,\\nit is the particular learning algorithm that you are using that cannot\\ncope with the data. Maybe if you switched to a different learning\\nalgorithm, you would be able to learn well. For instance, Neptunians\\nmight have evolved to care greatly about whether backgrounds are\\nin focus, and for them this would be an easy classiﬁcation to learn.\\nFor the other three sources of error, it is not an issue to do with the\\nparticular learning algorithm. The error is a fundamental part of the\\nlearning problem.\\n2.4Underﬁtting and Overﬁtting\\nAs with many problems, it is useful to think about the extreme cases\\nof learning algorithms. In particular, the extreme cases of decision\\ntrees. In one extreme, the tree is “empty” and we do not ask any\\nquestions at all. We simply immediately make a prediction. In the\\nother extreme, the tree is “full.” That is, every possible question\\nis asked along every branch. In the full tree, there may be leaves\\nwith no associated training data. For these we must simply choose\\narbitrarily whether to say “yes” or “no.”limits of learning 23\\nConsider the course recommendation data from Table 1. Sup-\\npose we were to build an “empty” decision tree on this data. Such a\\ndecision tree will make the same prediction regardless of its input,\\nbecause it is not allowed to ask any questions about its input. Since\\nthere are more “likes” than “hates” in the training data (12 versus\\n8), our empty decision tree will simply always predict “likes.” The\\ntraining error, ˆe, is 8/20 =40%.\\nOn the other hand, we could build a “full” decision tree. Since\\neach row in this data is unique, we can guarantee that any leaf in a\\nfull decision tree will have either 0 or 1 examples assigned to it (20\\nof the leaves will have one example; the rest will have none). For the\\nleaves corresponding to training points, the full decision tree will\\nalways make the correct prediction. Given this, the training error, ˆe, is\\n0/20 =0%.\\nOf course our goal is notto build a model that gets 0% error on\\nthe training data. This would be easy! Our goal is a model that will\\ndo well on future, unseen data. How well might we expect these two\\nmodels to do on future data? The “empty” tree is likely to do not\\nmuch better and not much worse on future data. We might expect\\nthat it would continue to get around 40% error.\\nLife is more complicated for the “full” decision tree. Certainly\\nif it is given a test example that is identical to one of the training\\nexamples, it will do the right thing (assuming no noise). But for\\neverything else, it will only get about 50% error. This means that\\neven if every other test point happens to be identical to one of the\\ntraining points, it would only get about 25% error. In practice, this is\\nprobably optimistic, and maybe only one in every 10 examples would\\nmatch a training example, yielding a 35% error. Convince yourself (either by proof\\nor by simulation) that even in the\\ncase of imbalanced data – for in-\\nstance data that is on average 80%\\npositive and 20% negative – a pre-\\ndictor that guesses randomly ( 50/50\\npositive/negative) will get about\\n50% error.?So, in one case (empty tree) we’ve achieved about 40% error and\\nin the other case (full tree) we’ve achieved 35% error. This is not\\nvery promising! One would hope to do better! In fact, you might\\nnotice that if you simply queried on a single feature for this data, you\\nwould be able to get very low training error, but wouldn’t be forced\\nto “guess” randomly.\\nWhich feature is it, and what is it’s\\ntraining error??This example illustrates the key concepts of underﬁtting and\\nover ﬁtting. Underﬁtting is when you had the opportunity to learn\\nsomething but didn’t. A student who hasn’t studied much for an up-\\ncoming exam will be underﬁt to the exam, and consequently will not\\ndo well. This is also what the empty tree does. Overﬁtting is when\\nyou pay too much attention to idiosyncracies of the training data,\\nand aren’t able to generalize well. Often this means that your model\\nis ﬁtting noise, rather than whatever it is supposed to ﬁt. A student\\nwho memorizes answers to past exam questions without understand-\\ning them has overﬁt the training data. Like the full tree, this student24 a course in machine learning\\nConsider some random event, like spins of a roulette wheel, cars driving through an intersection, the\\noutcome of an election, or pasta being appropriately al dente. We often want to make a conclusion\\nabout the entire population (the pot of pasta) based on a much smaller sample (biting a couple pieces\\nof pasta). The law of large numbers tells us that under mild conditions this is an okay thing to do.\\nFormally, suppose that v1,v2, . . . , vNare random variables (e.g., vnmeasures if the nth spaghetti is\\nal dente ). Assume that these random variables are independent (i.e., v2and v3are uncorrelated—\\nthey weren’t both taken from the same place in the pot) and iden tically distributed (they were all\\ndrawn from the same population—pot—that we wish to measure). We can compute the sample av-\\nerage ¯v=1\\nNåN\\nn=1vnand under the strong law oflarge num bers , you can prove that ¯v!E[v]as\\nN!¥. Namely, the empirical sample average approaches the population average as the number of\\nsamples goes do inﬁnity.\\n(Technical note: the notion of convergence here is almost sure convergence. In particular, the formal result is\\nthat Pr\\x10\\nlim N!¥1\\nNånvn=E[v]\\x11\\n= 1. Or, in words, with probability one the sample average reaches the\\npopulation average.)MATHREVIEW | LAW OF LARGE NUMBERS\\nFigure 2.3:\\nalso will not do well on the exam. A model that is neither overﬁt nor\\nunderﬁt is the one that is expected to do best in the future.\\n2.5Separation of Training and Test Data\\nSuppose that, after graduating, you get a job working for a company\\nthat provides personalized recommendations for pottery. You go in\\nand implement new algorithms based on what you learned in your\\nmachine learning class (you have learned the power of generaliza-\\ntion!). All you need to do now is convince your boss that you have\\ndone a good job and deserve a raise!\\nHow can you convince your boss that your fancy learning algo-\\nrithms are really working?\\nBased on what we’ve talked about already with underﬁtting and\\noverﬁtting, it is not enough to just tell your boss what your training\\nerror is. Noise notwithstanding, it is easy to get a training error of\\nzero using a simple database query (or grep , if you prefer). Your boss\\nwill not fall for that.\\nThe easiest approach is to set aside some of your available data as\\n“test data” and use this to evaluate the performance of your learning\\nalgorithm. For instance, the pottery recommendation service that you\\nwork for might have collected 1000 examples of pottery ratings. You\\nwill select 800 of these as train ingdata and set aside the ﬁnal 200limits of learning 25\\nastest data . You will run your learning algorithms only on the 800\\ntraining points. Only once you’re done will you apply your learned\\nmodel to the 200 test points, and report your test erroron those 200\\npoints to your boss.\\nThe hope in this process is that however well you do on the 200\\ntest points will be indicative of how well you are likely to do in the\\nfuture. This is analogous to estimating support for a presidential\\ncandidate by asking a small (random!) sample of people for their\\nopinions. Statistics (speciﬁcally, concentration bounds of which the\\n“Central limit theorem” is a famous example) tells us that if the sam-\\nple is large enough, it will be a good representative. The 80/20split\\nis not magic: it’s simply fairly well established. Occasionally people\\nuse a 90/10split instead, especially if they have a lotof data. If you have more data at your dis-\\nposal, why might a 90/10split be\\npreferable to an 80/20split?? The cardinal rule of machine learning is: never touch your test\\ndata. Ever. If that’s not clear enough:\\nNever ever touch your test data!\\nIf there is only one thing you learn from this book, let it be that.\\nDo not look at your test data. Even once. Even a tiny peek. Once\\nyou do that, it is not test data any more. Yes, perhaps your algorithm\\nhasn’t seen it. But you have. And you are likely a better learner than\\nyour learning algorithm. Consciously or otherwise, you might make\\ndecisions based on whatever you might have seen. Once you look at\\nthe test data, your model’s performance on it is no longer indicative\\nof it’s performance on future unseen data. This is simply because\\nfuture data is unseen, but your “test” data no longer is.\\n2.6Models, Parameters and Hyperparameters\\nThe general approach to machine learning, which captures many ex-\\nisting learning algorithms, is the mod elingapproach. The idea is that\\nwe come up with some formal model of our data. For instance, we\\nmight model the classiﬁcation decision of a student/course pair as a\\ndecision tree. The choice of using a treeto represent this model is our\\nchoice. We also could have used an arithmetic circuit or a polynomial\\nor some other function. The model tells us what sort of things we can\\nlearn, and also tells us what our inductive bias is.\\nFor most models, there will be associated parameters. These are\\nthe things that we use the data to decide on. Parameters in a decision\\ntree include: the speciﬁc questions we asked, the order in which we\\nasked them, and the classiﬁcation decisions at the leaves. The job of\\nour decision tree learning algorithm D ecision TreeTrain is to take\\ndata and ﬁgure out a good set of parameters.26 a course in machine learning\\nMany learning algorithms will have additional knobs that you can\\nadjust. In most cases, these knobs amount to tuning the inductive\\nbias of the algorithm. In the case of the decision tree, an obvious\\nknob that one can tune is the max imum depth of the decision tree.\\nThat is, we could modify the D ecision TreeTrain function so that\\nitstops recursing once it reaches some pre-deﬁned maximum depth.\\nBy playing with this depth knob, we can adjust between underﬁtting\\n(the empty tree, depth =0) and overﬁtting (the full tree, depth =¥). Go back to the D ecision Tree-\\nTrain algorithm and modify it so\\nthat it takes a maximum depth pa-\\nrameter. This should require adding\\ntwo lines of code and modifying\\nthree others.?Such a knob is called a hyperparameter. It is so called because it\\nis a parameter that controls other parameters of the model. The exact\\ndeﬁnition of hyperparameter is hard to pin down: it’s one of those\\nthings that are easier to identify than deﬁne. However, one of the\\nkey identiﬁers for hyperparameters (and the main reason that they\\ncause consternation) is that they cannot be naively adjusted using the\\ntraining data.\\nIn Decision TreeTrain , as in most machine learning, the learn-\\ning algorithm is essentially trying to adjust the parameters of the\\nmodel so as to minimize training error. This suggests an idea for\\nchoosing hyperparameters: choose them so that they minimize train-\\ning error.\\nWhat is wrong with this suggestion? Suppose that you were to\\ntreat “maximum depth” as a hyperparameter and tried to tune it on\\nyour training data. To do this, maybe you simply build a collection\\nof decision trees, tree 0, tree 1, tree 2, . . . , tree 100, where tree dis a tree\\nof maximum depth d. We then computed the training error of each\\nof these trees and chose the “ideal” maximum depth as that which\\nminimizes training error? Which one would it pick?\\nThe answer is that it would pick d=100. Or, in general, it would\\npick das large as possible. Why? Because choosing a bigger dwill\\nnever hurt on the training data. By making dlarger, you are simply\\nencouraging overﬁtting. But by evaluating on the training data, over-\\nﬁtting actually looks like a good idea!\\nAn alternative idea would be to tune the maximum depth on test\\ndata. This is promising because test data peformance is what we\\nreally want to optimize, so tuning this knob on the test data seems\\nlike a good idea. That is, it won’t accidentally reward overﬁtting. Of\\ncourse, it breaks our cardinal rule about test data: that you should\\nnever touch your test data. So that idea is immediately off the table.\\nHowever, our “test data” wasn’t magic. We simply took our 1000\\nexamples, called 800 of them “training” data and called the other 200\\n“test” data. So instead, let’s do the following. Let’s take our original\\n1000 data points, and select 700 of them as training data. From the\\nremainder, take 100 as development data1and the remaining 2001Some people call this “ validation\\ndata ” or “ held -outdata .”as test data. The job of the development data is to allow us to tunelimits of learning 27\\nhyperparameters. The general approach is as follows:\\n1. Split your data into 70% training data, 10% development data and\\n20% test data.\\n2. For each possible setting of your hyperparameters:\\n(a) Train a model using that setting of hyperparameters on the\\ntraining data.\\n(b) Compute this model’s error rate on the development data.\\n3. From the above collection of models, choose the one that achieved\\nthe lowest error rate on development data.\\n4. Evaluate that model on the test data to estimate future test perfor-\\nmance.\\nIn step 3, you could either choose\\nthe model (trained on the 70% train-\\ning data) that did the best on the\\ndevelopment data. Or you could\\nchoose the hyperparameter settings\\nthat did best and retrain the model\\non the 80% union of training and\\ndevelopment data. Is either of these\\noptions obviously better or worse??2.7Real World Applications of Machine Learning\\nFigure 2.4shows a typical sequence of decisions that must be made\\nto deploy a machine learning approach in the real world. In the left\\ncolumn, you can see the generic decision being made. In the right\\ncolumn, an example of this decision for the particular case of adver-\\ntising placement on a search engine we’ve built.\\nIn this sequence, ( 1) we have some real world goal like increasing\\nrevenue for our search engine, and decide to try to increase rev-\\nenue by ( 2) displaying better ads. We convert this task into a ma-\\nchine learning problem by ( 3) deciding to train a classiﬁer to predict\\nwhether a user will click on an ad or not. In order to apply machine\\nlearning, we must collect some training data; in this case, ( 4) we col-\\nlect data by logging user interactions with the current system. We\\nmust choose what to log; ( 5) we choose to log the ad being displayed,\\nthe query the user entered into our search engine, and binary value\\nshowing if they clicked or not.1real world\\ngoalincrease\\nrevenue\\n2real world\\nmechanismbetter ad\\ndisplay\\n3learning\\nproblemclassify\\nclick-through\\n4 data collectioninteraction w/\\ncurrent system\\n5 collected data query, ad, click\\n6data\\nrepresentationbow2,\\x06click\\n7select model\\nfamilydecision trees,\\ndepth 20\\n8select training\\ndatasubset from\\napril’ 16\\n9train model &\\nhyperparamsﬁnal decision\\ntree\\n10predict on test\\ndatasubset from\\nmay’ 16\\n11 evaluate errorzero/one loss\\nfor\\x06click\\n12 deploy!(hope we\\nachieve our\\ngoal)\\nFigure 2.4: A typical design process for\\na machine learning application.In order to make these logs consumable by a machine learning\\nalgorithm, ( 6) we convert the data into input/output pairs: in this\\ncase, pairs of words from a bag-of-words representing the query and\\na bag-of-words representing the ad as input, and the click as a \\x06\\nlabel. We then ( 7) select a model family (e.g., depth 20decision trees),\\nand thereby an inductive bias, for instance depth \\x1420 decision trees.\\nWe’re now ready to ( 8) select a speciﬁc subset of data to use as\\ntraining data: in this case, data from April 2016 . We split this into\\ntraining and development and ( 9) learn a ﬁnal decision tree, tuning\\nthe maximum depth on the development data. We can then use this\\ndecision tree to ( 10) make predictions on some held-out test data, in28 a course in machine learning\\nthis case from the following month. We can ( 11) measure the overall\\nquality of our predictor as zero/one loss (clasiﬁcation error) on this\\ntest data and ﬁnally ( 12) deploy our system.\\nThe important thing about this sequence of steps is that in any\\none, things can go wrong. That is, between any two rows of this table,\\nwe are necessarily accumulating some additional error against our\\noriginal real world goal of increasing revenue. For example, in step 5,\\nwe decided on a representation that left out many possible variables\\nwe could have logged, like time of day or season of year. By leaving\\nout those variables, we set an explicit upper bound on how well our\\nlearned system can do.\\nIt is often an effective strategy to run an oracleexperiment . In an\\noracle experiment, we assume that everything below some line can be\\nsolved perfectly, and measure how much impact that will have on a\\nhigher line. As an extreme example, before embarking on a machine\\nlearning approach to the ad display problem, we should measure\\nsomething like: if our classiﬁer were perfect , how much more money\\nwould we make? If the number is not very high, perhaps there is\\nsome better for our time.\\nFinally, although this sequence is denoted linearly, the entire pro-\\ncess is highly interactive in practice. A large part of “debugging”\\nmachine learning (covered more extensively in Chapter 5involves\\ntrying to ﬁgure out where in this sequence the biggest losses are and\\nﬁxing that step. In general, it is often useful to build the stupidest thing\\nthat could possibly work , then look at how well it’s doing, and decide if\\nand where to ﬁx it.\\n2.8Further Reading\\nTODO further reading3 | G EOMETRY AND NEAREST NEIGHBORS\\nDependencies: Chapter 1You can think of prediction tasks as mapping inputs (course\\nreviews) to outputs (course ratings). As you learned in the previ-\\nous chapter, decomposing an input into a collection of features (e.g.,\\nwords that occur in the review) forms a useful abstraction for learn-\\ning. Therefore, inputs are nothing more than lists of feature values.\\nThis suggests a geometricview of data, where we have one dimen-\\nsion for every feature. In this view, examples are points in a high-\\ndimensional space.\\nOnce we think of a data set as a collection of points in high dimen-\\nsional space, we can start performing geometric operations on this\\ndata. For instance, suppose you need to predict whether Alice will\\nlike Algorithms. Perhaps we can try to ﬁnd another student who is\\nmost “similar” to Alice, in terms of favorite courses. Say this student\\nis Jeremy. If Jeremy liked Algorithms, then we might guess that Alice\\nwill as well. This is an example of a near estneigh bormodel of learn-\\ning. By inspecting this model, we’ll see a completely different set of\\nanswers to the key learning questions we discovered in Chapter 1.\\n3.1From Data to Feature Vectors\\nAn example is just a collection of feature values about that example,\\nfor instance the data in Table 1from the Appendix. To a person, these\\nfeatures have meaning. One feature might count how many times the\\nreviewer wrote “excellent” in a course review. Another might count\\nthe number of exclamation points. A third might tell us if any text is\\nunderlined in the review.\\nTo a machine, the features themselves have no meaning. Only\\nthefeature values, and how they vary across examples, mean some-\\nthing to the machine. From this perspective, you can think about an\\nexample as being represented by a feature vectorconsisting of one\\n“dimension” for each feature, where each dimenion is simply some\\nreal value.\\nConsider a review that said “excellent” three times, had one excla-Learning Objectives:\\n• Describe a data set as points in a\\nhigh dimensional space.\\n• Explain the curse of dimensionality.\\n• Compute distances between points\\nin high dimensional space.\\n• Implement a K-nearest neighbor\\nmodel of learning.\\n• Draw decision boundaries.\\n• Implement the K-means algorithm\\nfor clustering.Our brains have evolved to get us out of the rain, ﬁnd where the\\nberries are, and keep us from getting killed. Our brains did not\\nevolve to help us grasp really large numbers or to look at things in\\na hundred thousand dimensions. – Ronald Graham30 a course in machine learning\\nmation point and no underlined text. This could be represented by\\nthe feature vector h3, 1, 0i. An almost identical review that happened\\nto have underlined text would have the feature vector h3, 1, 1i.\\nNote, here, that we have imposed the convention that for binary\\nfeatures (yes/no features), the corresponding feature values are 0\\nand 1, respectively. This was an arbitrary choice. We could have\\nmade them 0.92 and \\x0016.1 if we wanted. But 0/1 is convenient and\\nhelps us interpret the feature values. When we discuss practical\\nissues in Chapter 5, you will see other reasons why 0/1 is a good\\nchoice.\\neasy?AI?\\nAI?sys?\\neasy?sys?\\nFigure 3.1: A ﬁgure showing projections\\nof data in two dimension in three\\nways – see text. Top: horizontal axis\\ncorresponds to the ﬁrst feature (easy)\\nand the vertical axis corresponds to\\nthe second feature (AI?); Middle:\\nhorizontal is second feature and vertical\\nis third (systems?); Bottom: horizontal\\nis ﬁrst and vertical is third. Truly,\\nthe data points would like exactly on\\n(0, 0)or(1, 0), etc., but they have been\\npurturbed slightly to show duplicates.Figure 3.1shows the data from Table 1in three views. These three\\nviews are constructed by considering two features at a time in differ-\\nent pairs. In all cases, the plusses denote positive examples and the\\nminuses denote negative examples. In some cases, the points fall on\\ntop of each other, which is why you cannot see 20unique points in\\nall ﬁgures.\\nMatch the example ids from Table 1\\nwith the points in Figure 3.1. ?The mapping from feature values to vectors is straighforward in\\nthe case of real valued features (trivial) and binary features (mapped\\nto zero or one). It is less clear what to do with categoricalfeatures .\\nFor example, if our goal is to identify whether an object in an image\\nis a tomato, blueberry, cucumber or cockroach, we might want to\\nknow its color: is it R ed, Blue, Green or Black ?\\nOne option would be to map R edto a value of 0, B lue to a value\\nof 1, G reen to a value of 2 and B lack to a value of 3. The problem\\nwith this mapping is that it turns an unordered set (the set of colors)\\ninto an ordered set (the set f0, 1, 2, 3g). In itself, this is not necessarily\\na bad thing. But when we go to usethese features, we will measure\\nexamples based on their distances to each other. By doing this map-\\nping, we are essentially saying that R edand B lue are more similar\\n(distance of 1) than R edand B lack (distance of 3). This is probably\\nnot what we want to say!\\nA solution is to turn a categorical feature that can take four dif-\\nferent values (say: R ed, Blue, Green and B lack ) into four binary\\nfeatures (say: IsItRed?, IsItBlue?, IsItGreen? and IsItBlack?). In gen-\\neral, if we start from a categorical feature that takes Vvalues, we can\\nmap it to V-many binary indicator features.\\nThe computer scientist in you might\\nbe saying: actually we could map it\\nto log2V-many binary features! Is\\nthis a good idea or not??With that, you should be able to take a data set and map each\\nexample to a feature vector through the following mapping:\\n• Real-valued features get copied directly.\\n• Binary features become 0 (for false) or 1 (for true).\\n• Categorical features with Vpossible values get mapped to V-many\\nbinary indicator features.geometry and nearest neighbors 31\\nAfter this mapping, you can think of a single example as a vec-\\ntorin a high-dimensional feature space . If you have D-many fea-\\ntures (after expanding categorical features), then this feature vector\\nwill have D-many components. We will denote feature vectors as\\nx=hx1,x2, . . . , xDi, so that xddenotes the value of the dth fea-\\nture of x. Since these are vectors with real-valued components in\\nD-dimensions, we say that they belong to the space RD.\\nForD=2, our feature vectors are just points in the plane, like in\\nFigure 3.1. For D=3 this is three dimensional space. For D>3 it\\nbecomes quite hard to visualize. (You should resist the temptation\\nto think of D=4 as “time” – this will just make things confusing.)\\nUnfortunately, for the sorts of problems you will encounter in ma-\\nchine learning, D\\x1920 is considered “low dimensional,” D\\x191000 is\\n“medium dimensional” and D\\x19100000 is “high dimensional.” Can you think of problems (per-\\nhaps ones already mentioned in this\\nbook!) that are low dimensional?\\nThat are medium dimensional?\\nThat are high dimensional??\\n3.2K-Nearest Neighbors\\nThe biggest advantage to thinking of examples as vectors in a high\\ndimensional space is that it allows us to apply geometric concepts\\nto machine learning. For instance, one of the most basic things\\nthat one can do in a vector space is compute distances . In two-\\ndimensional space, the distance between h2, 3iandh6, 1iis given\\nbyp\\n(2\\x006)2+ (3\\x001)2=p\\n18\\x194.24. In general, in D-dimensional\\nspace, the Euclidean distance between vectors aand bis given by\\nEq (3.1) (see Figure 3.2for geometric intuition in three dimensions):\\nd(a,b) =\"\\nD\\nå\\nd=1(ad\\x00bd)2#1\\n2\\n(3.1)\\n(0, .4, .5)(.6, 1, .8)\\nFigure 3.2: A ﬁgure showing Euclidean\\ndistance in three dimensions. The\\nlength of the green segments are 0.6, 0.6\\nand 0.3 respectively, in the x-, y-, and\\nz-axes. The total distance between the\\nred dot and the orange dot is thereforep\\n0.62+0.62+0.32=0.9.\\nVerify that dfrom Eq ( 3.1) gives the\\nsame result (4.24) for the previous\\ncomputation.?\\n?\\nFigure 3.3: A ﬁgure showing an easy\\nNN classiﬁcation problem where the\\ntest point is a ? and should be negative.Now that you have access to distances between examples, you\\ncan start thinking about what it means to learn again. Consider Fig-\\nure3.3. We have a collection of training data consisting of positive\\nexamples and negative examples. There is a test point marked by a\\nquestion mark. Your job is to guess the correct label for that point.\\nMost likely, you decided that the label of this test point is positive.\\nOne reason why you might have thought that is that you believe\\nthat the label for an example should be similar to the label of nearby\\npoints. This is an example of a new form of inductive bias .\\nThe near estneigh borclassiﬁer is build upon this insight. In com-\\nparison to decision trees, the algorithm is ridiculously simple. At\\ntraining time, we simply store the entire training set. At test time,\\nwe get a test example ˆ x. To predict its label, we ﬁnd the training ex-\\nample xthat is most similar to ˆ x. In particular, we ﬁnd the training32 a course in machine learning\\nexample xthat minimizes d (x, ˆx). Since xis a training example, it has\\na corresponding label, y. We predict that the label of ˆ xis also y.\\nDespite its simplicity, this nearest neighbor classiﬁer is incred-\\nibly effective. (Some might say frustratingly effective.) However, it\\nis particularly prone to overﬁtting label noise. Consider the data in\\nFigure 3.4. You would probably want to label the test point positive.\\nUnfortunately, it’s nearest neighbor happens to be negative. Since the\\nnearest neighbor algorithm only looks at the single nearest neighbor,\\nit cannot consider the “preponderance of evidence” that this point\\nshould probably actually be a positive example. It will make an un-\\nnecessary error.\\n?\\nFigure 3.4: A ﬁgure showing an easy\\nNN classiﬁcation problem where the\\ntest point is a ? and should be positive,\\nbut its NN is actually a negative point\\nthat’s noisy.A solution to this problem is to consider more than just the single\\nnearest neighbor when making a classiﬁcation decision. We can con-\\nsider the K-near estneigh bors and let them vote on the correct class\\nfor this test point. If you consider the 3-nearest neighbors of the test\\npoint in Figure 3.4, you will see that two of them are positive and one\\nis negative. Through voting, positive would win.\\nWhy is it a good idea to use an odd\\nnumber for K? ?The full algorithm for K-nearest neighbor classiﬁcation is given\\nin Algorithm 3.2. Note that there actually is no “training” phase for\\nK-nearest neighbors. In this algorithm we have introduced ﬁve new\\nconventions:\\n1. The training data is denoted by D.\\n2. We assume that there are N-many training examples.\\n3. These examples are pairs (x1,y1),(x2,y2), . . . ,(xN,yN).\\n(Warning: do not confuse xn, the nth training example, with xd,\\nthedth feature for example x.)\\n4. We use [ ]to denote an empty list and \\x08\\x01to append\\x01to that list.\\n5. Our prediction on ˆ xis called ˆy.\\nThe ﬁrst step in this algorithm is to compute distances from the\\ntest point to all training points (lines 2-4). The data points are then\\nsorted according to distance. We then apply a clever trick of summing\\nthe class labels for each of the Knearest neighbors (lines 6-10) and\\nusing the sign of this sum as our prediction. Why is the sign of the sum com-\\nputed in lines 2-4the same as the\\nmajority vote of the associated\\ntraining examples??The big question, of course, is how to choose K. As we’ve seen,\\nwith K=1, we run the risk of overﬁtting. On the other hand, if\\nKis large (for instance, K=N), then KNN-P redict will always\\npredict the majority class. Clearly that is underﬁtting. So, Kis a\\nhyperparameter of the KNN algorithm that allows us to trade-off\\nbetween overﬁtting (small value of K) and underﬁtting (large value of\\nK).\\nWhy can’t you simply pick the\\nvalue of Kthat does best on the\\ntraining data? In other words, why\\ndo we have to treat it like a hy-\\nperparameter rather than just a\\nparameter.?geometry and nearest neighbors 33\\nAlgorithm 3KNN-P redict (D,K, ˆx)\\n1:S [ ]\\n2:forn=1toNdo\\n3:S S\\x08hd(xn, ˆx),ni // store distance to training example n\\n4:end for\\n5:S sort (S) // put lowest-distance objects ﬁrst\\n6:ˆy 0\\n7:fork=1toKdo\\n8:hdist,ni Sk //nthis is the kth closest data point\\n9: ˆy ˆy+yn // vote according to the label for the nth training point\\n10:end for\\n11:return sign (ˆy) // return +1ifˆy>0and\\x001ifˆy<0\\nOne aspect of inductive bias that we’ve seen for KNN is that it\\nassumes that nearby points should have the same label. Another\\naspect, which is quite different from decision trees, is that all features\\nare equally important! Recall that for decision trees, the key question\\nwas which features are most useful for classiﬁcation? The whole learning\\nalgorithm for a decision tree hinged on ﬁnding a small set of good\\nfeatures. This is all thrown away in KNN classiﬁers: every feature\\nis used, and they are all used the same amount. This means that if\\nyou have data with only a few relevant features and lots of irrelevant\\nfeatures, KNN is likely to do poorly.\\nFigure 3.5: A ﬁgure of a ski and a\\nsnowboard.\\nFigure 3.6: Classiﬁcation data for ski vs\\nsnowboard in 2d\\nA related issue with KNN is feature scale . Suppose that we are\\ntrying to classify whether some object is a ski or a snowboard (see\\nFigure 3.5). We are given two features about this data: the width\\nand height. As is standard in skiing, width is measured in millime-\\nters and height is measured in centimeters. Since there are only two\\nfeatures, we can actually plot the entire training set; see Figure 3.6\\nwhere ski is the positive class. Based on this data, you might guess\\nthat a KNN classiﬁer would do well.\\nSuppose, however, that our measurement of the width was com-\\nputed in millimeters (instead of centimeters). This yields the data\\nshown in Figure 3.7. Since the width values are now tiny, in compar-\\nison to the height values, a KNN classiﬁer will effectively ignore the\\nwidth values and classify almost purely based on height. The pre-\\ndicted class for the displayed test point had changed because of this\\nfeature scaling.\\nFigure 3.7: Classiﬁcation data for ski vs\\nsnowboard in 2d, with width rescaled\\nto mm.We will discuss feature scaling more in Chapter 5. For now, it is\\njust important to keep in mind that KNN does not have the power to\\ndecide which features are important.34 a course in machine learning\\nA (real-valued) vector is just an array of real values, for instance x=h1, 2.5,\\x006iis a three-dimensional\\nvector. In general, if x=hx1,x2, . . . , xDi, then xdis it’s dth component. So x3=\\x006 in the previous ex-\\nample.\\nVector sums are computed pointwise, and are only deﬁned when dimensions match, so h1, 2.5,\\x006i+\\nh2,\\x002.5, 3i=h3, 0,\\x003i. In general, if c=a+bthen cd=ad+bdfor all d. Vector addition can\\nbe viewed geometrically as taking a vector a, then tacking on bto the end of it; the new end point is\\nexactly c.\\nVectors can be scaled by real values; for instance 2 h1, 2.5,\\x006i=h2, 5,\\x0012i; this is called scalar multi-\\nplication. In general, ax=hax1,ax2, . . . , axDi.\\nThe norm of a vector x, writtenjjxjjis its length. Unless otherwise speciﬁed, this is its Euclidean length,\\nnamely:jjxjj=q\\nådx2\\nd.MATHREVIEW | VECTOR ARITHMETIC AND VECTOR NORMS\\nFigure 3.8:\\n3.3Decision Boundaries\\nThe standard way that we’ve been thinking about learning algo-\\nrithms up to now is in the query model . Based on training data, you\\nlearn something. I then give you a query example and you have to\\nguess it’s label.\\nFigure 3.9: decision boundary for 1nn.An alternative, less passive, way to think about a learned model\\nis to ask: what sort of test examples will it classify as positive, and\\nwhat sort will it classify as negative. In Figure 3.9, we have a set of\\ntraining data. The background of the image is colored blue in regions\\nthat would be classiﬁed as positive (if a query were issued there)\\nand colored red in regions that would be classiﬁed as negative. This\\ncoloring is based on a 1-nearest neighbor classiﬁer.\\nIn Figure 3.9, there is a solid line separating the positive regions\\nfrom the negative regions. This line is called the decision bound ary\\nfor this classiﬁer. It is the line with positive land on one side and\\nnegative land on the other side.\\nFigure 3.10: decision boundary for knn\\nwith k= 3.Decision boundaries are useful ways to visualize the com plex -\\nityof a learned model. Intuitively, a learned model with a decision\\nboundary that is really jagged (like the coastline of Norway) is really\\ncomplex and prone to overﬁtting. A learned model with a decision\\nboundary that is really simple (like the bounary between Arizona\\nand Utah) is potentially underﬁt.\\nNow that you know about decision boundaries, it is natural to ask:\\nwhat do decision boundaries for decision trees look like? In ordergeometry and nearest neighbors 35\\nto answer this question, we have to be a bit more formal about how\\nto build a decision tree on real-valued features. (Remember that the\\nalgorithm you learned in the previous chapter implicitly assumed\\nbinary feature values.) The idea is to allow the decision tree to ask\\nquestions of the form: “is the value of feature 5 greater than 0.2?”\\nThat is, for real-valued features, the decision tree nodes are param-\\neterized by a feature and a threshold for that feature. An example\\ndecision tree for classifying skis versus snowboards is shown in Fig-\\nure3.11.\\nFigure 3.11: decision tree for ski vs.\\nsnowboard\\nFigure 3.12: decision boundary for dt in\\nprevious ﬁgureNow that a decision tree can handle feature vectors, we can talk\\nabout decision boundaries. By example, the decision boundary for\\nthe decision tree in Figure 3.11is shown in Figure 3.12. In the ﬁgure,\\nspace is ﬁrst split in half according to the ﬁrst query along one axis.\\nThen, depending on which half of the space you look at, it is either\\nsplit again along the other axis, or simply classiﬁed.\\nFigure 3.12is a good visualization of decision boundaries for\\ndecision trees in general. Their decision boundaries are axis-aligned\\ncuts. The cuts must be axis-aligned because nodes can only query on\\na single feature at a time. In this case, since the decision tree was so\\nshallow, the decision boundary was relatively simple.\\nWhat sort of data might yield a\\nvery simple decision boundary with\\na decision tree and very complex\\ndecision boundary with 1-nearest\\nneighbor? What about the other\\nway around??3.4K-Means Clustering\\nUp through this point, you have learned all about supervised learn-\\ning (in particular, binary classiﬁcation). As another example of the\\nuse of geometric intuitions and data, we are going to temporarily\\nconsider an unsupervised learn ingproblem. In unsupervised learn-\\ning, our data consists only of examples xnand does notcontain corre-\\nsponding labels. Your job is to make sense of this data, even though\\nno one has provided you with correct labels. The particular notion of\\n“making sense of” that we will talk about now is the clusteringtask.\\nFigure 3.13: simple clustering data...\\nclusters in UL, UR and BC.Consider the data shown in Figure 3.13. Since this is unsupervised\\nlearning and we do not have access to labels, the data points are\\nsimply drawn as black dots. Your job is to split this data set into\\nthree clusters. That is, you should label each data point as A, B or C\\nin whatever way you want.\\nFor this data set, it’s pretty clear what you should do. You prob-\\nably labeled the upper-left set of points A, the upper-right set of\\npoints B and the bottom set of points C. Or perhaps you permuted\\nthese labels. But chances are your clusters were the same as mine.\\nThe K-means clustering algorithm is a particularly simple and\\neffective approach to producing clusters on data like you see in Fig-\\nure3.13. The idea is to represent each cluster by it’s cluster center.\\nGiven cluster centers, we can simply assign each point to its nearest36 a course in machine learning\\ncenter. Similarly, if we know the assignment of points to clusters, we\\ncan compute the centers. This introduces a chicken-and-egg problem.\\nIf we knew the clusters, we could compute the centers. If we knew\\nthe centers, we could compute the clusters. But we don’t know either.\\nFigure 3.14: ﬁrst few iterations of\\nk-means running on previous data setThe general computer science answer to chicken-and-egg problems\\nisiteration. We will start with a guess of the cluster centers. Based\\non that guess, we will assign each data point to its closest center.\\nGiven these new assignments, we can recompute the cluster centers.\\nWe repeat this process until clusters stop moving. The ﬁrst few it-\\nerations of the K-means algorithm are shown in Figure 3.14. In this\\nexample, the clusters converge very quickly.\\nAlgorithm 3.4spells out the K-means clustering algorithm in de-\\ntail. The cluster centers are initialized randomly. In line 6, data point\\nxnis compared against each cluster center mk. It is assigned to cluster\\nkifkis the center with the smallest distance. (That is the “argmin”\\nstep.) The variable znstores the assignment (a value from 1 to K) of\\nexample n. In lines 8-12, the cluster centers are re-computed. First, Xk\\nstores all examples that have been assigned to cluster k. The center of\\ncluster k,mkis then computed as the mean of the points assigned to\\nit. This process repeats until the centers converge.\\nAn obvious question about this algorithm is: does it converge?\\nA second question is: how long does it take to converge. The ﬁrst\\nquestion is actually easy to answer. Yes, it does. And in practice, it\\nusually converges quite quickly (usually fewer than 20 iterations). In\\nChapter 15, we will actually prove that it converges. The question of\\nhow long it takes to converge is actually a really interesting question.\\nEven though the K-means algorithm dates back to the mid 1950 s, the\\nbest known convergence rates were terrible for a long time. Here, ter-\\nrible means exponential in the number of data points. This was a sad\\nsituation because empirically we knew that it converged very quickly.\\nNew algorithm analysis techniques called “smoothed analysis” were\\ninvented in 2001 and have been used to show very fast convergence\\nforK-means (among other algorithms). These techniques are well\\nbeyond the scope of this book (and this author!) but sufﬁce it to say\\nthat K-means is fast in practice and is provably fast in theory.\\nIt is important to note that although K-means is guaranteed to\\nconverge and guaranteed to converge quickly, it is notguaranteed to\\nconverge to the “right answer.” The key problem with unsupervised\\nlearning is that we have no way of knowing what the “right answer”\\nis. Convergence to a bad solution is usually due to poor initialization. What is the difference between un-\\nsupervised and supervised learning\\nthat means that we know what the\\n“right answer” is for supervised\\nlearning but not for unsupervised\\nlearning??geometry and nearest neighbors 37\\nAlgorithm 4K-Means (D,K)\\n1:fork=1toKdo\\n2:mk some random location // randomly initialize center for kth cluster\\n3:end for\\n4:repeat\\n5:forn=1toNdo\\n6: zn argminkjjmk\\x00xnjj // assign example nto closest center\\n7:end for\\n8:fork=1toKdo\\n9: Xk f xn:zn=kg // points assigned to cluster k\\n10: mk mean (Xk) // re-estimate center of cluster k\\n11:end for\\n12:until ms stop changing\\n13:return z // return cluster assignments\\n3.5Warning: High Dimensions are Scary\\nVisualizing one hundred dimensional space is incredibly difﬁcult for\\nhumans. After huge amounts of training, some people have reported\\nthat they can visualize four dimensional space in their heads. But\\nbeyond that seems impossible.1 1If you want to try to get an intu-\\nitive sense of what four dimensions\\nlooks like, I highly recommend the\\nshort 1884 book Flatland: A Romance\\nof Many Dimensions by Edwin Abbott\\nAbbott. You can even read it online at\\ngutenberg.org/ebooks/201 .In addition to being hard to visualize, there are at least two addi-\\ntional problems in high dimensions, both refered to as thecurse of\\ndimen sion ality. One is computational, the other is mathematical.\\nFigure 3.15:2d knn with an overlaid\\ngrid, cell with test point highlightedFrom a computational perspective, consider the following prob-\\nlem. For K-nearest neighbors, the speed of prediction is slow for a\\nvery large data set. At the very least you have to look at every train-\\ning example every time you want to make a prediction. To speed\\nthings up you might want to create an indexing data structure. You\\ncan break the plane up into a grid like that shown in Figure 3.15.\\nNow, when the test point comes in, you can quickly identify the grid\\ncell in which it lies. Now, instead of considering alltraining points,\\nyou can limit yourself to training points in that grid cell (and perhaps\\nthe neighboring cells). This can potentially lead to huge computa-\\ntional savings.\\nIn two dimensions, this procedure is effective. If we want to break\\nspace up into a grid whose cells are 0.2 \\x020.2, we can clearly do this\\nwith 25 grid cells in two dimensions (assuming the range of the\\nfeatures is 0 to 1 for simplicity). In three dimensions, we’ll need\\n125=5\\x025\\x025 grid cells. In four dimensions, we’ll need 625. By the\\ntime we get to “low dimensional” data in 20 dimensions, we’ll need\\n95, 367, 431, 640, 625 grid cells (that’s 95 trillion, which is about 6 to\\n7 times the US national debt as of January 2011 ). So if you’re in 20\\ndimensions, this gridding technique will only be useful if you have at\\nleast 95 trillion training examples.38 a course in machine learning\\nFor “medium dimensional” data (approximately 1000) dimesions,\\nthe number of grid cells is a 9 followed by 698 numbers before the\\ndecimal point. For comparison, the number of atoms in the universe\\nis approximately 1 followed by 80 zeros. So even if each atom yielded\\na googul training examples, we’d still have far fewer examples than\\ngrid cells. For “high dimensional” data (approximately 100000) di-\\nmensions, we have a 1 followed by just under 70, 000 zeros. Far too\\nbig a number to even really comprehend.\\nSufﬁce it to say that for even moderately high dimensions, the\\namount of computation involved in these problems is enormous. How does the above analysis relate\\nto the number of data points you\\nwould need to ﬁll out a full decision\\ntree with D-many features? What\\ndoes this say about the importance\\nof shallow trees??In addition to the computational difﬁculties of working in high\\ndimensions, there are a large number of strange mathematical oc-\\ncurances there. In particular, many of your intuitions that you’ve\\nbuilt up from working in two and three dimensions just do not carry\\nover to high dimensions. We will consider two effects, but there are\\ncountless others. The ﬁrst is that high dimensional spheres look more\\nlike porcupines than like balls.2The second is that distances between\\n2This result was related to me by Mark\\nReid, who heard about it from Marcus\\nHutter.points in high dimensions are all approximately the same.\\nFigure 3.16:2d spheres in spheresLet’s start in two dimensions as in Figure 3.16. We’ll start with\\nfour green spheres, each of radius one and each touching exactly two\\nother green spheres. (Remember that in two dimensions a “sphere”\\nis just a “circle.”) We’ll place a red sphere in the middle so that it\\ntouches all four green spheres. We can easily compute the radius of\\nthis small sphere. The pythagorean theorem says that 12+12= (1+\\nr)2, so solving for rwe get r=p\\n2\\x001\\x190.41. Thus, by calculation,\\nthe blue sphere lies entirely within the cube (cube = square) that\\ncontains the grey spheres. (Yes, this is also obvious from the picture,\\nbut perhaps you can see where this is going.)\\nFigure 3.17:3d spheres in spheresNow we can do the same experiment in three dimensions, as\\nshown in Figure 3.17. Again, we can use the pythagorean theorem\\nto compute the radius of the blue sphere. Now, we get 12+12+12=\\n(1+r)2, sor=p\\n3\\x001\\x190.73. This is still entirely enclosed in the\\ncube of width four that holds all eight grey spheres.\\nAt this point it becomes difﬁcult to produce ﬁgures, so you’ll\\nhave to apply your imagination. In four dimensions, we would have\\n16 green spheres (called hyperspheres ), each of radius one. They\\nwould still be inside a cube (called a hypercube ) of width four. The\\nblue hypersphere would have radius r=p\\n4\\x001=1. Continuing\\nto ﬁve dimensions, the blue hypersphere embedded in 256 green\\nhyperspheres would have radius r=p\\n5\\x001\\x191.23 and so on.\\nIn general, in D-dimensional space, there will be 2Dgreen hyper-\\nspheres of radius one. Each green hypersphere will touch exactly\\nn-many other hyperspheres. The blue hyperspheres in the middle\\nwill touch them all and will have radius r=p\\nD\\x001.geometry and nearest neighbors 39\\nThink about this for a moment. As the number of dimensions\\ngrows, the radius of the blue hypersphere grows without bound! . For\\nexample, in 9-dimensions the radius of the blue hypersphere is nowp\\n9\\x001=2. But with a radius of two, the blue hypersphere is now\\n“squeezing” between the green hypersphere and touching the edges\\nof the hypercube. In 10 dimensional space, the radius is approxi-\\nmately 2.16 and it pokes outside the cube.\\nThe second strange fact we will consider has to do with the dis-\\ntances between points in high dimensions. We start by considering\\nrandom points in one dimension. That is, we generate a fake data set\\nconsisting of 100 random points between zero and one. We can do\\nthe same in two dimensions and in three dimensions. See Figure ??\\nfor data distributed uniformly on the unit hypercube in different\\ndimensions.\\nNow, pick two of these points at random and compute the dis-\\ntance between them. Repeat this process for all pairs of points and\\naverage the results. For the data shown in Figure ??, the average\\ndistance between points in one dimension is about 0.346; in two di-\\nmensions is about 0.518; and in three dimensions is 0.615. The fact\\nthat these increase as the dimension increases is not surprising. The\\nfurthest two points can be in a 1-dimensional hypercube (line) is 1;\\nthe furthest in a 2-dimensional hypercube (square) isp\\n2 (opposite\\ncorners); the furthest in a 3-d hypercube isp\\n3 and so on. In general,\\nthe furthest two points in a D-dimensional hypercube will bep\\nD.\\nYou can actually compute these values analytically. Write UniD\\nfor the uniform distribution in Ddimensions. The quantity we are\\ninterested in computing is:\\navgDist (D) =Ea\\x18UniDh\\nEb\\x18UniDh\\njja\\x00bjjii\\n(3.2)\\nWe can actually compute this in closed form and arrive at avgDist (D) =p\\nD/3. Because we know that the maximum distance between two\\npoints grows likep\\nD, this says that the ratio between average dis-\\ntance and maximum distance converges to 1/3.\\nWhat is more interesting, however, is the variance of the distribu-\\ntion of distances. You can show that in Ddimensions, the variance\\nisconstant 1/p\\n18,independent of D . This means that when you look\\nat (variance) divided-by (max distance), the variance behaves like\\n1/p\\n18D, which means that the effective variance continues to shrink\\nasDgrows3.3Brin 1995\\n0.0 0.2 0.4 0.6 0.8 1.0\\ndistance / sqrt(dimensionality)02000400060008000100001200014000# of pairs of points at that distancedimensionality versus uniform point distances\\n2 dims\\n8 dims\\n32 dims\\n128 dims\\n512 dims\\nFigure 3.18: histogram of distances in\\nD=2,8,32,128,512When I ﬁrst saw and re-proved this result, I was skeptical, as I\\nimagine you are. So I implemented it. In Figure 3.18you can see\\nthe results. This presents a histogram of distances between random\\npoints in Ddimensions for D2f1, 2, 3, 10, 20, 100g. As you can see,\\nall of these distances begin to concentrate around 0.4p\\nD, even for40 a course in machine learning\\n“medium dimension” problems.\\nYou should now be terriﬁed: the only bit of information that KNN\\ngets is distances. And you’ve just seen that in moderately high di-\\nmensions, all distances becomes equal. So then isn’t it the case that\\nKNN simply cannot work?\\nFigure 3.19:knn:mnist : histogram of\\ndistances in multiple D for mnistThe answer has to be no. The reason is that the data that we get\\nisnotuniformly distributed over the unit hypercube. We can see this\\nby looking at two real-world data sets. The ﬁrst is an image data set\\nof hand-written digits (zero through nine); see Section ??. Although\\nthis data is originally in 256 dimensions (16 pixels by 16 pixels), we\\ncan artiﬁcally reduce the dimensionality of this data. In Figure 3.19\\nyou can see the histogram of average distances between points in this\\ndata at a number of dimensions.\\nAs you can see from these histograms, distances have notcon-\\ncentrated around a single value. This is very good news: it means\\nthat there is hope for learning algorithms to work! Nevertheless, the\\nmoral is that high dimensions are weird.\\n3.6Further Reading\\nTODO further reading4 | T HEPERCEPTRON\\nDependencies: Chapter 1, Chapter 3So far ,you’ve seen two types of learning models: in decision\\ntrees, only a small number of features are used to make decisions; in\\nnearest neighbor algorithms, all features are used equally. Neither of\\nthese extremes is always desirable. In some problems, we might want\\nto use most of the features, but use some more than others.\\nIn this chapter, we’ll discuss the perceptron algorithm for learn-\\ningweights for features. As we’ll see, learning weights for features\\namounts to learning a hyperplane classiﬁer: that is, basically a di-\\nvision of space into two halves by a straight line, where one half is\\n“positive” and one half is “negative.” In this sense, the perceptron\\ncan be seen as explicitly ﬁnding a good lineardecision bound ary.\\n4.1Bio-inspired Learning\\nFigure 4.1: a picture of a neuronFolk biology tells us that our brains are made up of a bunch of little\\nunits, called neurons , that send electrical signals to one another. The\\nrateof ﬁring tells us how “activated” a neuron is. A single neuron,\\nlike that shown in Figure 4.1might have three incoming neurons.\\nThese incoming neurons are ﬁring at different rates (i.e., have dif-\\nferent activations ). Based on how much these incoming neurons are\\nﬁring, and how “strong” the neural connections are, our main neu-\\nron will “decide” how strongly it wants to ﬁre. And so on through\\nthe whole brain. Learning in the brain happens by neurons becom-\\nming connected to other neurons, and the strengths of connections\\nadapting over time.\\nFigure 4.2: ﬁgure showing feature\\nvector and weight vector and products\\nand sumThe real biological world is much more complicated than this.\\nHowever, our goal isn’t to build a brain, but to simply be inspired\\nby how they work. We are going to think of our learning algorithm\\nas a single neuron. It receives input from D-many other neurons,\\none for each input feature. The strength of these inputs are the fea-\\nture values. This is shown schematically in Figure 4.1. Each incom-\\ning connection has a weight and the neuron simply sums up all the\\nweighted inputs. Based on this sum, it decides whether to “ﬁre” orLearning Objectives:\\n• Describe the biological motivation\\nbehind the perceptron.\\n• Classify learning algorithms based\\non whether they are error-driven or\\nnot.\\n• Implement the perceptron algorithm\\nfor binary classiﬁcation.\\n• Draw perceptron weight vectors\\nand the corresponding decision\\nboundaries in two dimensions.\\n• Contrast the decision boundaries\\nof decision trees, nearest neighbor\\nalgorithms and perceptrons.\\n• Compute the margin of a given\\nweight vector on a given data set.Algebra is nothing more than geometry, in words; geometry is\\nnothing more than algebra, in pictures. – Sophie Germain42 a course in machine learning\\nnot. Firing is interpreted as being a positive example and not ﬁring is\\ninterpreted as being a negative example. In particular, if the weighted\\nsum is positive, it “ﬁres” and otherwise it doesn’t ﬁre. This is shown\\ndiagramatically in Figure 4.2.\\nMathematically, an input vector x=hx1,x2, . . . , xDiarrives. The\\nneuron stores D-many weights, w1,w2, . . . , wD. The neuron computes\\nthe sum:\\na=D\\nå\\nd=1wdxd (4.1)\\nto determine it’s amount of “activation.” If this activiation is posi-\\ntive (i.e., a>0) it predicts that this example is a positive example.\\nOtherwise it predicts a negative example.\\nThe weights of this neuron are fairly easy to interpret. Suppose\\nthat a feature, for instance “is this a System’s class?” gets a zero\\nweight. Then the activation is the same regardless of the value of\\nthis feature. So features with zero weight are ignored. Features with\\npositive weights are indicative of positive examples because they\\ncause the activation to increase. Features with negative weights are\\nindicative of negative examples because they cause the activiation to\\ndecrease. What would happen if we encoded\\nbinary features like “is this a Sys-\\ntem’s class” as no=0 and yes= \\x001\\n(rather than the standard no=0 and\\nyes=+1)??It is often convenient to have a non-zero thresh old. In other\\nwords, we might want to predict positive if a>qfor some value\\nq. The way that is most convenient to achieve this is to introduce a\\nbias term into the neuron, so that the activation is always increased\\nby some ﬁxed value b. Thus, we compute:\\na=\"\\nD\\nå\\nd=1wdxd#\\n+b (4.2)\\nIf you wanted the activation thresh-\\nold to be a>qinstead of a>0,\\nwhat value would bhave to be??This is the complete neural model of learning. The model is pa-\\nrameterized by D-many weights, w1,w2, . . . , wD, and a single scalar\\nbias value b.\\n4.2Error-Driven Updating: The Perceptron Algorithm\\nThe perceptron is a classic learning algorithm for the neural model\\nof learning. Like K-nearest neighbors, it is one of those frustrating\\nalgorithms that is incredibly simple and yet works amazingly well,\\nfor some types of problems.\\nThe algorithm is actually quite different than either the decision\\ntree algorithm or the KNN algorithm. First, it is online. This means\\nthat instead of considering the entire data set at the same time, it only\\never looks at one example. It processes that example and then goesthe perceptron 43\\nAlgorithm 5Perceptron Train (D,MaxIter )\\n1:wd 0, for all d=1. . .D // initialize weights\\n2:b 0 // initialize bias\\n3:foriter=1. . .MaxIter do\\n4:for all (x,y)2Ddo\\n5: a åD\\nd=1wdxd+b // compute activation for this example\\n6: ifya\\x140then\\n7: wd wd+yxd, for all d=1. . .D // update weights\\n8: b b+y // update bias\\n9: end if\\n10:end for\\n11:end for\\n12:return w0,w1, . . . , wD,b\\nAlgorithm 6Perceptron Test(w0,w1, . . . , wD,b, ˆx)\\n1:a åD\\nd=1wdˆxd+b // compute activation for the test example\\n2:return sign (a)\\non to the next one. Second, it is errordriven . This means that, so\\nlong as it is doing well, it doesn’t bother updating its parameters.\\nThe algorithm maintains a “guess” at good parameters (weights\\nand bias) as it runs. It processes one example at a time. For a given\\nexample, it makes a prediction. It checks to see if this prediction\\nis correct (recall that this is training data , so we have access to true\\nlabels). If the prediction is correct, it does nothing. Only when the\\nprediction is incorrect does it change its parameters, and it changes\\nthem in such a way that it would do better on this example next\\ntime around. It then goes on to the next example. Once it hits the\\nlast example in the training set, it loops back around for a speciﬁed\\nnumber of iterations.\\nThe training algorithm for the perceptron is shown in Algo-\\nrithm 4.2and the corresponding prediction algorithm is shown in\\nAlgorithm 4.2. There is one “trick” in the training algorithm, which\\nprobably seems silly, but will be useful later. It is in line 6, when we\\ncheck to see if we want to make an update or not. We want to make\\nan update if the current prediction (just sign (a)) is incorrect. The\\ntrick is to multiply the true label yby the activation aand compare\\nthis against zero. Since the label yis either +1 or\\x001, you just need\\nto realize that yais positive whenever aand yhave the same sign.\\nIn other words, the product yais positive if the current prediction is\\ncorrect. It is very very important to check\\nya\\x140 rather than ya<0. Why? ?The particular form of update for the perceptron is quite simple.\\nThe weight wdis increased by yxdand the bias is increased by y. The\\ngoal of the update is to adjust the parameters so that they are “bet-\\nter” for the current example. In other words, if we saw this example44 a course in machine learning\\ntwice in a row, we should do a better job the second time around.\\nTo see why this particular update achieves this, consider the fol-\\nlowing scenario. We have some current set of parameters w1, . . . , wD,b.\\nWe observe an example (x,y). For simplicity, suppose this is a posi-\\ntive example, so y= + 1. We compute an activation a, and make an\\nerror. Namely, a<0. We now update our weights and bias. Let’s call\\nthe new weights w0\\n1, . . . , w0\\nD,b0. Suppose we observe the same exam-\\nple again and need to compute a new activation a0. We proceed by a\\nlittle algebra:\\na0=D\\nå\\nd=1w0\\ndxd+b0(4.3)\\n=D\\nå\\nd=1(wd+xd)xd+ (b+1) (4.4)\\n=D\\nå\\nd=1wdxd+b+D\\nå\\nd=1xdxd+1 ( 4.5)\\n=a+D\\nå\\nd=1x2\\nd+1> a (4.6)\\nSo the difference between the old activation aand the new activa-\\ntion a0isådx2\\nd+1. But x2\\nd\\x150, since it’s squared. So this value is\\nalways at least one. Thus, the new activation is always at least the old\\nactivation plus one. Since this was a positive example, we have suc-\\ncessfully moved the activation in the proper direction. (Though note\\nthat there’s no guarantee that we will correctly classify this point the\\nsecond, third or even fourth time around!) This analysis hold for the case pos-\\nitive examples ( y= + 1). It should\\nalso hold for negative examples.\\nWork it out.?\\nFigure 4.3: training and test error via\\nearly stoppingThe only hyperparameterof the perceptron algorithm is MaxIter ,\\nthe number of passes to make over the training data. If we make\\nmany many passes over the training data, then the algorithm is likely\\nto overﬁt. (This would be like studying too long for an exam and just\\nconfusing yourself.) On the other hand, going over the data only\\none time might lead to underﬁtting. This is shown experimentally in\\nFigure 4.3. The x-axis shows the number of passes over the data and\\nthe y-axis shows the training error and the test error. As you can see,\\nthere is a “sweet spot” at which test performance begins to degrade\\ndue to overﬁtting.\\nOne aspect of the perceptron algorithm that is left underspeciﬁed\\nis line 4, which says: loop over all the training examples. The natural\\nimplementation of this would be to loop over them in a constant\\norder. The is actually a bad idea.\\nConsider what the perceptron algorithm would do on a data set\\nthat consisted of 500 positive examples followed by 500 negative\\nexamples. After seeing the ﬁrst few positive examples (maybe ﬁve),\\nit would likely decide that every example is positive, and would stopthe perceptron 45\\nlearning anything. It would do well for a while (next 495examples),\\nuntil it hit the batch of negative examples. Then it would take a while\\n(maybe ten examples) before it would start predicting everything as\\nnegative. By the end of one pass through the data, it would really\\nonly have learned from a handful of examples (ﬁfteen in this case).\\nFigure 4.4: training and test error for\\npermuting versus not-permutingSo one thing you need to avoid is presenting the examples in some\\nﬁxed order. This can easily be accomplished by permuting the order\\nof examples once in the beginning and then cycling over the data set\\nin the same (permuted) order each iteration. However, it turns out\\nthat you can actually do better if you re-permute the examples in each\\niteration. Figure 4.4shows the effect of re-permuting on convergence\\nspeed. In practice, permuting each iteration tends to yield about 20%\\nsavings in number of iterations. In theory, you can actually prove that\\nit’s expected to be about twice as fast. If permuting the data each iteration\\nsaves somewhere between 20% and\\n50% of your time, are there any\\ncases in which you might notwant\\nto permute the data every iteration??4.3Geometric Intrepretation\\nA question you should be asking yourself by now is: what does the\\ndecision boundary of a perceptron look like? You can actually answer\\nthat question mathematically. For a perceptron, the decision bound-\\nary is precisely where the sign of the activation, a, changes from\\x001\\nto+1. In other words, it is the set of points xthat achieve zero ac-\\ntivation. The points that are not clearly positive nor negative. For\\nsimplicity, we’ll ﬁrst consider the case where there is no “bias” term\\n(or, equivalently, the bias is zero). Formally, the decision boundary B\\nis:\\nB=(\\nx:å\\ndwdxd=0)\\n(4.7)\\nWe can now apply some linear algebra. Recall that ådwdxdis just\\nthedotprod uctbetween the vector w=hw1,w2, . . . , wDiand the\\nvector x. We will write this as w\\x01x. Two vectors have a zero dot\\nproduct if and only if they are perpendicular. Thus, if we think of\\nthe weights as a vector w, then the decision boundary is simply the\\nplane perpendicular to w.46 a course in machine learning\\nuv}a\\n}b\\nGiven two vectors uand vtheir dot product u\\x01visådudvd. The dot product\\ngrows large and positive when uand vpoint in same direction, grows large\\nand negative when uand vpoint in opposite directions, and is zero when\\ntheir are perpendicular. A useful geometric interpretation of dot products is\\nprojection. Supposejjujj=1, so that uis aunit vector. We can think of any\\nother vector vas consisting of two components: (a) a component in the di-\\nrection of uand (b) a component that’s perpendicular to u. This is depicted\\ngeometrically to the right: Here, u=h0.8, 0.6iand v=h0.37, 0.73i. We\\ncan think of vas the sum of two vectors, aand b, where ais parallel to uand bis perpendicular. The\\nlength of bis exactly u\\x01v=0.734, which is why you can think of dot products as projections: the dot\\nproduct between uand vis the “projection of vonto u.”MATHREVIEW | DOTPRODUCTS\\nFigure 4.5:\\nFigure 4.6: picture of data points with\\nhyperplane and weight vectorThis is shown pictorially in Figure 4.6. Here, the weight vector is\\nshown, together with it’s perpendicular plane. This plane forms the\\ndecision boundary between positive points and negative points. The\\nvector points in the direction of the positive examples and away from\\nthe negative examples.\\nOne thing to notice is that the scale of the weight vector is irrele-\\nvant from the perspective of classiﬁcation. Suppose you take a weight\\nvector wand replace it with 2 w. All activations are now doubled.\\nBut their sign does not change. This makes complete sense geometri-\\ncally, since all that matters is which side of the plane a test point falls\\non, now how far it is from that plane. For this reason, it is common\\nto work with normalized weight vectors, w, that have length one; i.e.,\\njjwjj=1. If I give you a non-zero weight vec-\\ntorw, how do I compute a weight\\nvector w0that points in the same\\ndirection but has a norm of one??\\nFigure 4.7: The same picture as before,\\nbut with projections onto weight vector;\\nthen, below, those points along a one-\\ndimensional axis with zero marked.The geometric intuition can help us even more when we realize\\nthat dot products compute projections. That is, the value w\\x01xis\\njust the distance of xfrom the origin when projected onto the vector\\nw. This is shown in Figure 4.7. In that ﬁgure, all the data points are\\nprojected onto w. Below, we can think of this as a one-dimensional\\nversion of the data, where each data point is placed according to its\\nprojection along w. This distance along wis exactly the activiation of\\nthat example, with no bias.\\nFrom here, you can start thinking about the role of the bias term.\\nPreviously, the threshold would be at zero. Any example with a\\nnegative projection onto wwould be classiﬁed negative; any exam-\\nple with a positive projection, positive. The bias simply moves this\\nthreshold. Now, after the projection is computed, bis added to get\\nthe overall activation. The projection plus b is then compared againstthe perceptron 47\\nzero.\\nFigure 4.8:perc:bias : perceptron\\npicture with biasThus, from a geometric perspective, the role of the bias is to shift\\nthe decision boundary away from the origin, in the direction of w. It\\nis shifted exactly\\x00bunits. So if bis positive, the boundary is shifted\\naway from wand if bis negative, the boundary is shifted toward w.\\nThis is shown in Figure 4.8. This makes intuitive sense: a positive\\nbias means that more examples should be classiﬁed positive. By\\nmoving the decision boundary in the negative direction, more space\\nyields a positive classiﬁcation.\\nThe decision boundary for a perceptron is a very magical thing. In\\nDdimensional space, it is always a D\\x001-dimensional hyperplane.\\n(In two dimensions, a 1-d hyperplane is simply a line. In three di-\\nmensions, a 2-d hyperplane is like a sheet of paper.) This hyperplane\\ndivides space in half. In the rest of this book, we’ll refer to the weight\\nvector, and to hyperplane it deﬁnes, interchangeably.\\nFigure 4.9: perceptron picture with\\nupdate, no biasThe perceptron update can also be considered geometrically. (For\\nsimplicity, we will consider the unbiased case.) Consider the sit-\\nuation in Figure 4.9. Here, we have a current guess as to the hy-\\nperplane, and positive training example comes in that is currently\\nmis-classiﬁed. The weights are updated: w w+yx. This yields the\\nnew weight vector, also shown in the Figure. In this case, the weight\\nvector changed enough that this training example is now correctly\\nclassiﬁed.\\n4.4Interpreting Perceptron Weights\\nYou may ﬁnd yourself having run the perceptron, learned a really\\nawesome classiﬁer, and then wondering “what the heck is this clas-\\nsiﬁer doing?” You might ask this question because you’re curious to\\nlearn something about the underlying data. You might ask this ques-\\ntion because you want to make sure that the perceptron is learning\\n“the right thing.” You might ask this question because you want to\\nremove a bunch of features that aren’t very useful because they’re\\nexpensive to compute or take a lot of storage.\\nThe perceptron learns a classiﬁer of the form x7!sign(ådwdxd+b).\\nA reasonable question to ask is: how sensitive is the ﬁnal classiﬁca-\\ntion to small changes in some particular feature. We can answer this\\nquestion by taking a derivative. If we arbitrarily take the 7th fea-\\nture we can compute¶\\n¶x7(ådwdxd+b)=w7. This says: the rate at\\nwhich the activation changes as a function of the 7th feature is ex-\\nactly w7. This gives rise to a useful heuristic for interpreting percep-\\ntron weights: sort all the weights from largest (positive) to largest\\n(negative), and take the top ten and bottom ten . The top ten are the\\nfeatures that the perceptron is most sensitive to for making positive48 a course in machine learning\\npredictions. The bottom ten are the features that the perceptron is\\nmost sensitive to for making negative predictions.\\nThis heuristic is useful, especially when the inputs xconsist en-\\ntirely of binary values (like a bag of words representation). The\\nheuristic is less useful when the range of the individual features\\nvaries signiﬁcantly. The issue is that if you have one feat x5that’s ei-\\nther 0 or 1, and another feature x7that’s either 0 or 100, but w5=w7,\\nit’s reasonable to say that w7is more important because it is likely to\\nhave a much larger inﬂuence on the ﬁnal prediction. The easiest way\\nto compensate for this is simply to scale your features ahead of time:\\nthis is another reason why feature scaling is a useful preprocessing\\nstep.\\n4.5Perceptron Convergence and Linear Separability\\nYou already have an intuitive feeling for why the perceptron works:\\nit moves the decision boundary in the direction of the training exam-\\nples. A question you should be asking yourself is: does the percep-\\ntron converge? If so, what does it converge to? And how long does it\\ntake?\\nIt is easy to construct data sets on which the perceptron algorithm\\nwill never converge. In fact, consider the (very uninteresting) learn-\\ning problem with no features . You have a data set consisting of one\\npositive example and one negative example. Since there are no fea-\\ntures, the only thing the perceptron algorithm will ever do is adjust\\nthe bias. Given this data, you can run the perceptron for a bajillion\\niterations and it will never settle down. As long as the bias is non-\\nnegative, the negative example will cause it to decrease. As long as\\nit is non-positive, the positive example will cause it to increase. Ad\\ninﬁnitum. (Yes, this is a very contrived example.)\\nFigure 4.10: separable dataWhat does it mean for the perceptron to converge? It means that\\nit can make an entire pass through the training data without making\\nanymore updates. In other words, it has correctly classiﬁed every\\ntraining example. Geometrically, this means that it was found some\\nhyperplane that correctly segregates the data into positive and nega-\\ntive examples, like that shown in Figure 4.10.\\nFigure 4.11: inseparable dataIn this case, this data is linearly separable. This means that there\\nexists some hyperplane that puts all the positive examples on one side\\nand all the negative examples on the other side. If the training is not\\nlinearly separable, like that shown in Figure 4.11, then the perceptron\\nhas no hope of converging. It could never possibly classify each point\\ncorrectly.\\nThe somewhat surprising thing about the perceptron algorithm is\\nthat ifthe data is linearly separable, then it will converge to a weightthe perceptron 49\\nvector that separates the data. (And if the data is inseparable, then it\\nwill never converge.) This is great news. It means that the perceptron\\nconverges whenever it is even remotely possible to converge.\\nThe second question is: how long does it take to converge? By\\n“how long,” what we really mean is “how many updates?” As is the\\ncase for much learning theory, you will not be able to get an answer\\nof the form “it will converge after 5293 updates.” This is asking too\\nmuch. The sort of answer we can hope to get is of the form “it will\\nconverge after at most 5293 updates.”\\nWhat you might expect to see is that the perceptron will con-\\nverge more quickly for easy learning problems than for hard learning\\nproblems. This certainly ﬁts intuition. The question is how to deﬁne\\n“easy” and “hard” in a meaningful way. One way to make this def-\\ninition is through the notion of margin. If I give you a data set and\\nhyperplane that separates itthen the margin is the distance between\\nthe hyperplane and the nearest point. Intuitively, problems with large\\nmargins should be easy (there’s lots of “wiggle room” to ﬁnd a sepa-\\nrating hyperplane); and problems with small margins should be hard\\n(you really have to get a very speciﬁc well tuned weight vector).\\nFormally, given a data set D, a weight vector wand bias b, the\\nmargin of w,bonDis deﬁned as:\\nmargin (D,w,b) =(\\nmin(x,y)2Dy\\x00\\nw\\x01x+b\\x01\\nifwseparates D\\n\\x00¥ otherwise(4.8)\\nIn words, the margin is only deﬁned if w,bactually separate the data\\n(otherwise it is just \\x00¥). In the case that it separates the data, we\\nﬁnd the point with the minimum activation, after the activation is\\nmultiplied by the label. So long as the margin is not \\x00¥,\\nit is always positive. Geometrically\\nthis makes sense, but why does\\nEq (4.8) yield this??For some historical reason (that is unknown to the author), mar-\\ngins are always denoted by the Greek letter g(gamma). One often\\ntalks about the marginofadata set. The margin of a data set is the\\nlargest attainable margin on this data. Formally:\\nmargin (D) =sup\\nw,bmargin (D,w,b) (4.9)\\nIn words, to compute the margin of a data set, you “try” every possi-\\nblew,bpair. For each pair, you compute its margin. We then take the\\nlargest of these as the overall margin of the data.1If the data is not1You can read “sup” as “max” if you\\nlike: the only difference is a technical\\ndifference in how the \\x00¥case is\\nhandled.linearly separable, then the value of the sup, and therefore the value\\nof the margin, is\\x00¥.\\nThere is a famous theorem due to Rosenblatt2that shows that the2Rosenblatt 1958\\nnumber of errors that the perceptron algorithm makes is bounded by\\ng\\x002. More formally:50 a course in machine learning\\nTheorem 2(Perceptron Convergence Theorem) .Suppose the perceptron\\nalgorithm is run on a linearly separable data set Dwith margin g>0.\\nAssume thatjjxjj\\x141for all x2D. Then the algorithm will converge after\\nat most1\\ng2updates.\\nThe proof of this theorem is elementary, in the sense that it does\\nnot use any fancy tricks: it’s all just algebra. The idea behind the\\nproof is as follows. If the data is linearly separable with margin g,\\nthen there exists some weight vector w\\x03that achieves this margin.\\nObviously we don’t know what w\\x03is, but we know it exists. The\\nperceptron algorithm is trying to ﬁnd a weight vector wthat points\\nroughly in the same direction as w\\x03. (For large g, “roughly” can be\\nvery rough. For small g, “roughly” is quite precise.) Every time the\\nperceptron makes an update, the angle between wand w\\x03changes.\\nWhat we prove is that the angle actually decreases. We show this in\\ntwo steps. First, the dot product w\\x01w\\x03increases a lot. Second, the\\nnormjjwjjdoes not increase very much. Since the dot product is\\nincreasing, but wisn’t getting too long, the angle between them has\\nto be shrinking. The rest is algebra.\\nProof of Theorem 2.The margin g>0 must be realized by some set\\nof parameters, say x\\x03. Suppose we train a perceptron on this data.\\nDenote by w(0)the initial weight vector, w(1)the weight vector after\\ntheﬁrst update , and w(k)the weight vector after the kth update. (We\\nare essentially ignoring data points on which the perceptron doesn’t\\nupdate itself.) First, we will show that w\\x03\\x01w(k)grows quicky as\\na function of k. Second, we will show that\\x0c\\x0c\\x0c\\x0cw(k)\\x0c\\x0c\\x0c\\x0cdoes not grow\\nquickly.\\nFirst, suppose that the kth update happens on example (x,y). We\\nare trying to show that w(k)is becoming aligned with w\\x03. Because we\\nupdated, know that this example was misclassiﬁed: yw(k-1)\\x01x<0.\\nAfter the update, we get w(k)=w(k-1)+yx. We do a little computa-\\ntion:\\nw\\x03\\x01w(k)=w\\x03\\x01\\x10\\nw(k-1)+yx\\x11\\ndeﬁnition of w(k)(4.10)\\n=w\\x03\\x01w(k-1)+yw\\x03\\x01x vector algebra (4.11)\\n\\x15w\\x03\\x01w(k-1)+g w\\x03has margin g (4.12)\\nThus, every time w(k)is updated, its projection onto w\\x03increases by\\nat least g. Therefore: w\\x03\\x01w(k)\\x15kg.\\nNext, we need to show that the increase of galong w\\x03occurs\\nbecause w(k)is getting closer to w\\x03, not just because it’s getting ex-\\nceptionally long. To do this, we compute the norm of w(k):\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cw(k)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2the perceptron 51\\n=\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cw(k-1)+yx\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\ndef. of w(k)(4.13)\\n=\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cw(k-1)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n+y2jjxjj2+2yw(k-1)\\x01x quadratic rule (4.14)\\n\\x14\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cw(k-1)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n+1+0 assumption and a<0 (4.15)\\nThus, the squared norm of w(k)increases by at most one every up-\\ndate. Therefore:\\x0c\\x0c\\x0c\\x0cw(k)\\x0c\\x0c\\x0c\\x0c2\\x14k.\\nNow we put together the two things we have learned before. By\\nour ﬁrst conclusion, we know w\\x03\\x01w(k)\\x15kg. But our second con-\\nclusion,p\\nk\\x15\\x0c\\x0c\\x0c\\x0cw(k)\\x0c\\x0c\\x0c\\x0c2. Finally, because w\\x03is a unit vector, we know\\nthat\\x0c\\x0c\\x0c\\x0cw(k)\\x0c\\x0c\\x0c\\x0c\\x15w\\x03\\x01w(k). Putting this together, we have:\\np\\nk\\x15\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cw(k)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x15 w\\x03\\x01w(k)\\x15 kg (4.16)\\nTaking the left-most and right-most terms, we get thatp\\nk\\x15kg.\\nDividing both sides by k, we get1p\\nk\\x15gand therefore k\\x141\\ng2.\\nThis means that once we’ve made1\\ng2updates, we cannot make any\\nmore!\\nPerhaps we don’t want to assume\\nthat all xhave norm at most 1. If\\nthey have all have norm at most\\nR, you can achieve a very simi-\\nlar bound. Modify the perceptron\\nconvergence proof to handle this\\ncase.?It is important to keep in mind what this proof shows and what\\nit does not show. It shows that if I give the perceptron data that\\nis linearly separable with margin g>0, then the perceptron will\\nconverge to a solution that separates the data. And it will converge\\nquickly when gis large. It does not say anything about the solution,\\nother than the fact that it separates the data. In particular, the proof\\nmakes use of the maximum margin separator. But the perceptron\\nis not guaranteed to ﬁnd this maximum margin separator. The data\\nmay be separable with margin 0.9 and the perceptron might still\\nﬁnd a separating hyperplane with a margin of only 0.000001. Later\\n(in Chapter 7), we will see algorithms that explicitly try to ﬁnd the\\nmaximum margin solution. Why does the perceptron conver-\\ngence bound not contradict the\\nearlier claim that poorly ordered\\ndata points (e.g., all positives fol-\\nlowed by all negatives) will cause\\nthe perceptron to take an astronom-\\nically long time to learn?? 4.6Improved Generalization: Voting and Averaging\\nIn the beginning of this chapter, there was a comment that the per-\\nceptron works amazingly well. This was a half-truth. The “vanilla”\\nperceptron algorithm does well, but not amazingly well. In order to\\nmake it more competitive with other learning algorithms, you need\\nto modify it a bit to get better generalization. The key issue with the\\nvanilla perceptron is that it counts later points more than it counts earlier\\npoints .\\nTo see why, consider a data set with 10, 000 examples. Suppose\\nthat after the ﬁrst 100 examples, the perceptron has learned a really52 a course in machine learning\\ngood classiﬁer. It’s so good that it goes over the next 9899 exam-\\nples without making anyupdates. It reaches the 10, 000th example\\nand makes an error. It updates. For all we know, the update on this\\n10, 000th example completely ruins the weight vector that has done so\\nwell on 99.99% of the data!\\nWhat we would like is for weight vectors that “survive” a long\\ntime to get more say than weight vectors that are overthrown quickly.\\nOne way to achieve this is by voting. As the perceptron learns, it\\nremembers how long each hyperplane survives. At test time, each\\nhyperplane encountered during training “votes” on the class of a test\\nexample. If a particular hyperplane survived for 20 examples, then\\nit gets a vote of 20. If it only survived for one example, it only gets a\\nvote of 1. In particular, let (w,b)(1), . . . ,(w,b)(K)be the K+1 weight\\nvectors encountered during training, and c(1), . . . , c(K)be the survival\\ntimes for each of these weight vectors. (A weight vector that gets\\nimmediately updated gets c=1; one that survives another round\\ngets c=2 and so on.) Then the prediction on a test point is:\\nˆy=sign \\nK\\nå\\nk=1c(k)sign\\x10\\nw(k)\\x01ˆx+b(k)\\x11!\\n(4.17)\\nThis algorithm, known as the voted perceptron works quite well in\\npractice, and there is some nice theory showing that it is guaranteed\\nto generalize better than the vanilla perceptron. Unfortunately, it is\\nalso completely impractical. If there are 1000 updates made during\\nperceptron learning, the voted perceptron requires that you store\\n1000 weight vectors, together with their counts. This requires an\\nabsurd amount of storage, and makes prediction 1000 times slower\\nthan the vanilla perceptron.The training algorithm for the voted\\nperceptron is the same as the\\nvanilla perceptron. In particular,\\nin line 5of Algorithm 4.2, the ac-\\ntivation on a training example is\\ncomputed based on the current\\nweight vector , not based on the voted\\nprediction. Why??A much more practical alternative is the averaged perceptron .\\nThe idea is similar: you maintain a collection of weight vectors and\\nsurvival times. However, at test time, you predict according to the\\naverage weight vector, rather than the voting. In particular, the predic-\\ntion is:\\nˆy=sign \\nK\\nå\\nk=1c(k)\\x10\\nw(k)\\x01ˆx+b(k)\\x11!\\n(4.18)\\nThe only difference between the voted prediction, Eq ( 4.17), and the\\naveraged prediction, Eq ( 4.18), is the presense of the interior sign\\noperator. With a little bit of algebra, we can rewrite the test-time\\nprediction as:\\nˆy=sign  \\nK\\nå\\nk=1c(k)w(k)!\\n\\x01ˆx+K\\nå\\nk=1c(k)b(k)!\\n(4.19)\\nThe advantage of the averaged perceptron is that we can simply\\nmaintain a running sum of the averaged weight vector (the blue term)the perceptron 53\\nAlgorithm 7Averaged Perceptron Train (D,MaxIter )\\n1:w h0,0, . . .0i,b 0 // initialize weights and bias\\n2:u h0,0, . . .0i,b 0 // initialize cached weights and bias\\n3:c 1 // initialize example counter to one\\n4:foriter=1. . .MaxIter do\\n5:for all (x,y)2Ddo\\n6: ify(w\\x01x+b)\\x140then\\n7: w w+yx // update weights\\n8: b b+y // update bias\\n9: u u+y cx // update cached weights\\n10: b b+y c // update cached bias\\n11: end if\\n12: c c+1 // increment counter regardless of update\\n13:end for\\n14:end for\\n15:return w-1\\ncu,b-1\\ncb // return averaged weights and bias\\nand averaged bias (the red term). Test-time prediction is then just as\\nefﬁcient as it is with the vanilla perceptron.\\nThe full training algorithm for the averaged perceptron is shown\\nin Algorithm 4.6. Some of the notation is changed from the original\\nperceptron: namely, vector operations are written as vector opera-\\ntions, and the activation computation is folded into the error check-\\ning.\\nIt is probably not immediately apparent from Algorithm 4.6that\\nthe computation unfolding is precisely the calculation of the averaged\\nweights and bias. The most natural implementation would be to keep\\ntrack of an averaged weight vector u. At the end of every example,\\nyou would increase u u+w(and similarly for the bias). However,\\nsuch an implementation would require that you updated the aver-\\naged vector on every example, rather than just on the examples that\\nwere incorrectly classiﬁed! Since we hope that eventually the per-\\nceptron learns to do a good job, we would hope that it will not make\\nupdates on every example. So, ideally, you would like to only update\\nthe averaged weight vector when the actual weight vector changes.\\nThe slightly clever computation in Algorithm 4.6achieves this. By writing out the computation of\\nthe averaged weights from Eq ( 4.18)\\nas a telescoping sum, derive the\\ncomputation from Algorithm 4.6.?The averaged perceptron is almost always better than the percep-\\ntron, in the sense that it generalizes better to test data. However, that\\ndoes not free you from having to do early stop ping . It will, eventu-\\nally, overﬁt.\\n4.7Limitations of the Perceptron\\nAlthough the perceptron is very useful, it is fundamentally limited in\\na way that neither decision trees nor KNN are. Its limitation is that54 a course in machine learning\\nits decision boundaries can only be linear. The classic way of showing\\nthis limitation is through the XOR problem (XOR = exclusive or). The\\nXOR problem is shown graphically in Figure 4.12. It consists of four\\ndata points, each at a corner of the unit square. The labels for these\\npoints are the same, along the diagonals. You can try, but you will\\nnot be able to ﬁnd a linear decision boundary that perfectly separates\\nthese data points.\\nFigure 4.12: picture of xor problemOne question you might ask is: do XOR-like problems exist in\\nthe real world? Unfortunately for the perceptron, the answer is yes.\\nConsider a sentiment classiﬁcation problem that has three features\\nthat simply say whether a given word is contained in a review of\\na course. These features are: excellent ,terrible andnot . The\\nexcellent feature is indicative of positive reviews and the terrible\\nfeature is indicative of negative reviews. But in the presence of the\\nnot feature, this categorization ﬂips.\\nOne way to address this problem is by adding feature com bina-\\ntions . We could add two additional features: excellent -and -not\\nandterrible -and -not that indicate a conjunction of these base\\nfeatures. By assigning weights as follows, you can achieve the desired\\neffect:\\nwexecellent = + 1 wterrible =\\x001 wnot=0\\nwexecllent -and -not=\\x002 wterrible -and -not= + 2\\nIn this particular case, we have addressed the problem. However, if\\nwe start with D-many features, if we want to add all pairs, we’ll blow\\nup to (D\\n2)=O(D2)features through this feature map ping . And\\nthere’s no guarantee that pairs of features is enough. We might need\\ntriples of features, and now we’re up to (D\\n3)=O(D2)features. These\\nadditional features will drastically increase computation and will\\noften result in a stronger propensity to overﬁtting. Suppose that you took the XOR\\nproblem and added one new fea-\\nture: x3=x1^x2(the logical and\\nof the two existing features). Write\\nout feature weights and a bias that\\nwould achieve perfect classiﬁcation\\non this data.?In fact, the “XOR problem” is so signiﬁcant that it basically killed\\nresearch in classiﬁers with linear decision boundaries for a decade\\nor two. Later in this book, we will see two alternative approaches to\\ntaking key ideas from the perceptron and generating classiﬁers with\\nnon-linear decision boundaries. One approach is to combine multi-\\nple perceptrons in a single framework: this is the neuralnetworks\\napproach (see Chapter 10). The second approach is to ﬁnd computa-\\ntionally efﬁcient ways of doing feature mapping in a computationally\\nand statistically efﬁcient way: this is the kernels approach (see Chap-\\nter11).\\n4.8Further Reading\\nTODO further reading5 | P RACTICAL ISSUES\\nDependencies: Chapter 1,Chap-\\nter3,Chapter 4At this point , you have seen three qualitatively different models\\nfor learning: decision trees, nearest neighbors, and perceptrons. You\\nhave also learned about clustering with the K-means algorithm. You\\nwill shortly learn about more complex models, most of which are\\nvariants on things you already know. However, before attempting\\nto understand more complex models of learning, it is important to\\nhave a ﬁrm grasp on how to use machine learning in practice. This\\nchapter is all about how to go from an abstract learning problem\\nto a concrete implementation. You will see some examples of “best\\npractices” along with justiﬁcations of these practices.\\nIn many ways, going from an abstract problem to a concrete learn-\\ning task is more of an art than a science. However, this art can have\\na huge impact on the practical performance of learning systems. In\\nmany cases, moving to a more complicated learning algorithm will\\ngain you a few percent improvement. Going to a better representa-\\ntion will gain you an order of magnitude improvement. To this end,\\nwe will discuss several high level ideas to help you develop a better\\nartistic sensibility.\\n5.1The Importance of Good Features\\nMachine learning is magical. You give it data and it manages to\\nclassify that data. For many, it can actually outperform a human! But,\\nlike so many problems in the world, there is a signiﬁcant “garbage\\nin, garbage out” aspect to machine learning. If the data you give it is\\ntrash, the learning algorithm is unlikely to be able to overcome it.\\nConsider a problem of object recognition from images. If you start\\nwith a 100\\x02100 pixel image, a very easy feature representation of\\nthis image is as a 30, 000 dimensional vector, where each dimension\\ncorresponds to the red, green or blue component of some pixel in\\nthe image. So perhaps feature 1is the amount of red in pixel (1, 1);\\nfeature 2is the amount of green in that pixel; and so on. This is the\\npixel representation of images.\\nFigure 5.1: object recognition in pixelsLearning Objectives:\\n• Translate between a problem de-\\nscription and a concrete learning\\nproblem.\\n• Perform basic feature engineering on\\nimage and text data.\\n• Explain how to use cross-validation\\nto tune hyperparameters and esti-\\nmate future performance.\\n• Compare and contrast the differ-\\nences between several evaluation\\nmetrics.\\n• Explain why feature combinations\\nare important for learning with\\nsome models but not others.\\n• Explain the relationship between the\\nthree learning techniques you have\\nseen so far.\\n• Apply several debugging techniques\\nto learning algorithms.A ship in port is safe, but that is not what ships are for. –\\nGrace Hopper56 a course in machine learning\\nOne thing to keep in mind is that the pixel representation throws\\naway all locality information in the image. Learning algorithms don’t\\ncare about features: they only care about feature values. So I can\\npermute all of the features, with no effect on the learning algorithm\\n(so long as I apply the same permutation to all training and test\\nexamples). Figure 5.1shows some images whose pixels have been\\nrandomly permuted (in this case only the pixels are permuted, not\\nthe colors). All of these objects are things that you’ve seen plenty of\\nexamples of; can you identify them? Should you expect a machine to\\nbe able to?\\nFigure 5.2: object recognition in patches\\nFigure 5.3: object recognition in shapesAn alternative representation of images is the patch represen-\\ntation, where the unit of interest is a small rectangular block of an\\nimage, rather than a single pixel. Again, permuting the patches has\\nno effect on the classiﬁer. Figure 5.2shows the same images in patch\\nrepresentation. Can you identify them? A ﬁnal representation is a\\nshape representation. Here, we throw out all color and pixel infor-\\nmation and simply provide a bounding polygon. Figure 5.3shows\\nthe same images in this representation. Is this now enough to iden-\\ntify them? (If not, you can ﬁnd the answers in Figure 5.15at the end\\nof the chapter.)\\ndata learning\\ntraining set\\npredict fea-\\nture function\\ntest machine\\nloss alice tree\\nguess features\\nalgorithmdata knn\\ndimensions\\npoints fea-\\nture ﬁgure\\ndecision fea-\\ntures point\\nﬁg training\\nset space\\nexamples\\nTable 5.1: Bag of (most frequent) words\\nrepresentation for the Decision Tree\\nand KNN chapters of this book, after\\ndropping high frequency words like\\n“the”.\\nFigure 5.4:prac:bow : BOW repr of one\\npositive and one negative reviewIn the context of text categorization (for instance, the sentiment\\nrecognition task), one standard representation is the bag ofwords\\nrepresentation. Here, we have one feature for each unique word that\\nappears in a document. For the feature happy , the feature value is\\nthe number of times that the word “happy” appears in the document.\\nThe bag of words (BOW) representation throws away all position\\ninformation. Table 5.1shows a BOW representation for two chapters\\nof this book. Can you tell which is which?\\n5.2Irrelevant and Redundant Features\\nOne big difference between learning models is how robust they are to\\nthe addition of noisy or irrelevant features. Intuitively, an irrelevant\\nfeature is one that is completely uncorrelated with the prediction\\ntask. A feature fwhose expectation does not depend on the label\\nE[fjY] =E[f]might be irrelevant. For instance, the presence of\\nthe word “the” might be largely irrelevant for predicting whether a\\ncourse review is positive or negative.\\nA secondary issue is how well these algorithms deal with redun-\\ndant features . Two features are redundant if they are highly cor-\\nrelated, regardless of whether they are correlated with the task or\\nnot. For example, having a bright red pixel in an image at position\\n(20, 93 )is probably highly redundant with having a bright red pixel\\nat position (21, 93 ). Both might be useful (e.g., for identifying ﬁre hy-practical issues 57\\ndrants), but because of how images are structured, these two features\\nare likely to co-occur frequently.Is it possible to have a feature f\\nwhose expectation does not depend\\non the label, but is nevertheless still\\nuseful for prediction??When thinking about robustness to irrelevant or redundant fea-\\ntures, it is usually not worthwhile thinking of the case where one has\\n999 great features and 1 bad feature. The interesting case is when the\\nbad features outnumber the good features, and often outnumber by a\\nlarge degree. The question is how robust are algorithms in this case.1\\n1You might think it’s absurd to have\\nso many irrelevant features, but the\\ncases you’ve seen so far (bag of words,\\nbag of pixels) are both reasonable\\nexamples of this! How many words,\\nout of the entire English vocabulary\\n(roughly 10, 000\\x00100, 000 words), are\\nactually useful for predicting positive\\nand negative course reviews?For shallow decision trees , the model explicitly selects features\\nthat are highly correlated with the label. In particular, by limiting the\\ndepth of the decision tree, one can at least hope that the model will be\\nable to throw away irrelevant features. Redundant features are almost\\ncertainly thrown out: once you select one feature, the second feature\\nnow looks mostly useless. The only possible issue with irrelevant\\nfeatures is that even though they’re irrelevant, they happen to correlate\\nwith the class label on the training data, but chance.\\nAs a thought experiment, suppose that we have Ntraining ex-\\namples, and exactly half are positive examples and half are negative\\nexamples. Suppose there’s some binary feature, f, that is completely\\nuncorrelated with the label. This feature has a 50/50chance of ap-\\npearing in any example, regardless of the label. In principle, the deci-\\nsion tree should notselect this feature. But, by chance, especially if N\\nis small, the feature might look correlated with the label. This is anal-\\nogous to ﬂipping two coins simultaneously Ntimes. Even though the\\ncoins are independent, it’s entirely possible that you will observe a\\nsequence like (H,H),(T,T),(H,H),(H,H), which makes them look\\nentirely correlated! The hope is that as Ngrows, this becomes less\\nand less likely. In fact, we can explicitly compute how likely this is to\\nhappen.\\nTo do this, let’s ﬁx the sequence of Nlabels. We now ﬂip a coin N\\ntimes and consider how likely it is that it exactly matches the label.\\nThis is easy: the probability is 0.5N. Now, we would also be confused\\nif it exactly matched notthe label, which has the same probability. So\\nthe chance that it looks perfectly correlated is 0.5N+0.5N=0.5N\\x001.\\nThankfully, this shrinks down very small (e.g., 10\\x006) after only 21\\ndata points, meaning that even with a very small training set, the\\nchance that a random feature happens to correlate exactly with the\\nlabel is tiny.\\nThis makes us happy. The problem is that we don’t have one irrel-\\nevant feature: we have many! If we randomly pick two irrelevant fea-\\ntures, each has the same probability of perfectly correlating: 0.5N\\x001.\\nBut since there are two and they’re independent coins, the chance\\nthat either correlates perfectly is 2 \\x020.5N\\x001=0.5N\\x002. In general,\\nif we have Kirrelevant features, all of which are random indepen-\\ndent coins, the chance that at least one of them perfectly correlates is58 a course in machine learning\\n0.5N\\x00K. This suggests that if we have a sizeable number Kof irrele-\\nvant features, we’d better have at least K +21 training examples.\\nUnfortunately, the situation is actually worse than this. In the\\nabove analysis we only considered the case of perfect correlation. We\\ncould also consider the case of partial correlation, which would yield\\neven higher probabilities. Sufﬁce it to say that even decision trees can\\nbecome confused.\\nFigure 5.5:prac:addirrel : data from\\nhigh dimensional warning, interpolatedIn the case of K-near estneigh bors , the situation is perhaps more\\ndire. Since KNN weighs each feature just as much as another feature,\\nthe introduction of irrelevant features can completely mess up KNN\\nprediction. In fact, as you saw, in high dimensional space, randomly\\ndistributed points all look approximately the same distance apart. If\\nwe add lots and lots of randomly distributed features to a data set,\\nthen all distances still converge.\\nIn the case of the perceptron , one can hope that it might learn to\\nassign zero weight to irrelevant features. For instance, consider a\\nbinary feature is randomly one or zero independent of the label. If\\nthe perceptron makes just as many updates for positive examples\\nas for negative examples, there is a reasonable chance this feature\\nweight will be zero. At the very least, it should be small. What happens with the perceptron\\nwith truly redundant features (i.e.,\\none is literally a copy of the other)??\\n5.3Feature Pruning and Normalization\\nIn text categorization problems, some words simply do not appear\\nvery often. Perhaps the word “groovy”2appears in exactly one train-\\n2This is typically positive indicator,\\nor at least it was back in the US in the\\n1970 s.ing document, which is positive. Is it really worth keeping this word\\naround as a feature? It’s a dangerous endeavor because it’s hard to\\ntell with just one training example if it is really correlated with the\\npositive class, or is it just noise. You could hope that your learning\\nalgorithm is smart enough to ﬁgure it out. Or you could just remove\\nit. That means that (a) the learning algorithm won’t have to ﬁgure it\\nout, and (b) you’ve reduced the number of dimensions you have, so\\nthings should be more efﬁcient, and less “scary.”\\nFigure 5.6:prac:pruning : effect of\\npruning on text dataThis idea of feature pruning is very useful and applied in many\\napplications. It is easiest in the case of binary features. If a binary\\nfeature only appears some small number Ktimes (in the training\\ndata: no fair looking at the test data!), you simply remove it from\\nconsideration. (You might also want to remove features that appear\\nin all-but- Kmany documents, for instance the word “the” appears in\\npretty much every English document ever written.) Typical choices\\nforKare 1, 2, 5, 10, 20, 50, mostly depending on the size of the data.\\nOn a text data set with 1000 documents, a cutoff of 5 is probably\\nreasonable. On a text data set the size of the web, a cut of 50 or even\\n100 or 200 is probably reasonable3. Figure 5.6shows the effect of3According to Google, the following\\nwords (among many others) appear\\n200 times on the web: moudlings, agag-\\ngagctg, setgravity, rogov, prosomeric,\\nspunlaid, piyushtwok, telelesson, nes-\\nmysl, brighnasa. For comparison, the\\nword “the” appears 19, 401, 194, 714 ( 19\\nbillion) times.practical issues 59\\nWe often need to discuss various statistics of a data set. Most often, it is enough to consider univariate\\n(one-dimensional) data. Suppose we have Nreal valued numbers z1,z2, . . . , zN. The samplemean (or\\njust mean ) of these numbers is just their average value, or expected value: m=1\\nNånzn. The sample\\nvariance (or just variance ) measures how much they vary around their mean: s2=1\\nN\\x001ån(zn\\x00m)2,\\nwhere mis the sample mean.\\nThe mean and variance have convenient interpretations in terms of prediction. Suppose we wanted\\nto choose a single constant value to “predict” the next z, and were minimizing squared error. Call\\nthis constant value a. Then a= argmina2R1\\n2ån(a\\x00zn)2. (Here, the1\\n2is for convenience and does\\nnot change the answer.) To solve for a, we can take derivatives and set to zero:¶\\n¶a1\\n2ån(a\\x00zn)2=\\nån(a\\x00zn) = Na\\x00ånzn; therefore Na=ånznand a=m. This means that the sample mean is\\nthe number that minimizes squared error to the sample. Moreover, the variance is propotional to the\\nsquared error of that “predictor.”MATHREVIEW | DATASTATISTICS : MEANS AND VARIANCES\\nFigure 5.7:\\npruning on a sentiment analysis task. In the beginning, pruning does\\nnot hurt (and sometimes helps!) but eventually we prune away all the\\ninteresting words and performance suffers.\\nFigure 5.8:prac:variance : effect of\\npruning on visionIn the case of real-valued features, the question is how to extend\\nthe idea of “does not occur much” to real values. A reasonable def-\\ninition is to look for features with low variance . In fact, for binary\\nfeatures, ones that almost never appear or almost always appear will\\nalso have low variance. Figure 5.8shows the result of pruning low-\\nvariance features on the digit recognition task. Again, at ﬁrst pruning\\ndoes not hurt (and sometimes helps!) but eventually we have thrown\\nout all the useful features.\\nEarlier we discussed the problem\\nofscale of features (e.g., millimeters\\nversus centimeters). Does this have\\nan impact on variance-based feature\\npruning??It is often useful to normalizethe data so that it is consistent in\\nsome way. There are two basic types of normalization: feature nor-\\nmalization and examplenormalization. In feature normalization,\\nyou go through each feature and adjust it the same way across all\\nexamples. In example normalization, each example is adjusted indi-\\nvidually.\\nFigure 5.9:prac:transform : picture\\nof centering, scaling by variance and\\nscaling by absolute valueThe goal of both types of normalization is to make it easier for your\\nlearning algorithm to learn. In feature normalization, there are two\\nstandard things to do:\\n1. Centering: moving the entire data set so that it is centered around\\nthe origin.\\n2. Scaling: rescaling each feature so that one of the following holds:\\n(a) Each feature has variance 1 across the training data.60 a course in machine learning\\n(b) Each feature has maximum absolute value 1 across the train-\\ning data.\\nThese transformations are shown geometrically in Figure 5.9. The\\ngoal of centering is to make sure that no features are arbitrarily large.\\nThe goal of scaling is to make sure that all features have roughly the\\nsame scale (to avoid the issue of centimeters versus millimeters). For the three models you know\\nabout (KNN, DT, Perceptron),\\nwhich are most sensitive to center-\\ning? Which are most sensitive to\\nscaling??These computations are fairly straightforward. Here, xn,drefers\\nto the dth feature of example n. Since it is very rare to apply scaling\\nwithout previously applying centering, the expressions below for\\nscaling assume that the data is already centered.\\nCentering: xn,d xn,d\\x00md (5.1)\\nVariance Scaling: xn,d xn,d/sd (5.2)\\nAbsolute Scaling: xn,d xn,d/rd (5.3)\\nwhere: md=1\\nNå\\nnxn,d (5.4)\\nsd=s\\n1\\nN\\x001å\\nn(xn,d\\x00md)2 (5.5)\\nrd=max\\nn\\x0c\\x0cxn,d\\x0c\\x0c (5.6)\\nIn practice, if the dynamic range of your features is already some\\nsubset of [\\x002, 2]or[\\x003, 3], then it is probably not worth the effort of\\ncentering and scaling. (It’s an effort because you have to keep around\\nyour centering and scaling calculations so that you can apply them\\nto the test data as well!) However, if some of your features are orders\\nof magnitude larger than others, it might be helpful. Remember that\\nyou might know best: if the difference in scale is actually signiﬁcant\\nfor your problem, then rescaling might throw away useful informa-\\ntion.\\nOne thing to be wary of is centering binary data. In many cases,\\nbinary data is very sparse : for a given example, only a few of the\\nfeatures are “on.” For instance, out of a vocabulary of 10, 000 or\\n100, 000 words, a given document probably only contains about 100.\\nFrom a storage and computation perspective, this is very useful.\\nHowever, after centering, the data will no longer be sparse and you\\nwill pay dearly with outrageously slow implementations.\\nFigure 5.10:prac:exnorm : example of\\nexample normalizationInexamplenormalization, you view examples one at a time. The\\nmost standard normalization is to ensure that the length of each\\nexample vector is one: namely, each example lies somewhere on the\\nunit hypersphere. This is a simple transformation:\\nExample Normalization: xn xn/jjxnjj (5.7)\\nThis transformation is depicted in Figure 5.10.practical issues 61\\nThe main advantage to example normalization is that it makes\\ncomparisons more straightforward across data sets. If I hand you\\ntwo data sets that differ only in the norm of the feature vectors (i.e.,\\none is just a scaled version of the other), it is difﬁcult to compare the\\nlearned models. Example normalization makes this more straightfor-\\nward. Moreover, as you saw in the perceptron convergence proof, it is\\noften just mathematically easier to assume normalized data.\\n5.4Combinatorial Feature Explosion\\nYou learned in Chapter 4that linear models (like the perceptron)\\ncannot solve the XOR problem. You also learned that by performing\\na combinatorial feature explosion, they could. But that came at the\\ncomputational expense of gigantic feature vectors.\\nOf the algorithms that you’ve seen so far, the perceptron is the one\\nthat has the most to gain by feature combination. And the decision\\ntree is the one that has the least to gain. In fact, the decision tree\\nconstruction is essentially building meta features for you. (Or, at\\nleast, it is building meta features constructed purely through “logical\\nands.”)\\nFigure 5.11:prac:dttoperc : turning a\\nDT into a set of meta featuresThis observation leads to a heuristic for constructing meta features\\nforperceptrons from decision trees. The idea is to train a decision\\ntree on the training data. From that decision tree, you can extract\\nmeta features by looking at feature combinations along branches. You\\ncan then add only those feature combinations as meta features to the\\nfeature set for the perceptron. Figure 5.11shows a small decision tree\\nand a set of meta features that you might extract from it. There is a\\nhyperparameter here of what length paths to extract from the tree: in\\nthis case, only paths of length two are extracted. For bigger trees, or\\nif you have more data, you might beneﬁt from longer paths.\\nFigure 5.12:prac:log : performance on\\ntext categ with word counts versus log\\nword countsIn addition to combinatorial transformations, the logarithmic\\ntrans formation can be quite useful in practice. It seems like a strange\\nthing to be useful, since it doesn’t seem to fundamentally change\\nthe data. However, since many learning algorithms operate by linear\\noperations on the features (both perceptron and KNN do this), the\\nlog-transform is a way to get product-like operations. The question is\\nwhich of the following feels more applicable to your data: ( 1) every\\ntime this feature increases by one, I’m equally more likely to predict\\na positive label; ( 2) every time this feature doubles, I’m equally more\\nlike to predict a positive label. In the ﬁrst case, you should stick\\nwith linear features and in the second case you should switch to\\na log-transform. This is an important transformation in text data,\\nwhere the presence of the word “excellent” once is a good indicator\\nof a positive review; seeing “excellent” twice is a better indicator;62 a course in machine learning\\nbut the difference between seeing “excellent” 10times and seeing it\\n11times really isn’t a big deal any more. A log-transform achieves\\nthis. Experimentally, you can see the difference in test performance\\nbetween word count data and log-word count data in Figure 5.12.\\nHere, the transformation is actually xd7!log2(xd+1)to ensure that\\nzeros remain zero and sparsity is retained. In the case that feature\\nvalues can also be negative, the slightly more complex mapping\\nxd7!log2(jxdj+1)sign(xd), where sign (xd)denotes the sign of xd.\\n5.5Evaluating Model Performance\\nSo far, our focus has been on classiﬁers that achieve high accuracy .\\nIn some cases, this is not what you might want. For instance, if you\\nare trying to predict whether a patient has cancer or not, it might be\\nbetter to err on one side (saying they have cancer when they don’t)\\nthan the other (because then they die). Similarly, letting a little spam\\nslip through might be better than accidentally blocking one email\\nfrom your boss.\\nThere are two major types of binary classiﬁcation problems. One\\nis “X versus Y.” For instance, positive versus negative sentiment.\\nAnother is “X versus not-X.” For instance, spam versus non-spam.\\n(The argument being that there are lots of types of non-spam.) Or\\nin the context of web search, relevant document versus irrelevant\\ndocument. This is a subtle and subjective decision. But “X versus not-\\nX” problems often have more of the feel of “X spotting” rather than\\na true distinction between X and Y. (Can you spot the spam? can you\\nspot the relevant documents?)\\nFor spotting problems (X versus not-X), there are often more ap-\\npropriate success metrics than accuracy. A very popular one from\\ninformation retrieval is the precision /recall metric. Precision asks\\nthe question: of all the X’s that you found, how many of them were\\nactually X’s? Recall asks: of all the X’s that were out there, how many\\nof them did you ﬁnd?4Formally, precision and recall are deﬁned as:4A colleague make the analogy to the\\nUS court system’s saying “Do you\\npromise to tell the whole truth and\\nnothing but the truth?” In this case, the\\n“whole truth” means high recall and\\n“nothing but the truth” means high\\nprecision.”P=I\\nS(5.8)\\nR=I\\nT(5.9)\\nS=number of Xs that your system found ( 5.10)\\nT=number of Xs in the data ( 5.11)\\nI=number of correct Xs that your system found ( 5.12)\\nHere, Sis mnemonic for “System,” Tis mnemonic for “Truth” and I\\nis mnemonic for “Intersection.” It is generally accepted that 0/0 =1\\nin these deﬁnitions. Thus, if you system found nothing, your preci-practical issues 63\\nsion is always perfect; and if there is nothing to ﬁnd, your recall is\\nalways perfect.\\nFigure 5.13:prac:spam : show a bunch\\nof emails spam/nospam sorted by\\nmodel predicion, not perfectOnce you can compute precision and recall, you are often able to\\nproduce precision/recall curves . Suppose that you are attempting\\nto identify spam. You run a learning algorithm to make predictions\\non a test set. But instead of just taking a “yes/no” answer, you allow\\nyour algorithm to produce its conﬁdence. For instance, in perceptron,\\nyou might use the distance from the hyperplane as a conﬁdence\\nmeasure. You can then sort all of your test emails according to this\\nranking. You may put the most spam-like emails at the top and the\\nleast spam-like emails at the bottom, like in Figure 5.13.\\nHow would you get a conﬁdence\\nout of a decision tree or KNN??\\nFigure 5.14:prac:prcurve : precision\\nrecall curveOnce you have this sorted list, you can choose how aggressively\\nyou want your spam ﬁlter to be by setting a threshold anywhere on\\nthis list. One would hope that if you set the threshold very high, you\\nare likely to have high precision (but low recall). If you set the thresh-\\nold very low, you’ll have high recall (but low precision). By consider-\\ningevery possible place you could put this threshold, you can trace out\\na curve of precision/recall values, like the one in Figure 5.14. This\\nallows us to ask the question: for some ﬁxed precision, what sort of\\nrecall can I get. Obviously, the closer your curve is to the upper-right\\ncorner, the better. And when comparing learning algorithms A and\\nB you can say that A dom inates B if A’s precision/recall curve is\\nalways higher than B’s.\\n0.0 0 .2 0 .4 0 .6 0 .8 1 .0\\n0.00.000.000.000.000.00 0.00\\n0.20.000.200.260.300.32 0.33\\n0.40.000.260.400.480.53 0.57\\n0.60.000.300.480.600.68 0.74\\n0.80.000.320.530.680.80 0.88\\n1.00.000.330.570.740.881.00\\nTable 5.2: Table of f-measures when\\nvarying precision and recall values.Precision/recall curves are nice because they allow us to visualize\\nmany ways in which we could use the system. However, sometimes\\nwe like to have a single number that informs us of the quality of the\\nsolution. A popular way of combining precision and recall into a\\nsingle number is by taking their harmonic mean. This is known as\\nthe balanced f-measure (or f-score):\\nF=2\\x02P\\x02R\\nP+R(5.13)\\nThe reason that you want to use a harmonic mean rather than an\\narithmetic mean (the one you’re more used to) is that it favors sys-\\ntems that achieve roughly equal precision and recall. In the extreme\\ncase where P=R, then F=P=R. But in the imbalanced case, for\\ninstance P=0.1 and R=0.9, the overall f-measure is a modest 0.18.\\nTable 5.2shows f-measures as a function of precision and recall, so\\nthat you can see how important it is to get balanced values.\\nIn some cases, you might believe that precision is more impor-\\ntant than recall. This idea leads to the weighted f-measure, which is\\nparameterized by a weight b2[0,¥)(beta):\\nFb=(1+b2)\\x02P\\x02R\\nb2\\x02P+R(5.14)64 a course in machine learning\\nForb=1, this reduces to the standard f-measure. For b=0, it\\nfocuses entirely on recall and for b!¥it focuses entirely on preci-\\nsion. The interpretation of the weight is that Fbmeasures the perfor-\\nmance for a user who cares btimes as much about precision as about\\nrecall.\\nOne thing to keep in mind is that precision and recall (and hence\\nf-measure) depend crucially on which class is considered the thing\\nyou wish to ﬁnd. In particular, if you take a binary data set if ﬂip\\nwhat it means to be a positive or negative example, you will end\\nup with completely difference precision and recall values. It is not\\nthe case that precision on the ﬂipped task is equal to recall on the\\noriginal task (nor vice versa). Consequently, f-measure is also not the\\nsame. For some tasks where people are less sure about what they\\nwant, they will occasionally report two sets of precision/recall/f-\\nmeasure numbers, which vary based on which class is considered the\\nthing to spot.\\nThere are other standard metrics that are used in different com-\\nmunities. For instance, the medical community is fond of the sensi-\\ntivity/speci ﬁcity metric. A sensitive classiﬁer is one which almost\\nalways ﬁnds everything it is looking for: it has high recall. In fact,\\nsensitivity is exactly the same as recall. A speciﬁc classiﬁer is one\\nwhich does a good job notﬁnding the things that it doesn’t want to\\nﬁnd. Speciﬁcity is precision on the negation of the task at hand.\\nYou can compute curves for sensitivity and speciﬁcity much like\\nthose for precision and recall. The typical plot, referred to as the re-\\nceiver operatingchar acteristic(orROC curve ) plots the sensitivity\\nagainst 1\\x00speciﬁcity. Given an ROC curve, you can compute the\\narea underthecurve (orAUC ) metric, which also provides a mean-\\ningful single number for a system’s performance. Unlike f-measures,\\nwhich tend to be low because the require agreement, AUC scores\\ntend to be very high, even for not great systems. This is because ran-\\ndom chance will give you an AUC of 0.5 and the best possible AUC\\nis 1.0.\\nThe main message for evaluation metrics is that you should choose\\nwhichever one makes the most sense. In many cases, several might\\nmake sense. In that case, you should do whatever is more commonly\\ndone in your ﬁeld. There is no reason to be an outlier without cause.\\n5.6Cross Validation\\nIn Chapter 1, you learned about using development data (or held-out\\ndata) to set hyperparameters. The main disadvantage to the develop-\\nment data approach is that you throw out some of your training data,\\njust for estimating one or two hyperparameters.practical issues 65\\nAlgorithm 8Cross Validate (LearningAlgorithm ,Data ,K)\\n1:ˆe ¥ // store lowest error encountered so far\\n2:ˆa unknown // store the hyperparameter setting that yielded it\\n3:for all hyperparameter settings ado\\n4:err [ ] // keep track of the K-many error estimates\\n5:fork=1toKdo\\n6: train f(xn,yn)2Data :nmod K6=k\\x001g\\n7: test f(xn,yn)2Data :nmod K=k\\x001g// test every Kth example\\n8: model Run LearningAlgorithm ontrain\\n9: err err\\x08error of model ontest // add current error to list of errors\\n10:end for\\n11:avgErr mean of set err\\n12:ifavgErr <ˆethen\\n13: ˆe avgErr // remember these settings\\n14: ˆa a // because they’re the best so far\\n15:end if\\n16:end for\\nAn alternative is the idea of cross validation. In cross validation,\\nyou break your training data up into 10 equally-sized partitions. You\\ntrain a learning algorithm on 9 of them and test it on the remaining\\n1. You do this 10 times, each time holding out a different partition as\\nthe “development” part. You can then average your performance over\\nall ten parts to get an estimate of how well your model will perform\\nin the future. You can repeat this process for every possible choice of\\nhyperparameters to get an estimate of which one performs best. The\\ngeneral K-fold cross validation technique is shown in Algorithm 5.6,\\nwhere K=10 in the preceeding discussion.\\nIn fact, the development data approach can be seen as an approxi-\\nmation to cross validation, wherein only one of the Kloops (line 5in\\nAlgorithm 5.6) is executed.\\nTypical choices for Kare 2, 5, 10 and N\\x001. By far the most com-\\nmon is K=10:10-fold cross validation. Sometimes 5 is used for\\nefﬁciency reasons. And sometimes 2 is used for subtle statistical rea-\\nsons, but that is quite rare. In the case that K=N\\x001, this is known\\nasleave -one-outcross validation (or abbreviated as LOO cross val-\\nidation). After running cross validation, you have two choices. You\\ncan either select one of the Ktrained models as your ﬁnal model to\\nmake predictions with, or you can train a new model on all of the\\ndata, using the hyperparameters selected by cross-validation. If you\\nhave the time, the latter is probably a better options.\\nIt may seem that LOO cross validation is prohibitively expensive\\nto run. This is true for most learning algorithms except for K -nearest\\nneighbors. For KNN, leave-one-out is actually very natural. We loop\\nthrough each training point and ask ourselves whether this example\\nwould be correctly classiﬁed for all different possible values of K.66 a course in machine learning\\nAlgorithm 9KNN-T rain -LOO( D)\\n1:errk 0,81\\x14k\\x14N\\x001 // err kstores how well you do with kNN\\n2:forn=1toNdo\\n3:Sm hjjxn\\x00xmjj,mi,8m6=n // compute distances to other points\\n4:S sort (S) // put lowest-distance objects ﬁrst\\n5: ˆy 0 // current label prediction\\n6:fork=1toN\\x001do\\n7:hdist,mi Sk\\n8: ˆy ˆy+ym // let kth closest point vote\\n9: ifˆy6=ymthen\\n10: errk errk+1 // one more error for kNN\\n11: end if\\n12:end for\\n13:end for\\n14:return argminkerrk // return the Kthat achieved lowest error\\nThis requires only as much computation as computing the Knearest\\nneighbors for the highest value of K. This is such a popular and\\neffective approach for KNN classiﬁcation that it is spelled out in\\nAlgorithm 5.6.\\nOverall, the main advantage to cross validation over develop-\\nment data is robustness. The main advantage of development data is\\nspeed.\\nOne warning to keep in mind is that the goal of both cross valida-\\ntion and development data is to estimate how well you will do in the\\nfuture. This is a question of statistics, and holds only if your test data\\nreally looks like your training data. That is, it is drawn from the same\\ndistribution. In many practical cases, this is not entirely true.\\nFor example, in person identiﬁcation, we might try to classify\\nevery pixel in an image based on whether it contains a person or not.\\nIf we have 100 training images, each with 10, 000 pixels, then we have\\na total of 1 mtraining examples. The classiﬁcation for a pixel in image\\n5 is highly dependent on the classiﬁcation for a neighboring pixel in\\nthe same image. So if one of those pixels happens to fall in training\\ndata, and the other in development (or cross validation) data, your\\nmodel will do unreasonably well. In this case, it is important that\\nwhen you cross validate (or use development data), you do so over\\nimages , not over pixels . The same goes for text problems where you\\nsometimes want to classify things at a word level, but are handed a\\ncollection of documents. The important thing to keep in mind is that\\nit is the images (or documents) that are drawn independently from\\nyour data distribution and notthe pixels (or words), which are drawn\\ndependently.practical issues 67\\n5.7Hypothesis Testing and Statistical Signiﬁcance\\nSuppose that you’ve presented a machine learning solution to your\\nboss that achieves 7% error on cross validation. Your nemesis, Gabe,\\ngives a solution to your boss that achieves 6.9% error on cross vali-\\ndation. How impressed should your boss be? It depends. If this 0.1%\\nimprovement was measured over 1000 examples, perhaps not too\\nimpressed. It would mean that Gabe got exactly one more example\\nright than you did. (In fact, they probably got 15 more right and 14\\nmore wrong.) If this 0.1% impressed was measured over 1, 000, 000\\nexamples, perhaps this is more impressive.\\nThis is one of the most fundamental questions in statistics. You\\nhave a scientiﬁc hypothesis of the form “Gabe’s algorithm is better\\nthan mine.” You wish to test whether this hypothesis is true. You\\nare testing it against the null hypoth esis, which is that Gabe’s algo-\\nrithm is no better than yours. You’ve collected data (either 1000 or\\n1mdata points) to measure the strength of this hypothesis. You want\\nto ensure that the difference in performance of these two algorithms\\nisstatistically significant : i.e., is probably not just due to random\\nluck. (A more common question statisticians ask is whether one drug\\ntreatment is better than another, where “another” is either a placebo\\nor the competitor’s drug.)\\nThere are about ¥-many ways of doing hypoth esistesting. Like\\nevaluation metrics and the number of folds of cross validation, this is\\nsomething that is very discipline speciﬁc. Here, we will discuss two\\npopular tests: the paired t-test and boot strap ping . These tests, and\\nother statistical tests, have underlying assumptions (for instance, as-\\nsumptions about the distribution of observations) and strengths (for\\ninstance, small or large samples). In most cases, the goal of hypoth-\\nesis testing is to compute a p-value : namely, the probability that the\\nobserved difference in performance was by chance. The standard way\\nof reporting results is to say something like “there is a 95% chance\\nthat this difference was not by chance.” The value 95% is arbitrary,\\nand occasionally people use weaker (90%) test or stronger (99.5%)\\ntests.\\nThe t-test is an example of a para metrictest. It is applicable when\\nthe null hypothesis states that the difference between two responses\\nhas mean zero and unknown variance. The t-test actually assumes\\nthat data is distributed according to a Gaussian distribution, which is\\nprobably nottrue of binary responses. Fortunately, for large samples\\n(at least a few hundred), binary samples are well approximated by\\na Gaussian distribution. So long as your sample is sufﬁciently large,\\nthe t-test is reasonable either for regression or classiﬁcation problems.t signiﬁcance\\n\\x151.28 90.0%\\n\\x151.64 95.0%\\n\\x151.96 97.5%\\n\\x152.58 99.5%\\nTable 5.3: Table of signiﬁcance values\\nfor the t-test.Suppose that you evaluate two algorithm on N-many examples.68 a course in machine learning\\nOn each example, you can compute whether the algorithm made\\nthe correct prediction. Let a1, . . . , aNdenote the error of the ﬁrst\\nalgorithm on each example. Let b1, . . . , bNdenote the error of the\\nsecond algorithm. You can compute maandmbas the means of aand\\nb, respecitively. Finally, center the data as ˆa=a\\x00maand ˆb=b\\x00mb.\\nThe t-statistic is deﬁned as:\\nt=(ma\\x00mb)s\\nN(N\\x001)\\nån(ˆan\\x00ˆbn)2(5.15)\\nAfter computing the t-value, you can compare it to a list of values\\nfor computing conﬁdence intervals. Assuming you have a lot of data\\n(Nis a few hundred or more), then you can compare your t-value to\\nTable 5.3to determine the signiﬁcance level of the difference. What does it mean for the means\\nmaandmbto become further apart?\\nHow does this affect the t-value?\\nWhat happens if the variance of a\\nincreases??One disadvantage to the t-test is that it cannot easily be applied\\nto evaluation metrics like f-score. This is because f-score is a com-\\nputed over an entire test set and does not decompose into a set of\\nindividual errors. This means that the t-test cannot be applied.\\nFortunately, cross validation gives you a way around this problem.\\nWhen you do K-fold cross validation, you are able to compute K\\nerror metrics over the same data. For example, you might run 5-fold\\ncross validation and compute f-score for every fold. Perhaps the f-\\nscores are 92.4, 93.9, 96.1, 92.2 and 94.4. This gives you an average\\nf-score of 93.8 over the 5 folds. The standard deviation of this set of\\nf-scores is:\\ns=s\\n1\\nN\\x001å\\nn(ai\\x00m)2 (5.16)\\n=r\\n1\\n4(1.96+0.01+5.29+2.56+0.36) (5.17)\\n=1.595 ( 5.18)\\nYou can now assume that the distribution of scores is approximately\\nGaussian. If this is true, then approximately 70% of the proba-\\nbility mass lies in the range [m\\x00s,m+s]; 95% lies in the range\\n[m\\x002s,m+2s]; and 99.5% lies in the range [m\\x003s,m+3s]. So, if we\\nwere comparing our algorithm against one whose average f-score was\\n90.6%, we could be 95% certain that our superior performance was\\nnot due to chance.5 5Had we run 10-fold cross validation\\nwe might be been able to get tighter\\nconﬁdence intervals.WARNING: A conﬁdence of 95% does not mean “There is a 95%\\nchance that I am better.” All it means is that if I reran the same ex-\\nperiment 100 times, then in 95 of those experiments I would still win.\\nThese are very different statements. If you say the ﬁrst one, people\\nwho know about statistics will get very mad at you!\\nOne disadvantage to cross validation is that it is computationally\\nexpensive. More folds typically leads to better estimates, but everypractical issues 69\\nAlgorithm 10Bootstrap Evaluate (y, ˆy,NumFolds )\\n1:scores [ ]\\n2:fork=1toNumFolds do\\n3:truth [ ] // list of values we want to predict\\n4:pred [ ] // list of values we actually predicted\\n5:forn=1toNdo\\n6: m uniform random value from 1toN // sample a test point\\n7: truth truth\\x08ym // add on the truth\\n8: pred pred\\x08ˆym // add on our prediction\\n9:end for\\n10:scores scores\\x08f-score (truth ,pred) // evaluate\\n11:end for\\n12:return (mean (scores ),stddev (scores ))\\nnew fold requires training a new classiﬁer. This can get very time\\nconsuming. The technique of boot strap ping (and closely related idea\\nofjack -knifingcan address this problem.\\nSuppose that you didn’t want to run cross validation. All you have\\nis a single held-out test set with 1000 data points in it. You can run\\nyour classiﬁer and get predictions on these 1000 data points. You\\nwould like to be able to compute a metric like f-score on this test set,\\nbut also get conﬁdence intervals. The idea behind bootstrapping is\\nthat this set of 1000 is a random draw from some distribution. We\\nwould like to get multiple random draws from this distribution on\\nwhich to evaluate. We can simulate multiple draws by repeatedly\\nsubsampling from these 1000 examples, with replacement.\\nTo perform a single bootstrap, you will sample 1000 random points\\nfrom your test set of 1000 random points. This sampling must be\\ndone with replacement (so that the same example can be sampled\\nmore than once), otherwise you’ll just end up with your original test\\nset. This gives you a bootstrapped sample. On this sample, you can\\ncompute f-score (or whatever metric you want). You then do this 99\\nmore times, to get a 100-fold bootstrap. For each bootstrapped sam-\\nple, you will be a different f-score. The mean and standard deviation\\nof this set of f-scores can be used to estimate a conﬁdence interval for\\nyour algorithm.\\nThe bootstrap resampling procedure is sketched in Algorithm 5.7.\\nThis takes three arguments: the true labels y, the predicted labels ˆ y\\nand the number of folds to run. It returns the mean and standard\\ndeviation from which you can compute a conﬁdence interval.\\n5.8Debugging Learning Algorithms\\nLearning algorithms are notoriously hard to debug, as you may have\\nalready experienced if you have implemented any of the models70 a course in machine learning\\npresented so far. The main issue is that when a learning algorithm\\ndoesn’t learn, it’s unclear if this is because there’s a bug or because\\nthe learning problem is too hard (or there’s too much noise, or . . . ).\\nMoreover, sometimes bugs lead to learning algorithms performing\\nbetter than they should: these are especially hard to catch (and always\\na bit disappointing when you do catch them).\\nIn order to debug failing learning models, it is useful to revisit the\\nnotion of: where can error enter our system? In Chapter 2, we con-\\nsidered a typical design process for machine learning in Figure 2.4.\\nLeaving off the top steps in that are not relevant to machine learning\\nin particular, the basic steps that go into crafting a machine learning\\nsystem are: collect data, choose features, choose model family, choose\\ntraining data, train model, evaluate on test data. In each of these\\nsteps, things can go wrong. Below are some strategies for isolating\\nthe cause of error.\\nIs the problem with generalization to the test data? We have\\ntalked a lot about training error versus test error. In general, it’s\\nunrealistic to expect to do better on the test data than on the training\\ndata. Can your learning system do well on ﬁtting the training data?\\nIf so, then the problem is in generalization (perhaps your model\\nfamily is too complicated, you have too many features or not enough\\ndata). If not, then the problem is in representation (you probably\\nneed better features or better data).\\nDo you have train/test mismatch? If you can ﬁt the training data,\\nbut it doesn’t generalize, it could be because there’s something dif-\\nferent about your test data. Try shufﬂing your training data and test\\ndata together and then randomly selecting a new test set. If you do\\nwell in that condition, then probably the test distribution is strange\\nin some way. If reselecting the test data doesn’t help, you have other\\ngeneralization problems.\\nIs your learning algorithm implemented correctly? This often\\nmeans: is it optimizing what you think it’s optimizing. Instead\\nof measuring accuracy, try measuring whatever-quantity-your-\\nalgorithm-is-supposedly-optimizing (like log loss or hinge loss) and\\nmake sure that the optimizer is successfully minimizing this quantity.\\nIt is usually useful to hand-craft some datasets on which you know\\nthe desired behavior. For instance, you could run KNN on the XOR\\ndata. Or you could run perceptron on some easily linearly separa-\\nble data (for instance positive points along the line x2=x1+1 and\\nnegative points along the line x2=x1\\x001). Or a decision tree on\\nnice axis-aligned data. Finally, can you compare against a reference\\nimplementation?\\nDo you have an adequate representation? If you cannot even\\nﬁt the training data, you might not have a rich enough feature set.practical issues 71\\nThe easiest way to try to get a learning algorithm to overﬁt is to add\\na new feature to it. You can call this feature the C heating IsFun\\nfeature. The feature value associated with this feature is +1 if this\\nis a positive example and \\x001 (or zero) if this is a negative example.\\nIn other words, this feature is a perfect indicator of the class of this\\nexample. If you add the C heating IsFunfeature and your algorithm\\ndoes not get near 0% training error, this could be because there are\\ntoo many noisy features confusing it. You could either remove a lot\\nof the other features, or make the feature value for C heating IsFun\\neither +100 or\\x00100 so that the algorithm really looks at it. If you\\ndo this and your algorithm still cannot overﬁt then you likely have a\\nbug. (Remember to remove the C heating IsFunfeature from your\\nﬁnal implementation!) If the C heating IsFuntechnique gets you\\nnear 0% error, then you need to work on better feature design or pick\\nanother learning model (e.g., decision tree versus linear model). If\\nnot, you probably don’t have enough data or have too many features;\\ntry removing as many features as possible.\\nDo you have enough data? Try training on 80% of your training\\ndata and look at how much this hurts performance. If it hurts a lot,\\nthen getting more data is likely to help; if it only hurts a little, you\\nmight be data saturated.\\n5.9Bias/Variance Trade-off\\nBecause one of the key questions in machine learning is the question\\nof representation, it is common to think about test error in terms of a\\ndecomposition into two terms. Let fbe the learned classiﬁer, selected\\nfrom a setFof “all possible classiﬁers using a ﬁxed representation,”\\nthen:\\nerror(f) =\\x14\\nerror(f)\\x00min\\nf\\x032Ferror(f\\x03)\\x15\\n|{z }\\nestimation error+\\x14\\nmin\\nf\\x032Ferror(f)\\x15\\n|{z}\\napprox imation error(5.19)\\nHere, the second term, the approx imation error, measures the qual-\\nity of the model family6. One way of thinking of approximation error6The “model family” (such as depth\\n20decision trees, or linear classiﬁers)\\nis often refered to as the hypoth esis\\nclass . The hypothesis class Fdenotes\\nthe set of all possible classiﬁers we\\nconsider, such as all linear classiﬁers.\\nAn classiﬁer f2F is sometimes called\\nahypoth esis, though we generally\\navoid this latter terminology here.is: suppose someone gave me inﬁnite data to train on—how well\\ncould I do with this representation? The ﬁrst term, the estimation\\nerror, measures how far the actual learned classiﬁer fis from the\\noptimal classiﬁer f\\x03. You can think of this term as measuring how\\nmuch you have to pay for the fact that you don’t have inﬁnite training\\ndata.\\nUnfortunately, it is nearly impossible to compute the estima-\\ntion error and approxiation error, except in constructed cases. This\\ndoesn’t make the decomposition useless. Decompositions like this72 a course in machine learning\\nare very useful for designing debugging strategies. For instance, the\\nCheatingIsFun strategy is designed explicitly to ensure that the ap-\\nproximation error is zero, and therefore isolating all error into the\\nestimation error.\\nThere is a fundamental trade-off between estimation error and ap-\\nproximation error. As you make your representation more complex,\\nyou makeFbigger. This will typically cause a decrease in approxi-\\nmation error, because you can now ﬁt more functions. But you run a\\nrisk of increasing the estimation error, because you have added more\\nparameters to ﬁt, and you are likely to suffer from overﬁtting.\\nThe trade-off between estimation error and approximation error\\nis often called the bias/variance trade -off, where “approximation\\nerror” is “bias” and “estimation error” is “variance.” To understand\\nthis connection, consider a very simple hypothesis class Fthat only\\ncontains two functions: the always positive classiﬁer (that returns +1\\nregardless of input) and the always negative classiﬁer. Suppose you\\nhave a data generating distribution Dthat is 60% positive examples\\nand40% negative examples. You draw a training set of 41 exam-\\nples. There’s about a 90% chance that the majority of these training\\nexamples will be positive, so on this impoverished hypothesis class\\nF, there’s a 90% chance that it will learn the “all positive” classiﬁer.\\nThat is: 90% of the time, regardless of the training set, the learning\\nalgorithm learns the same thing. This is low variance as a function of\\nthe random draw of the training set. On the other hand, the learned\\nclassiﬁer is very insensitive to the input example (in this extreme\\ncase, it’s completely insensitive): it is strongly biased toward predicting\\n+1 even if everything about the input contradicts this.\\n5.10 Further Reading\\nTODO\\nFigure 5.15: object recognition with full\\ninformation6 | B EYOND BINARY CLASSIFICATION\\nDependencies:In the preceeding chapters , you have learned all about a very\\nsimple form of prediction: predicting bits. In the real world, how-\\never, we often need to predict much more complex objects. You may\\nneed to categorize a document into one of several categories: sports,\\nentertainment, news, politics, etc. You may need to rank web pages\\nor ads based on relevance to a query. These problems are all com-\\nmonly encountered, yet fundamentally more complex than binary\\nclassiﬁcation.\\nIn this chapter, you will learn how to use everything you already\\nknow about binary classiﬁcation to solve these more complicated\\nproblems. You will see that it’s relatively easy to think of a binary\\nclassiﬁer as a black box, which you can reuse for solving these more\\ncomplex problems. This is a very useful abstraction, since it allows us\\nto reuse knowledge, rather than having to build new learning models\\nand algorithms from scratch.\\n6.1Learning with Imbalanced Data\\nYour boss tells you to build a classiﬁer that can identify fraudulent\\ntransactions in credit card histories. Fortunately, most transactions\\nare legitimate, so perhaps only 0.1% of the data is a positive in-\\nstance. The imbalanced data problem refers to the fact that for a\\nlarge number of real world problems, the number of positive exam-\\nples is dwarfed by the number of negative examples (or vice versa).\\nThis is actually something of a misnomer: it is not the data that is\\nimbalanced, but the distribution from which the data is drawn. (And\\nsince the distribution is imbalanced, so must the data be.)\\nImbalanced data is a problem because machine learning algo-\\nrithms are too smart for your own good. For most learning algo-\\nrithms, if you give them data that is 99.9% negative and 0.1% posi-\\ntive, they will simply learn to always predict negative. Why? Because\\nthey are trying to minimize error, and they can achieve 0.1% error by\\ndoing nothing! If a teacher told you to study for an exam with 1000Learning Objectives:\\n• Represent complex prediction prob-\\nlems in a formal learning setting.\\n• Be able to artiﬁcally “balance”\\nimbalanced data.\\n• Understand the positive and neg-\\native aspects of several reductions\\nfrom multiclass classiﬁcation to\\nbinary classiﬁcation.\\n• Recognize the difference between\\nregression and ordinal regression.Different general classiﬁcation methods can give different, but\\nequally plausible, classiﬁcations, so you need an application\\ncontext to choose among them. – Karen Spärck-Jones74 a course in machine learning\\ntrue/false questions and only one of them is true, it is unlikely you\\nwill study very long.\\nReally, the problem is not with the data, but rather with the way\\nthat you have deﬁned the learning problem. That is to say, what you\\ncare about is notaccuracy: you care about something else. If you\\nwant a learning algorithm to do a reasonable job, you have to tell it\\nwhat you want!\\nMost likely, what you want is notto optimize accuracy, but rather\\nto optimize some other measure, like f-score or AUC. You want your\\nalgorithm to make some positive predictions, and simply prefer those\\nto be “good.” We will shortly discuss two heuristics for dealing with\\nthis problem: subsampling and weighting. In subsampling, you throw\\noutsome of your negative examples so that you are left with a bal-\\nanced data set (50% positive, 50% negative). This might scare you\\na bit since throwing out data seems like a bad idea, but at least it\\nmakes learning much more efﬁcient. In weighting, instead of throw-\\ning out positive examples, we just give them lower weight. If you\\nassign an importance weight of 0.00101 to each of the positive ex-\\namples, then there will be as much weight associated with positive\\nexamples as negative examples.\\nBefore formally deﬁning these heuristics, we need to have a mech-\\nanism for formally deﬁning supervised learning problems. We will\\nproceed by example, using binary classiﬁcation as the canonical\\nlearning problem.\\nGiven:\\n1. An input space X\\n2. An unknown distribution DoverX\\x02f\\x00 1,+1g\\n3. A training set Dsampled fromD\\nCompute: A function fminimizing: E(x,y)\\x18D\\x02\\nf(x)6=y\\x03TASK: BINARY CLASSIFICATION\\nAs in all the binary classiﬁcation examples you’ve seen, you have\\nsome input space (which has always been RD). There is some distri-\\nbution that produces labeled examples over the input space. You do\\nnot have access to that distribution, but can obtain samples from it.\\nYour goal is to ﬁnd a classiﬁer that minimizes error on that distribu-\\ntion.\\nA small modiﬁcation on this deﬁnition gives a a-weighted classiﬁ-\\ncation problem, where you believe that the positive class is a-times asbeyond binary classification 75\\nAlgorithm 11Subsample Map(Dweighted,a)\\n1:while truedo\\n2:(x,y)\\x18Dweighted// draw an example from the weighted distribution\\n3:u\\x18uniform random variable in [0, 1]\\n4:ify=+1oru<1\\nathen\\n5: return (x,y)\\n6:end if\\n7:end while\\nAlgorithm 12Subsample Test(fbinary, ˆx)\\n1:return fbinary(ˆx)\\nimportant as the negative class.\\nGiven:\\n1. An input space X\\n2. An unknown distribution DoverX\\x02f\\x00 1,+1g\\n3. A training set Dsampled fromD\\nCompute: A function fminimizing: E(x,y)\\x18Dh\\nay=1\\x02\\nf(x)6=y\\x03iTASK:a-WEIGHTED BINARY CLASSIFICATION\\nThe objects given to you in weighted binary classiﬁcation are iden-\\ntical to standard binary classiﬁcation. The only difference is that the\\ncostof misprediction for y= + 1 isa, while the cost of misprediction\\nfory=\\x001 is 1. In what follows, we assume that a>1. If it is not,\\nyou can simply swap the labels and use 1/ a.\\nThe question we will ask is: suppose that I have a good algorithm\\nfor solving the BINARY CLASSIFICATION problem. Can I turn that into\\na good algorithm for solving the a-WEIGHTED BINARY CLASSIFICATION\\nproblem?\\nIn order to do this, you need to deﬁne a transformation that maps\\na concrete weighted problem into a concrete unweighted problem.\\nThis transformation needs to happen both at training time and at test\\ntime (though it need not be the same transformation!). Algorithm 6.1\\nsketches a training-time sub-sampling transformation and Algo-\\nrithm 6.1sketches a test-time transformation (which, in this case, is\\ntrivial). All the training algorithm is doing is retaining all positive ex-\\namples and a 1/ afraction of all negative examples. The algorithm is\\nexplicitly turning the distribution over weighted examples into a (dif-\\nferent) distribution over binary examples. A vanilla binary classiﬁer76 a course in machine learning\\nis trained on this induced distribution.\\nAside from the fact that this algorithm throws out a lot of data\\n(especially for large a), it does seem to be doing a reasonable thing.\\nIn fact, from a reductions perspective, it is an optimal algorithm. You\\ncan prove the following result:\\nTheorem 3(Subsampling Optimality) .Suppose the binary classiﬁer\\ntrained in Algorithm 6.1achieves a binary error rate of e. Then the error\\nrate of the weighted predictor is equal to ae.\\nThis theorem states that if your binary classiﬁer does well (on the\\ninduced distribution), then the learned predictor will also do well\\n(on the original distribution). Thus, we have successfully converted\\na weighted learning problem into a plain classiﬁcation problem! The\\nfact that the error rate of the weighted predictor is exactly atimes\\nmore than that of the unweighted predictor is unavoidable: the error\\nmetric on which it is evaluated is atimes bigger! Why is it unreasonable to expect\\nto be able to achieve, for instance,\\nan error ofpae, or anything that is\\nsublinear in a??The proof of this theorem is so straightforward that we will prove\\nit here. It simply involves some algebra on expected values.\\nProof of Theorem 3.LetDwbe the original distribution and let Dbbe\\nthe induced distribution. Let fbe the binary classiﬁer trained on data\\nfromDbthat achieves a binary error rate of ebon that distribution.\\nWe will compute the expected error ewoffon the weighted problem:\\new=E(x,y)\\x18Dwh\\nay=1\\x02\\nf(x)6=y\\x03i\\n(6.1)\\n=å\\nx2Xå\\ny2\\x061Dw(x,y)ay=1\\x02\\nf(x)6=y\\x03\\n(6.2)\\n=aå\\nx2X\\x10\\nDw(x,+1)\\x02\\nf(x)6= + 1\\x03+Dw(x,\\x001)1\\na\\x02\\nf(x)6=\\x001\\x03\\x11\\n(6.3)\\n=aå\\nx2X\\x10\\nDb(x,+1)\\x02\\nf(x)6= + 1\\x03+Db(x,\\x001)\\x02\\nf(x)6=\\x001\\x03\\x11\\n(6.4)\\n=aE(x,y)\\x18Db\\x02\\nf(x)6=y\\x03\\n(6.5)\\n=aeb(6.6)\\nAnd we’re done! (We implicitly assumed Xis discrete. In the case\\nof continuous data, you need to replace all the sums over xwith\\nintegrals over x, but the result still holds.)\\nInstead of subsampling the low-cost class, you could alternatively\\nover samplethe high-cost class. The easiest case is when ais an in-\\nteger, say 5. Now, whenever you get a positive point, you include 5\\ncopies of it in the induced distribution. Whenever you get a negative\\npoint, you include a single copy. How can you handle non-integral a,\\nfor instance 5.5??beyond binary classification 77\\nThis oversampling algorithm achieves exactly the same theoretical\\nresult as the subsampling algorithm. The main advantage to the over-\\nsampling algorithm is that it does not throw out any data. The main\\nadvantage to the subsampling algorithm is that it is more computa-\\ntionally efﬁcient. Modify the proof of optimality\\nfor the subsampling algorithm so\\nthat it applies to the oversampling\\nalgorithm.?You might be asking yourself: intuitively, the oversampling algo-\\nrithm seems like a much better idea than the subsampling algorithm,\\nat least if you don’t care about computational efﬁciency. But the the-\\nory tells us that they are the same! What is going on? Of course the\\ntheory isn’t wrong. It’s just that the assumptions are effectively dif-\\nferent in the two cases. Both theorems state that if you can get error\\nofeon the binary problem, you automatically get error of aeon the\\nweighted problem. But they do not say anything about how possible\\nit is to get error eon the binary problem. Since the oversampling al-\\ngorithm produces more data points than the subsampling algorithm\\nit is very concievable that you could get lower binary error with over-\\nsampling than subsampling.\\nThe primary drawback to oversampling is computational inefﬁ-\\nciency. However, for many learning algorithms, it is straightforward\\nto include weighted copies of data points at no cost. The idea is to\\nstore only the unique data points and maintain a counter saying how\\nmany times they are replicated. This is not easy to do for the percep-\\ntron (it can be done, but takes work), but it iseasy for both decision\\ntrees and KNN. For example, for decision trees (recall Algorithm 1.3),\\nthe only changes are to: ( 1) ensure that line 1computes the most fre-\\nquent weighted answer, and ( 2) change lines 10and11to compute\\nweighted errors. Why is it hard to change the per-\\nceptron? (Hint: it has to do with the\\nfact that perceptron is online.)?\\nHow would you modify KNN to\\ntake into account weights??\\n6.2Multiclass Classiﬁcation\\nMulticlass classiﬁcation is a natural extension of binary classiﬁcation.\\nThe goal is still to assign a discrete label to examples (for instance,\\nis a document about entertainment, sports, ﬁnance or world news?).\\nThe difference is that you have K>2 classes to choose from.78 a course in machine learning\\nAlgorithm 13OneVersus AllTrain (Dmulticlass, Binary Train )\\n1:fori=1toKdo\\n2:Dbin relabel Dmulticlassso class iis positive and:iis negative\\n3: fi Binary Train (Dbin)\\n4:end for\\n5:return f1, . . . , fK\\nAlgorithm 14OneVersus AllTest(f1, . . . , fK, ˆx)\\n1:score h0,0, . . . , 0i // initialize K-many scores to zero\\n2:fori=1toKdo\\n3:y fi(ˆx)\\n4:score i score i+y\\n5:end for\\n6:return argmaxkscore k\\nGiven:\\n1. An input space Xand number of classes K\\n2. An unknown distribution DoverX\\x02[K]\\n3. A training set Dsampled fromD\\nCompute: A function fminimizing: E(x,y)\\x18D\\x02\\nf(x)6=y\\x03TASK: MULTICLASS CLASSIFICATION\\nNote that this is identical to binary classiﬁcation, except for the\\npresence of Kclasses. (In the above, [K] =f1, 2, 3, . . . , Kg.) In fact, if\\nyou set K=2 you exactly recover binary classiﬁcation.\\nThe game we play is the same: someone gives you a binary classi-\\nﬁer and you have to use it to solve the multiclass classiﬁcation prob-\\nlem. A very common approach is the one versusalltechnique (also\\ncalled OV A orone versusrest). To perform OVA, you train K-many\\nbinary classiﬁers, f1, . . . , fK. Each classiﬁer sees allof the training\\ndata. Classiﬁer fireceives all examples labeled class ias positives\\nand all other examples as negatives. At test time, whichever classiﬁer\\npredicts “positive” wins, with ties broken randomly. Suppose that you have Ndata\\npoints in Kclasses, evenly divided.\\nHow long does it take to train an\\nOVA classiﬁer, if the base binary\\nclassiﬁer takesO(N)time to train?\\nWhat if the base classiﬁer takes\\nO(N2)time??The training and test algorithms for OVA are sketched in Algo-\\nrithms 6.2and6.2. In the testing procedure, the prediction of the ith\\nclassiﬁer is added to the overall score for class i. Thus, if the predic-\\ntion is positive, class igets a vote; if the prdiction is negative, every-\\none else (implicitly) gets a vote. (In fact, if your learning algorithm\\ncan output a conﬁdence, as discussed in Section ??, you can often do\\nbetter by using the conﬁdence as y, rather than a simple \\x061.)\\nWhy would using a conﬁdence\\nhelp??OVA is quite natural and easy to implement. It also works verybeyond binary classification 79\\nwell in practice, so long as you do a good job choosing a good binary\\nclassiﬁcation algorithm tuning its hyperparameters well. Its weakness\\nis that it can be somewhat brittle. Intuitively, it is not particularly\\nrobust to errors in the underlying classiﬁers. If oneclassiﬁer makes a\\nmistake, it is possible that the entire prediction is erroneous. In fact,\\nit is entirely possible that none of the Kclassiﬁers predicts positive\\n(which is actually the worst-case scenario from a theoretical perspec-\\ntive)! This is made explicit in the OVA error bound below.\\nTheorem 4(OVA Error Bound) .Suppose the average binary error of the\\nK binary classiﬁers is e. Then the error rate of the OV A multiclass predictor\\nisat most (K\\x001)e.\\nProof of Theorem 4.The key question is how erroneous predictions\\nfrom the binary classiﬁers lead to multiclass errors. We break it down\\ninto false negatives (predicting - 1when the truth is + 1) and false\\npositives (predicting + 1when the truth is - 1).\\nWhen a false negative occurs, then the testing procedure chooses\\nrandomly between available options, which is all labels. This gives a\\n(K\\x001)/Kprobability of multiclass error. Since only onebinary error\\nis necessary to make this happen, the efﬁciency of this error mode is\\n[(K\\x001)/K]/1= (K\\x001)/K.\\nMultiple false positives can occur simultaneously. Suppose there\\naremfalse positives. If there is simultaneously a false negative, the\\nerror is 1. In order for this to happen, there have to be m+1 errors,\\nso the efﬁciency is 1/ (M+1). In the case that there is not a simulta-\\nneous false negative, the error probability is m/(m+1). This requires\\nmerrors, leading to an efﬁciency of 1/ (m+1).\\nThe worse case, therefore, is the false negative case, which gives an\\nefﬁciency of (K\\x001)/K. Since we have K-many opportunities to err,\\nwe multiply this by Kand get a bound of (K\\x001)e.\\nThe constants in this are relatively unimportant: the aspect that\\nmatters is that this scales linearly inK. That is, as the number of\\nclasses grows, so does your expected error.\\nTo develop alternative approaches, a useful way to think about\\nturning multiclass classiﬁcation problems into binary classiﬁcation\\nproblems is to think of them like tournaments (football, soccer–aka\\nfootball, cricket, tennis, or whatever appeals to you). You have K\\nteams entering a tournament, but unfortunately the sport they are\\nplaying only allows two to compete at a time. You want to set up a\\nway of pairing the teams and having them compete so that you can\\nﬁgure out which team is best. In learning, the teams are now the\\nclasses and you’re trying to ﬁgure out which class is best.1 1The sporting analogy breaks down\\na bit for OVA: Kgames are played,\\nwherein each team will play simultane-\\nously against all other teams.One natural approach is to have every team compete against ev-\\nery other team. The team that wins the majority of its matches is80 a course in machine learning\\nAlgorithm 15AllVersus AllTrain (Dmulticlass, Binary Train )\\n1:fij Æ,81\\x14i<j\\x14K\\n2:fori=1toK-1do\\n3:Dpos allx2Dmulticlasslabeled i\\n4:forj=i+1toKdo\\n5: Dneg allx2Dmulticlasslabeled j\\n6: Dbin f(x,+1):x2Dposg[f(x,\\x001):x2Dnegg\\n7: fij Binary Train (Dbin)\\n8:end for\\n9:end for\\n10:return allfijs\\nAlgorithm 16AllVersus AllTest(allfij, ˆx)\\n1:score h0,0, . . . , 0i // initialize K-many scores to zero\\n2:fori=1toK-1do\\n3:forj=i+1toKdo\\n4: y fij(ˆx)\\n5: score i score i+y\\n6: score j score j-y\\n7:end for\\n8:end for\\n9:return argmaxkscore k\\ndeclared the winner. This is the allversusall(orA V A ) approach\\n(sometimes called allpairs ). The most natural way to think about it\\nis as training (K\\n2)classiﬁers. Say fijfor 1\\x14i<j\\x14kis the classiﬁer\\nthat pits class iagainst class j. This classiﬁer receives all of the class i\\nexamples as “positive” and all of the class jexamples as “negative.”\\nWhen a test point arrives, it is run through all fijclassiﬁers. Every\\ntime fijpredicts positive, class igets a point; otherwise, class jgets a\\npoint. After running all (K\\n2)classiﬁers, the class with the most votes\\nwins. Suppose that you have Ndata\\npoints in Kclasses, evenly divided.\\nHow long does it take to train an\\nAVA classiﬁer, if the base binary\\nclassiﬁer takesO(N)time to train?\\nWhat if the base classiﬁer takes\\nO(N2)time? How does this com-\\npare to OVA??The training and test algorithms for AVA are sketched in Algo-\\nrithms 6.2and6.2. In theory, the AVA mapping is more complicated\\nthan the weighted binary case. The result is stated below, but the\\nproof is omitted.\\nTheorem 5(AVA Error Bound) .Suppose the average binary error of\\nthe(K\\n2)binary classiﬁers is e. Then the error rate of the AV A multiclass\\npredictor is at most 2 (K\\x001)e.\\nThe bound for AVA is 2 (K\\x001)e; the\\nbound for OVA is (K\\x001)e. Does\\nthis mean that OVA is necessarily\\nbetter than AVA? Why or why not??\\nFigure 6.1: data set on which OVA will\\ndo terribly with linear classiﬁers\\nConsider the data in Figure 6.1and\\nassume that you are using a percep-\\ntron as the base classiﬁer. How well\\nwill OVA do on this data? What\\nabout AVA??At this point, you might be wondering if it’s possible to do bet-\\nter than something linear in K. Fortunately, the answer is yes! The\\nsolution, like so much in computer science, is divide and conquer.\\nThe idea is to construct a binary tree of classiﬁers. The leaves of this\\ntree correspond to the Klabels. Since there are only log2Kdecisions\\nmade to get from the root to a leaf, then there are only log2Kchancesbeyond binary classification 81\\nto make an error.\\nFigure 6.2: example classiﬁcation tree\\nforK=8An example of a classiﬁcation tree for K=8 classes is shown in\\nFigure 6.2. At the root, you distinguish between classes f1, 2, 3, 4g\\nand classesf5, 6, 7, 8g. This means that you will train a binary clas-\\nsiﬁer whose positive examples are all data points with multiclass\\nlabelf1, 2, 3, 4gand whose negative examples are all data points with\\nmulticlass labelf5, 6, 7, 8g. Based on what decision is made by this\\nclassiﬁer, you can walk down the appropriate path in the tree. When\\nKis not a power of 2, the tree will not be full. This classiﬁcation tree\\nalgorithm achieves the following bound.\\nTheorem 6(Tree Error Bound) .Suppose the average binary classiﬁers\\nerror is e. Then the error rate of the tree classiﬁer is at mostdlog2Kee.\\nProof of Theorem 6.A multiclass error is made if any classiﬁer on\\nthe path from the root to the correct leaf makes an error. Each has\\nprobability eof making an error and the path consists of at most\\ndlog2Kebinary decisions.\\nOne thing to keep in mind with tree classiﬁers is that you have\\ncontrol over how the tree is deﬁned. In OVA and AVA you have no\\nsay in what classiﬁcation problems are created. In tree classiﬁers,\\nthe only thing that matters is that, at the root, half of the classes are\\nconsidered positive and half are considered negative. You want to\\nsplit the classes in such a way that this classiﬁcation decision is as\\neasy as possible. You can use whatever you happen to know about\\nyour classiﬁcation problem to try to separate the classes out in a\\nreasonable way.\\nCan you do better than dlog2Kee? It turns out the answer is yes,\\nbut the algorithms to do so are relatively complicated. You can actu-\\nally do as well as 2 eusing the idea of error-correcting tournaments.\\nMoreover, you can prove a lower bound that states that the best you\\ncould possible do is e/2. This means that error-correcting tourna-\\nments are at most a factor of four worse than optimal.\\n6.3Ranking\\nYou start a new web search company called Goohooing. Like other\\nsearch engines, a user inputs a query and a set of documents is re-\\ntrieved. Your goal is to rank the resulting documents based on rel-\\nevance to the query. The ranking problem is to take a collection of\\nitems and sort them according to some notion of preference. One of\\nthe trickiest parts of doing ranking through learning is to properly\\ndeﬁne the loss function. Toward the end of this section you will see a\\nvery general loss function, but before that let’s consider a few special\\ncases.82 a course in machine learning\\nAlgorithm 17Naive Rank Train (RankingData , Binary Train )\\n1:D [ ]\\n2:forn=1toNdo\\n3:for all i,j=1toMand i6=jdo\\n4: ifiis prefered to jon query nthen\\n5: D D\\x08(xnij,+1)\\n6: else if jis prefered to ion query nthen\\n7: D D\\x08(xnij,\\x001)\\n8: end if\\n9:end for\\n10:end for\\n11:return Binary Train (D)\\nContinuing the web search example, you are given a collection of\\nqueries. For each query, you are also given a collection of documents,\\ntogether with a desired ranking over those documents. In the follow-\\ning, we’ll assume that you have N-many queries and for each query\\nyou have M-many documents. (In practice, Mwill probably vary\\nby query, but for ease we’ll consider the simpliﬁed case.) The goal is\\nto train a binary classiﬁer to predict a pref erence func tion. Given a\\nquery qand two documents diand dj, the classiﬁer should predict\\nwhether dishould be preferred to djwith respect to the query q.\\nAs in all the previous examples, there are two things we have to\\ntake care of: ( 1) how to train the classiﬁer that predicts preferences;\\n(2) how to turn the predicted preferences into a ranking. Unlike the\\nprevious examples, the second step is somewhat complicated in the\\nranking case. This is because we need to predict an entire ranking of\\na large number of documents, somehow assimilating the preference\\nfunction into an overall permutation.\\nFor notationally simplicity, let xnijdenote the features associated\\nwith comparing document ito document jon query n. Training is\\nfairly straightforward. For every nand every pair i6=j, we will\\ncreate a binary classiﬁcation example based on features xnij. This\\nexample is positive if iis preferred to jin the true ranking. It is neg-\\native if jis preferred to i. (In some cases the true ranking will not\\nexpress a preference between two objects, in which case we exclude\\nthei,jand j,ipair from training.)\\nNow, you might be tempted to evaluate the classiﬁcation perfor-\\nmance of this binary classiﬁer on its own. The problem with this\\napproach is that it’s impossible to tell—just by looking at its output\\non one i,jpair—how good the overall ranking is. This is because\\nthere is the intermediate step of turning these pairwise predictions\\ninto a coherent ranking. What you need to do is measure how well\\nthe ranking based on your predicted preferences compares to the true\\nordering. Algorithms 6.3and6.3show naive algorithms for trainingbeyond binary classification 83\\nAlgorithm 18Naive Rank Test(f, ˆx)\\n1:score h0,0, . . . , 0i // initialize M-many scores to zero\\n2:for all i,j=1toMand i6=jdo\\n3:y f(ˆxij) // get predicted ranking of iandj\\n4:score i score i+y\\n5:score j score j-y\\n6:end for\\n7:return argsort (score ) // return queries sorted by score\\nand testing a ranking function.\\nThese algorithms actually work quite well in the case of bipartite\\nrank ingprob lems . A bipartite ranking problem is one in which you\\nare only ever trying to predict a binary response, for instance “is this\\ndocument relevant or not?” but are being evaluated according to a\\nmetric like AUC . This is essentially because the only goal in bipartite\\nproblems is to ensure that all the relevant documents are ahead of\\nall the irrelevant documents. There is no notion that one relevant\\ndocument is more relevant than another.\\nFor non-bipartite ranking problems, you can do better. First, when\\nthe preferences that you get at training time are more nuanced than\\n“relevant or not,” you can incorporate these preferences at training\\ntime. Effectively, you want to give a higher weight to binary prob-\\nlems that are very different in terms of preference than others. Sec-\\nond, rather than producing a list of scores and then calling an arbi-\\ntrary sorting algorithm, you can actually use the preference function\\nas the sorting function inside your own implementation of quicksort.\\nWe can now formalize the problem. Deﬁne a ranking as a function\\nsthat maps the objects we are ranking (documents) to the desired\\nposition in the list, 1, 2, . . . M. Ifsu<svthen uis preferred to v(i.e.,\\nappears earlier on the ranked document list). Given data with ob-\\nserved rankings s, our goal is to learn to predict rankings for new\\nobjects, ˆs. We deﬁne SMas the set of all ranking functions over M\\nobjects. We also wish to express the fact that making a mistake on\\nsome pairs is worse than making a mistake on others. This will be\\nencoded in a cost function w(omega), where w(i,j)is the cost for\\naccidentally putting something in position jwhen it should have\\ngone in position i. To be a valid cost function, wmust be ( 1) symmet-\\nric, (2) monotonic and ( 3) satisfy the triangle inequality. Namely: ( 1)\\nw(i,j) = w(j,i); (2) ifi<j<kori>j>kthen w(i,j)\\x14w(i,k);\\n(3)w(i,j) +w(j,k)\\x15w(i,k). With these deﬁnitions, we can properly\\ndeﬁne the ranking problem.84 a course in machine learning\\nGiven:\\n1. An input space X\\n2. An unknown distribution DoverX\\x02SM\\n3. A training set Dsampled fromD\\nCompute: A function f:X!SMminimizing:\\nE(x,s)\\x18D\"\\nå\\nu6=v[su<sv] [ˆsv<ˆsu]w(su,sv)#\\n(6.7)\\nwhere ˆs=f(x)TASK:w-RANKING\\nIn this deﬁnition, the only complex aspect is the loss function 6.7.\\nThis loss sums over all pairs of objects uand v. If the true ranking ( s)\\nprefers utov, but the predicted ranking ( ˆs) prefers vtou, then you\\nincur a cost of w(su,sv).\\nDepending on the problem you care about, you can set wto many\\n“standard” options. If w(i,j) = 1 whenever i6=j, then you achieve\\nthe Kemeny distance measure, which simply counts the number of\\npairwise misordered items. In many applications, you may only care\\nabout getting the top Kpredictions correct. For instance, your web\\nsearch algorithm may only display K=10 results to a user. In this\\ncase, you can deﬁne:\\nw(i,j) =(\\n1 if minfi,jg\\x14 Kand i6=j\\n0 otherwise(6.8)\\nIn this case, only errors in the top Kelements are penalized. Swap-\\nping items 55 and 56 is irrelevant (for K<55).\\nFinally, in the bipartite ranking case, you can express the area\\nunderthecurve (AUC ) metric as:\\nw(i,j) =(M\\n2)\\nM+(M\\x00M+)\\x028\\n><\\n>:1 if i\\x14M+and j>M+\\n1 if j\\x14M+and i>M+\\n0 otherwise(6.9)\\nHere, Mis the total number of objects to be ranked and M+is the\\nnumber that are actually “good.” (Hence, M\\x00M+is the number\\nthat are actually “bad,” since this is a bipartite problem.) You are\\nonly penalized if you rank a good item in position greater than M+\\nor if you rank a bad item in a position less than or equal to M+.\\nIn order to solve this problem, you can follow a recipe similar to\\nthe naive approach sketched earlier. At training time, the biggestbeyond binary classification 85\\nAlgorithm 19Rank Train (Drank,w, Binary Train )\\n1:Dbin [ ]\\n2:for all (x,s)2Drankdo\\n3:for all u6=vdo\\n4: y sign (sv-su) // y is +1 if uis prefered to v\\n5: w w(su,sv) // w is the cost of misclassiﬁcation\\n6: Dbin Dbin\\x08(y,w,xuv)\\n7:end for\\n8:end for\\n9:return Binary Train (Dbin)\\nAlgorithm 20Rank Test(f, ˆx,obj)\\n1:ifobjcontains 0 or 1 elements then\\n2:return obj\\n3:else\\n4:p randomly chosen object in obj // pick pivot\\n5:left [ ] // elements that seem smaller than p\\n6:right [ ] // elements that seem larger than p\\n7:for all u2objnfpgdo\\n8: ˆy f(xup) // what is the probability that uprecedes p\\n9: ifuniform random variable <ˆythen\\n10: left left\\x08u\\n11: else\\n12: right right\\x08u\\n13: end if\\n14:end for\\n15:left Rank Test(f, ˆx,left) // sort earlier elements\\n16:right Rank Test(f, ˆx,right ) // sort later elements\\n17:return left\\x08hpi\\x08right\\n18:end if\\nchange is that you can weight each training example by how bad it\\nwould be to mess it up. This change is depicted in Algorithm 6.3,\\nwhere the binary classiﬁcation data has weights w provided for saying\\nhow important a given example is. These weights are derived from\\nthe cost function w.\\nAt test time, instead of predicting scores and then sorting the list,\\nyou essentially run the quicksort algorithm, using fas a comparison\\nfunction. At each step in Algorithm 6.3, a pivot pis chosen. Every\\nother object uis compared to pusing f. Iffthinks uis better, then it\\nis sorted on the left; otherwise it is sorted on the right. There is one\\nmajor difference between this algorithm and quicksort: the compar-\\nison function is allowed to be probabilistic . Iffoutputs probabilities,\\nfor instance it predicts that uhas an 80% probability of being better\\nthan p, then it puts it on the left with 80% probability and on the\\nright with 20% probability. (The pseudocode is written in such a way\\nthat even if fjust predicts\\x001,+1, the algorithm still works.)86 a course in machine learning\\nThis algorithm is better than the naive algorithm in at least two\\nways. First, it only makes O(Mlog2M)calls to f(in expectation),\\nrather thanO(M2)calls in the naive case. Second, it achieves a better\\nerror bound, shown below:\\nTheorem 7(Rank Error Bound) .Suppose the average binary error of f\\nise. Then the ranking algorithm achieves a test error of at most 2ein the\\ngeneral case, and ein the bipartite case.\\n6.4Further Reading\\nTODO further reading7 | L INEAR MODELS\\nDependencies:InChapter 4 ,you learned about the perceptron algorithm for\\nlinear classiﬁcation. This was both a model (linear classiﬁer) and al-\\ngorithm (the perceptron update rule) in one. In this section, we will\\nseparate these two, and consider general ways for optimizing lin-\\near models. This will lead us into some aspects of optimization (aka\\nmathematical programming), but not very far. At the end of this\\nchapter, there are pointers to more literature on optimization for\\nthose who are interested.\\nThe basic idea of the perceptron is to run a particular algorithm\\nuntil a linear separator is found. You might ask: are there better al-\\ngorithms for ﬁnding such a linear separator? We will follow this idea\\nand formulate a learning problem as an explicit optimization prob-\\nlem: ﬁnd me a linear separator that is not too complicated. We will\\nsee that ﬁnding an “optimal” separator is actually computationally\\nprohibitive, and so will need to “relax” the optimality requirement.\\nThis will lead us to a convexobjective that combines a loss func-\\ntion (how well are we doing on the training data?) and a regularizer\\n(how complicated is our learned model?). This learning framework\\nis known as both Tikhonov regularization and struc tural risk mini-\\nmiza tion.\\n7.1The Optimization Framework for Linear Models\\nYou have already seen the perceptron as a way of ﬁnding a weight\\nvector wand bias bthat do a good job of separating positive train-\\ning examples from negative training examples. The perceptron is a\\nmodel and algorithm in one. Here, we are interested in separating\\nthese issues. We will focus on linear models, like the perceptron.\\nBut we will think about other, more generic ways of ﬁnding good\\nparameters of these models.\\nThe goal of the perceptron was to ﬁnd a separatinghyperplane\\nfor some training data set. For simplicity, you can ignore the issue\\nof overﬁtting (but just for now!). Not all data sets are linearly sepa-Learning Objectives:\\n• Deﬁne and plot four surrogate loss\\nfunctions: squared loss, logistic loss,\\nexponential loss and hinge loss.\\n• Compare and contrast the optimiza-\\ntion of 0/1loss and surrogate loss\\nfunctions.\\n• Solve the optimization problem\\nfor squared loss with a quadratic\\nregularizer in closed form.\\n• Implement and debug gradient\\ndescent and subgradient descent.The essence of mathematics is not to make simple things compli-\\ncated, but to make complicated things simple. – Stanley Gudder88 a course in machine learning\\nrable. In the case that your training data isn’t linearly separable, you\\nmight want to ﬁnd the hyperplane that makes the fewest errors on\\nthe training data. We can write this down as a formal mathematics\\noptimiza tion prob lem as follows:\\nmin\\nw,bå\\nn1[yn(w\\x01xn+b)>0] (7.1)\\nIn this expression, you are optimizing over two variables, wand b.\\nThe objective func tion is the thing you are trying to minimize. In\\nthis case, the objective function is simply the errorrate (or0/1loss) of\\nthe linear classiﬁer parameterized by w,b. In this expression, 1[\\x01]is\\ntheindicatorfunc tion: it is one when (\\x01)is true and zero otherwise. You should remember the yw\\x01x\\ntrick from the perceptron discus-\\nsion. If not, re-convince yourself\\nthat this is doing the right thing.?We know that the perceptron algorithm is guaranteed to ﬁnd\\nparameters for this model if the data is linearly separable. In other\\nwords, if the optimum of Eq ( 7.1) is zero, then the perceptron will\\nefﬁciently ﬁnd parameters for this model. The notion of “efﬁciency”\\ndepends on the margin of the data for the perceptron.\\nYou might ask: what happens if the data is notlinearly separable?\\nIs there an efﬁcient algorithm for ﬁnding an optimal setting of the\\nparameters? Unfortunately, the answer is no.There is no polynomial\\ntime algorithm for solving Eq ( 7.1), unless P=NP . In other words,\\nthis problem is NP-hard. Sadly, the proof of this is quite complicated\\nand beyond the scope of this book, but it relies on a reduction from a\\nvariant of satisﬁability. The key idea is to turn a satisﬁability problem\\ninto an optimization problem where a clause is satisﬁed exactly when\\nthe hyperplane correctly separates the data.\\nYou might then come back and say: okay, well I don’t really need\\nanexact solution. I’m willing to have a solution that makes one or\\ntwo more errors than it has to. Unfortunately, the situation is really\\nbad. Zero/one loss is NP-hard to even appproximately minimize . In\\nother words, there is no efﬁcient algorithm for even ﬁnding a solution\\nthat’s a small constant worse than optimal. (The best known constant\\nat this time is 418/415 \\x191.007.)\\nHowever, before getting too disillusioned about this whole enter-\\nprise (remember: there’s an entire chapter about this framework, so\\nit must be going somewhere!), you should remember that optimizing\\nEq (7.1) perhaps isn’t even what you want to do! In particular, all it\\nsays is that you will get minimal training error. It says nothing about\\nwhat your test error will be like. In order to try to ﬁnd a solution that\\nwill generalize well to test data, you need to ensure that you do not\\noverﬁt the data. To do this, you can introduce a regularizer over the\\nparameters of the model. For now, we will be vague about what this\\nregularizer looks like, and simply call it an arbitrary function R(w,b).linear models 89\\nThis leads to the following, regularized objective:\\nmin\\nw,bå\\nn1[yn(w\\x01xn+b)>0] +lR(w,b) (7.2)\\nIn Eq ( 7.2), we are now trying to optimize a trade-off between a so-\\nlution that gives low training error (the ﬁrst term) and a solution\\nthat is “simple” (the second term). You can think of the maximum\\ndepth hyperparameter of a decision tree as a form of regularization\\nfor trees. Here, Ris a form of regularization for hyperplanes. In this\\nformulation, lbecomes a hyperparameterfor the optimization. Assuming Rdoes the “right thing,”\\nwhat value(s) of lwill lead to over-\\nﬁtting? What value(s) will lead to\\nunderﬁtting??The key remaining questions, given this formalism, are:\\n• How can we adjust the optimization problem so that there are\\nefﬁcient algorithms for solving it?\\n• What are good regularizers R(w,b)for hyperplanes?\\n• Assuming we can adjust the optimization problem appropriately,\\nwhat algorithms exist for efﬁciently solving this regularized opti-\\nmization problem?\\nWe will address these three questions in the next sections.\\n7.2Convex Surrogate Loss Functions\\nYou might ask: why is optimizing zero/one loss so hard? Intuitively,\\none reason is that small changes to w,bcan have a large impact on\\nthe value of the objective function. For instance, if there is a positive\\ntraining example with w,x\\x01+b=\\x000.0000001, then adjusting bup-\\nwards by 0.00000011 will decrease your error rate by 1. But adjusting\\nit upwards by 0.00000009 will have no effect. This makes it really\\ndifﬁcult to ﬁgure out good ways to adjust the parameters.\\nFigure 7.1: plot of zero/one versus\\nmarginTo see this more clearly, it is useful to look at plots that relate\\nmargin toloss. Such a plot for zero/one loss is shown in Figure 7.1.\\nIn this plot, the horizontal axis measures the margin of a data point\\nand the vertical axis measures the loss associated with that margin.\\nFor zero/one loss, the story is simple. If you get a positive margin\\n(i.e., y(w\\x01x+b)>0) then you get a loss of zero. Otherwise you get\\na loss of one. By thinking about this plot, you can see how changes\\nto the parameters that change the margin just a little bit can have an\\nenormous effect on the overall loss.\\nFigure 7.2: plot of zero/one versus\\nmargin and an S version of itYou might decide that a reasonable way to address this problem is\\nto replace the non-smooth zero/one loss with a smooth approxima-\\ntion. With a bit of effort, you could probably concoct an “S”-shaped\\nfunction like that shown in Figure 7.2. The beneﬁt of using such an\\nS-function is that it is smooth, and potentially easier to optimize. The\\ndifﬁculty is that it is not convex.90 a course in machine learning\\nIf you remember from calculus, a convex function is one that looks\\nlike a happy face ( ^). (On the other hand, a concave function is one\\nthat looks like a sad face ( _); an easy mnemonic is that you can hide\\nunder a con cave function.) There are two equivalent deﬁnitions of\\na convex function. The ﬁrst is that it’s second derivative is always\\nnon-negative. The second, more geometric, deﬁtion is that any chord\\nof the function lies above it. This is shown in Figure 7.3. There you\\ncan see a convex function and a non-convex function, both with two\\nchords drawn in. In the case of the convex function, the chords lie\\nabove the function. In the case of the non-convex function, there are\\nparts of the chord that lie below the function.\\nFigure 7.3: plot of convex and non-\\nconvex functions with two chords eachConvex functions are nice because they are easy to minimize . Intu-\\nitively, if you drop a ball anywhere in a convex function, it will even-\\ntually get to the minimum. This is not true for non-convex functions.\\nFor example, if you drop a ball on the very left end of the S-function\\nfrom Figure 7.2, it will not go anywhere.\\nThis leads to the idea of convex surrogate loss functions . Since\\nzero/one loss is hard to optimize, you want to optimize something\\nelse, instead. Since convex functions are easy to optimize, we want\\nto approximate zero/one loss with a convex function. This approxi-\\nmating function will be called a surrogate loss. The surrogate losses\\nwe construct will always be upper bounds on the true loss function:\\nthis guarantees that if you minimize the surrogate loss, you are also\\npushing down the real loss.\\nFigure 7.4: surrogate loss fnsThere are four common surrogate loss functions, each with their\\nown properties: hinge loss,logisticloss,exponentialloss and\\nsquared loss. These are shown in Figure 7.4and deﬁned below.\\nThese are deﬁned in terms of the true label y(which is justf\\x001,+1g)\\nand the predicted value ˆy=w\\x01x+b.\\nZero/one: `(0/1)(y,ˆy) =1[yˆy\\x140] (7.3)\\nHinge: `(hin)(y,ˆy) =maxf0, 1\\x00yˆyg (7.4)\\nLogistic: `(log)(y,ˆy) =1\\nlog 2log(1+exp[\\x00yˆy]) (7.5)\\nExponential: `(exp)(y,ˆy) =exp[\\x00yˆy] (7.6)\\nSquared: `(sqr)(y,ˆy) = ( y\\x00ˆy)2(7.7)\\nIn the deﬁnition of logistic loss, the1\\nlog 2term out front is there sim-\\nply to ensure that `(log)(y, 0) = 1. This ensures, like all the other\\nsurrogate loss functions, that logistic loss upper bounds the zero/one\\nloss. (In practice, people typically omit this constant since it does not\\naffect the optimization.)\\nThere are two big differences in these loss functions. The ﬁrst\\ndifference is how “upset” they get by erroneous predictions. In thelinear models 91\\ncase of hinge loss and logistic loss, the growth of the function as ˆy\\ngoes negative is linear. For squared loss and exponential loss, it is\\nsuper-linear. This means that exponential loss would rather get a few\\nexamples a little wrong than one example really wrong. The other\\ndifference is how they deal with very conﬁdent correct predictions.\\nOnce yˆy>1, hinge loss does not care any more, but logistic and\\nexponential still think you can do better. On the other hand, squared\\nloss thinks it’s just as bad to predict +3 on a positive example as it is\\nto predict\\x001 on a positive example.\\n7.3Weight Regularization\\nIn our learning objective, Eq ( 7.2), we had a term correspond to the\\nzero/one loss on the training data, plus a regularizer whose goal\\nwas to ensure that the learned function didn’t get too “crazy.” (Or,\\nmore formally, to ensure that the function did not overﬁt.) If you re-\\nplace to zero/one loss with a surrogate loss, you obtain the following\\nobjective:\\nmin\\nw,bå\\nn`(yn,w\\x01xn+b) +lR(w,b) (7.8)\\nThe question is: what should R(w,b)look like?\\nFrom the discussion of surrogate loss function, we would like\\nto ensure that Ris convex. Otherwise, we will be back to the point\\nwhere optimization becomes difﬁcult. Beyond that, a common desire\\nis that the components of the weight vector (i.e., the wds) should be\\nsmall (close to zero). This is a form of inductive bias .\\nWhy are small values of wdgood? Or, more precisely, why do\\nsmall values of wdcorrespond to simple functions ? Suppose that we\\nhave an example xwith label +1. We might believe that other ex-\\namples, x0that are nearby xshould also have label +1. For example,\\nif I obtain x0by taking xand changing the ﬁrst component by some\\nsmall value eand leaving the rest the same, you might think that the\\nclassiﬁcation would be the same. If you do this, the difference be-\\ntween ˆyand ˆy0will be exactly ew1. So if w1is reasonably small, this\\nis unlikely to have much of an effect on the classiﬁcation decision. On\\nthe other hand, if w1is large, this could have a large effect.\\nAnother way of saying the same thing is to look at the derivative\\nof the predictions as a function of w1. The derivative of w\\x01x+bwith\\nrespect to w1is:\\n¶[w\\x01x+b]\\n¶w1=¶[ådwdxd+b]\\n¶w1=x1 (7.9)\\nInterpreting the derivative as the rate of change, we can see that\\nthe rate of change of the prediction function is proportional to the92 a course in machine learning\\nindividual weights. So if you want the function to change slowly, you\\nwant to ensure that the weights stay small.\\nOne way to accomplish this is to simply use the norm of the\\nweight vector. Namely R(norm)(w,b) =jjwjj=q\\nådw2\\nd. This function\\nis convex and smooth, which makes it easy to minimize. In prac-\\ntice, it’s often easier to use the squared norm, namely R(sqr)(w,b) =\\njjwjj2=ådw2\\ndbecause it removes the ugly square root term and\\nremains convex. An alternative to using the sum of squared weights\\nis to use the sum of absolute weights: R(abs)(w,b) =ådjwdj. Both of\\nthese norms are convex. Why do we not regularize the bias\\nterm b? ?In addition to small weights being good, you could argue that zero\\nweights are better. If a weight wdgoes to zero, then this means that\\nfeature dis not used at all in the classiﬁcation decision. If there are a\\nlarge number of irrelevant features, you might want as many weights\\nto go to zero as possible. This suggests an alternative regularizer:\\nR(cnt)(w,b) =åd1[xd6=0].\\nWhy might you not want to use\\nR(cnt)as a regularizer? ?This line of thinking leads to the general concept of p-norms .\\n(Technically these are called `p(or “ell p”) norms, but this notation\\nclashes with the use of `for “loss.”) This is a family of norms that all\\nhave the same general ﬂavor. We write jjwjjpto denote the p-norm of\\nw.\\njjwjjp= \\nå\\ndjwdjp!1\\np\\n(7.10)\\nYou can check that the 2-norm exactly corresponds to the usual Eu-\\nclidean norm, and that the 1-norm corresponds to the “absolute”\\nregularizer described above.You can actually identify the R(cnt)\\nregularizer with a p-norm as well.\\nWhich value of pgives it to you?\\n(Hint: you may have to take a limit.)?\\nFigure 7.5:loss:norms2d : level sets of\\nthe same p-normsWhen p-norms are used to regularize weight vectors, the interest-\\ning aspect is how they trade-off multiple features. To see the behavior\\nofp-norms in two dimensions, we can plot their contour (orlevel -\\nset). Figure 7.5shows the contours for the same pnorms in two\\ndimensions. Each line denotes the two-dimensional vectors to which\\nthis norm assignes a total value of 1. By changing the value of p, you\\ncan interpolate between a square (the so-called “max norm”), down\\nto a circle (2-norm), diamond (1-norm) and pointy-star-shaped-thing\\n(p<1 norm).\\nThe max norm corresponds to\\nlim p!¥. Why is this called the max\\nnorm??In general, smaller values of p“prefer” sparser vectors. You can\\nsee this by noticing that the contours of small p-norms “stretch”\\nout along the axes. It is for this reason that small p-norms tend to\\nyield weight vectors with many zero entries (aka sparse weight vec-\\ntors). Unfortunately, for p<1 the norm becomes non-convex. As\\nyou might guess, this means that the 1-norm is a popular choice for\\nsparsity-seeking applications.linear models 93\\nA gradient is a multidimensional generalization of a derivative. Suppose you have a function\\nf:RD!Rthat takes a vector x=hx1,x2, . . . , xDias input and produces a scalar value as output.\\nYou can differentite this function according to any one of the inputs; for instance, you can compute¶f\\n¶x5\\nto get the derivative with respect to the ﬁfth input. The gradientoffis just the vector consisting of the\\nderivative fwith respect to each of its input coordinates independently, and is denoted rf, or, when\\nthe input to fis ambiguous,rxf. This is deﬁned as:\\nrxf=\\x1c¶f\\n¶x1,¶f\\n¶x2, . . . ,¶f\\n¶xD\\x1d\\n(7.11)\\nFor example, consider the function f(x1,x2,x3) =x3\\n1+5x1x2\\x003x2x2\\n3. The gradient is:\\nrxf=D\\n3x2\\n1+5x2, 5x1\\x003x2\\n3,\\x006x2x3E\\n(7.12)\\nNote that if f:RD!R, thenrf:RD!RD. If you evaluaterf(x), this will give you the gradient at\\nx, a vector in RD. This vector can be interpreted as the direction of steep estascent : namely, if you were\\nto travel an inﬁnitesimal amount in the direction of the gradient, you would go uphill (i.e., increase f)\\nthe most.MATHREVIEW | GRADIENTS\\nFigure 7.6:\\n7.4Optimization with Gradient Descent\\nEnvision the following problem. You’re taking up a new hobby:\\nblindfolded mountain climbing. Someone blindfolds you and drops\\nyou on the side of a mountain. Your goal is to get to the peak of the\\nmountain as quickly as possible. All you can do is feel the mountain\\nwhere you are standing, and take steps. How would you get to the\\ntop of the mountain? Perhaps you would feel to ﬁnd out what direc-\\ntion feels the most “upward” and take a step in that direction. If you\\ndo this repeatedly, you might hope to get the the top of the moun-\\ntain. (Actually, if your friend promises always to drop you on purely\\nconcave mountains, you willeventually get to the peak!)\\nThe idea of gradient-based methods of optimization is exactly the\\nsame. Suppose you are trying to ﬁnd the maximum of a function\\nf(x). The optimizer maintains a current estimate of the parameter of\\ninterest, x. At each step, it measures the gradientof the function it is\\ntrying to optimize. This measurement occurs atthe current location,\\nx. Call the gradient g. It then takes a step in the direction of the\\ngradient, where the size of the step is controlled by a parameter h\\n(eta). The complete step is x x+hg. This is the basic idea of\\ngradientascent .\\nThe opposite of gradient ascent is gradientdescent . All of our94 a course in machine learning\\nAlgorithm 21Gradient Descent (F,K,h1, . . . )\\n1:z(0) h0,0, . . . , 0i // initialize variable we are optimizing\\n2:fork=1. . .Kdo\\n3:g(k) r zFjz(k-1) // compute gradient at current location\\n4:z(k) z(k-1)\\x00h(k)g(k)// take a step down the gradient\\n5:end for\\n6:return z(K)\\nlearning problems will be framed as minimization problems (trying\\nto reach the bottom of a ditch, rather than the top of a hill). There-\\nfore, descent is the primary approach you will use. One of the major\\nconditions for gradient ascent being able to ﬁnd the true, global min-\\nimum , of its objective function is convexity. Without convexity, all is\\nlost.\\nThe gradient descent algorithm is sketched in Algorithm 7.4.\\nThe function takes as arguments the function Fto be minimized,\\nthe number of iterations Kto run and a sequence of learning rates\\nh1, . . . , hK. (This is to address the case that you might want to start\\nyour mountain climbing taking large steps, but only take small steps\\nwhen you are close to the peak.)\\nThe only real work you need to do to apply a gradient descent\\nmethod is be able to compute derivatives. For concreteness, suppose\\nthat you choose exponential loss as a loss function and the 2-norm as\\na regularizer. Then, the regularized objective function is:\\nL(w,b) =å\\nnexp\\x02\\x00yn(w\\x01xn+b)\\x03+l\\n2jjwjj2(7.13)\\nThe only “strange” thing in this objective is that we have replaced\\nlwithl\\n2. The reason for this change is just to make the gradients\\ncleaner. We can ﬁrst compute derivatives with respect to b:\\n¶L\\n¶b=¶\\n¶bå\\nnexp\\x02\\x00yn(w\\x01xn+b)\\x03+¶\\n¶bl\\n2jjwjj2(7.14)\\n=å\\nn¶\\n¶bexp\\x02\\x00yn(w\\x01xn+b)\\x03+0 ( 7.15)\\n=å\\nn\\x12¶\\n¶b\\x00yn(w\\x01xn+b)\\x13\\nexp\\x02\\x00yn(w\\x01xn+b)\\x03\\n(7.16)\\n=\\x00å\\nnynexp\\x02\\x00yn(w\\x01xn+b)\\x03\\n(7.17)\\nBefore proceeding, it is worth thinking about what this says. From a\\npractical perspective, the optimization will operate by updating b \\nb\\x00h¶L\\n¶b. Consider positive examples: examples with yn= + 1. We\\nwould hope for these examples that the current prediction, w\\x01xn+b,\\nis as large as possible. As this value tends toward ¥, the term in the\\nexp[]goes to zero. Thus, such points will not contribute to the step.linear models 95\\nHowever, if the current prediction is small, then the exp []term will\\nbe positive and non-zero. This means that the bias term bwill be\\nincreased , which is exactly what you would want. Moreover, once all\\npoints are very well classiﬁed, the derivative goes to zero. This considered the case of posi-\\ntive examples. What happens with\\nnegative examples?? Now that we have done the easy case, let’s do the gradient with\\nrespect to w.\\nrwL=rwå\\nnexp\\x02\\x00yn(w\\x01xn+b)\\x03+rwl\\n2jjwjj2(7.18)\\n=å\\nn(rw\\x00yn(w\\x01xn+b))exp\\x02\\x00yn(w\\x01xn+b)\\x03+lw\\n(7.19)\\n=\\x00å\\nnynxnexp\\x02\\x00yn(w\\x01xn+b)\\x03+lw (7.20)\\nNow you can repeat the previous exercise. The update is of the form\\nw w\\x00hrwL. For well classiﬁed points (ones that tend toward\\nyn¥), the gradient is near zero. For poorly classiﬁed points, the gra-\\ndient points in the direction \\x00ynxn, so the update is of the form\\nw w+cynxn, where cis some constant. This is just like the per-\\nceptron update! Note that cis large for very poorly classiﬁed points\\nand small for relatively well classiﬁed points.\\nBy looking at the part of the gradient related to the regularizer,\\nthe update says: w w\\x00lw= (1\\x00l)w. This has the effect of\\nshrinking the weights toward zero. This is exactly what we expect the\\nregulaizer to be doing!\\nFigure 7.7: good and bad step sizesThe success of gradient descent hinges on appropriate choices\\nfor the step size. Figure 7.7shows what can happen with gradient\\ndescent with poorly chosen step sizes. If the step size is too big, you\\ncan accidentally step over the optimum and end up oscillating. If the\\nstep size is too small, it will take way too long to get to the optimum.\\nFor a well-chosen step size, you can show that gradient descent will\\napproach the optimal value at a fast rate. The notion of convergence\\nhere is that the objective value converges to the true minimum.\\nTheorem 8(Gradient Descent Convergence) .Under suitable condi-\\ntions1, for an appropriately chosen constant step size (i.e., h1=h2,\\x01\\x01\\x01=1Speciﬁcally the function to be opti-\\nmized needs to be strongly convex.\\nThis is true for all our problems, pro-\\nvided l>0. For l=0 the rate could\\nbe as bad asO(1/p\\nk).h), the convergence rate of gradient descent is O(1/k). More speciﬁ-\\ncally, letting z\\x03be the global minimum of F, we have:F(z(k))\\x00F(z\\x03)\\x14\\n2jjz(0)\\x00z\\x03jj2\\nhk.\\nA naive reading of this theorem\\nseems to say that you should choose\\nhuge values of h. It should be obvi-\\nous that this cannot be right. What\\nis missing??The proof of this theorem is a bit complicated because it makes\\nheavy use of some linear algebra. The key is to set the learning rate\\nto 1/ L, where Lis the maximum curvature of the function that is\\nbeing optimized. The curvature is simply the “size” of the second\\nderivative. Functions with high curvature have gradients that change96 a course in machine learning\\nquickly, which means that you need to take small steps to avoid\\noverstepping the optimum.\\nThis convergence result suggests a simple approach to decid-\\ning when to stop optimizing: wait until the objective function stops\\nchanging by much. An alternative is to wait until the parameters stop\\nchanging by much. A ﬁnal example is to do what you did for percep-\\ntron: early stopping. Every iteration, you can check the performance\\nof the current model on some held-out data, and stop optimizing\\nwhen performance plateaus.\\n7.5From Gradients to Subgradients\\nAs a good exercise, you should try deriving gradient descent update\\nrules for the different loss functions and different regularizers you’ve\\nlearned about. However, if you do this, you might notice that hinge\\nlossand the 1-norm regularizer are not differentiable everywhere! In\\nparticular, the 1-norm is not differentiable around wd=0, and the\\nhinge loss is not differentiable around yˆy=1.\\nThe solution to this is to use subgradientoptimization. One way\\nto think about subgradients is just to not think about it: you essen-\\ntially need to just ignore the fact that you forgot that your function\\nwasn’t differentiable, and just try to apply gradient descent anyway.\\nTo be more concrete, consider the hinge function f(z) =maxf0, 1\\x00\\nzg. This function is differentiable for z>1 and differentiable for\\nz<1, but not differentiable at z=1. You can derive this using\\ndifferentiation by parts:\\n¶\\n¶zf(z) =¶\\n¶z(\\n0 if z>1\\n1\\x00zifz<1(7.21)\\n=(\\n¶\\n¶z0 if z>1\\n¶\\n¶z(1\\x00z)ifz<1(7.22)\\n=(\\n0 if z\\x151\\n\\x001 if z<1(7.23)\\nFigure 7.8: hinge loss with subThus, the derivative is zero for z<1 and\\x001 for z>1, matching\\nintuition from the Figure. At the non-differentiable point, z=1,\\nwe can use a subderiva tive: a generalization of derivatives to non-\\ndifferentiable functions. Intuitively, you can think of the derivative\\noffatzas the tangent line. Namely, it is the line that touches fat\\nzthat is always below f(for convex functions). The subderivative,\\ndenoted ¶¶¶f, is the setof all such lines. At differentiable positions,\\nthis set consists just of the actual derivative. At non-differentiable\\npositions, this contains all slopes that deﬁne lines that always lie\\nunder the function and make contact at the operating point. This islinear models 97\\nAlgorithm 22Hinge Regularized GD( D,l,MaxIter )\\n1:w h0,0, . . .0i,b 0 // initialize weights and bias\\n2:foriter=1. . .MaxIter do\\n3:g h0,0, . . .0i,g 0 // initialize gradient of weights and bias\\n4:for all (x,y)2Ddo\\n5: ify(w\\x01x+b)\\x141then\\n6: g g+yx // update weight gradient\\n7: g g+y // update bias derivative\\n8: end if\\n9:end for\\n10:g g\\x00lw // add in regularization term\\n11:w w+hg // update weights\\n12:b b+hg // update bias\\n13:end for\\n14:return w,b\\nshown pictorally in Figure 7.8, where example subderivatives are\\nshown for the hinge loss function. In the particular case of hinge loss,\\nany value between 0 and \\x001 is a valid subderivative at z=0. In fact,\\nthe subderivative is always a closed set of the form [a,b], where aand\\nbcan be derived by looking at limits from the left and right.\\nThis gives you a way of computing derivative-like things for non-\\ndifferentiable functions. Take hinge loss as an example. For a given\\nexample n, the subgradient of hinge loss can be computed as:\\n¶¶¶wmaxf0, 1\\x00yn(w\\x01xn+b)g (7.24)\\n=¶¶¶w(\\n0 if yn(w\\x01xn+b)>1\\n1\\x00yn(w\\x01xn+b)otherwise(7.25)\\n=(\\n¶¶¶w0 if yn(w\\x01xn+b)>1\\n¶¶¶w1\\x00yn(w\\x01xn+b)otherwise(7.26)\\n=(\\n0 ifyn(w\\x01xn+b)>1\\n\\x00ynxnotherwise(7.27)\\nIf you plug this subgradient form into Algorithm 7.4, you obtain\\nAlgorithm 7.5. This is the subgradientdescent for regularized hinge\\nloss (with a 2-norm regularizer).\\n7.6Closed-form Optimization for Squared Loss\\nAlthough gradient descent is a good, generic optimization algorithm,\\nthere are cases when you can do better. An example is the case of a\\n2-norm regularizer and squared error loss function. For this, you can\\nactually obtain a closed form solution for the optimal weights. How-\\never, to obtain this, you need to rewrite the optimization problem in\\nterms of matrix operations. For simplicity, we will only consider the98 a course in machine learning\\nIfAand Bare matrices, and AisN\\x02Kand BisK\\x02M(the inner dimensions must match), then the ma-\\ntrix product ABis a matrix Cthat is N\\x02M, with Cn,m=åkAn,kBk,m. Ifvis a vector in RD, we will\\ntreat is as a column vector , or a matrix of size D\\x021. Thus, Avis well deﬁned if AisD\\x02M, and the result-\\ning product is a vector uwith um=ådAd,mvd.\\nAside from matrix product, a fundamental matrix operation is inversion. We will often encounter a\\nform like Ax=y, where Aand yare known and we want to solve for A. IfAis square of size N\\x02N,\\nthen the inverse of A, denoted A\\x001, is also a square matrix of size N\\x02N, such that AA\\x001=IN=A\\x001A.\\nI.e., multiplying a matrix by its inverse (on either side) gives back the identity matrix. Using this, we\\ncan solve Ax=yby multiplying both sides by A\\x001on the left (recall that order matters in matrix mul-\\ntiplication), yielding A\\x001Ax=A\\x001yfrom which we can conclude x=A\\x001y. Note that not all square\\nmatrices are invertible. For instance, the all zeros matrix does not have an inverse (in the same way\\nthat 1/0 is not deﬁned for scalars). However, there are other matrices that do not have inverses; such\\nmatrices are called singular.MATHREVIEW | MATRIX MULTIPLICATION AND INVERSION\\nFigure 7.9:\\nunbiased version, but the extension is Exercise ??. This is precisely the\\nlinearregres sion setting.\\nYou can think of the training data as a large matrix Xof size N\\x02D,\\nwhere Xn,dis the value of the dth feature on the nth example. You\\ncan think of the labels as a column (“tall”) vector Yof dimension N.\\nFinally, you can think of the weights as a column vector wof size\\nD. Thus, the matrix-vector product a=Xwhas dimension N. In\\nparticular:\\nan=[Xw]n=å\\ndXn,dwd (7.28)\\nThis means, in particular, that ais actually the predictions of the\\nmodel. Instead of calling this a, we will call it ˆY. The squared error\\nsays that we should minimize1\\n2ån(ˆYn\\x00Yn)2, which can be written\\nin vector form as a minimization of1\\n2\\x0c\\x0c\\x0c\\x0cˆY\\x00Y\\x0c\\x0c\\x0c\\x0c2. Verify that the squared error can\\nactually be written as this vector\\nnorm.? This can be expanded visually as:\\n2\\n66664x1,1 x1,2 . . . x1,D\\nx2,1 x2,2 . . . x2,D\\n............\\nxN,1xN,2. . . xN,D3\\n77775\\n|{z }\\nX2\\n66664w1\\nw2\\n...\\nwD3\\n77775\\n|{z}\\nw=2\\n66664ådx1,dwd\\nådx2,dwd\\n...\\nådxN,dwd3\\n77775\\n|{z}\\nˆY\\x192\\n66664y1\\ny2\\n...\\nyN3\\n77775\\n|{z}\\nˆY\\n(7.29)linear models 99\\nSo, compactly, our optimization problem can be written as:\\nminwL(w) =1\\n2jjXw\\x00Yjj2+l\\n2jjwjj2(7.30)\\nIf you recall from calculus, you can minimize a function by setting its\\nderivative to zero. We start with the weights wand take gradients:\\nrwL(w) =X>(Xw\\x00Y)+lw (7.31)\\n=X>Xw\\x00X>Y+lw (7.32)\\n=\\x10\\nX>X+lI\\x11\\nw\\x00X>Y (7.33)\\nWe can equate this to zero and solve, yielding:\\n\\x10\\nX>X+lI\\x11\\nw\\x00X>Y=0 ( 7.34)\\n()\\x10\\nX>X+lID\\x11\\nw=X>Y (7.35)\\n() w=\\x10\\nX>X+lID\\x11\\n\\x001X>Y (7.36)\\nThus, the optimal solution of the weights can be computed by a few\\nmatrix multiplications and a matrix inversion. As a sanity check,\\nyou can make sure that the dimensions match. The matrix X>Xhas\\ndimension D\\x02D, and therefore so does the inverse term. The inverse\\nisD\\x02Dand X>isD\\x02N, so that product is D\\x02N. Multiplying through\\nby the N\\x021 vector Yyields a D\\x021 vector, which is precisely what we\\nwant for the weights. For those who are keen on linear\\nalgebra, you might be worried that\\nthe matrix you must invert might\\nnot be invertible. Is this actually a\\nproblem??Note that this gives an exact solution , modulo numerical innacu-\\nracies with computing matrix inverses. In contrast, gradient descent\\nwill give you progressively better solutions and will “eventually”\\nconverge to the optimum at a rate of 1/ k. This means that if you\\nwant an answer that’s within an accuracy of e=10\\x004, you will need\\nsomething on the order of one thousand steps.\\nThe question is whether getting this exact solution is always more\\nefﬁcient. To run gradient descent for one step will take O(ND)time,\\nwith a relatively small constant. You will have to run Kiterations,\\nyielding an overall runtime of O(KND ). On the other hand, the\\nclosed form solution requires constructing X>X, which takesO(D2N)\\ntime. The inversion take O(D3)time using standard matrix inver-\\nsion routines. The ﬁnal multiplications take O(ND)time. Thus, the\\noverall runtime is on the order O(D3+D2N). In most standard cases\\n(though this is becoming less true over time), N>D, so this is domi-\\nnated byO(D2N).\\nThus, the overall question is whether you will need to run more\\nthan D-many iterations of gradient descent. If so, then the matrix\\ninversion will be (roughly) faster. Otherwise, gradient descent will\\nbe (roughly) faster. For low- and medium-dimensional problems (say,100 a course in machine learning\\nD\\x14100), it is probably faster to do the closed form solution via\\nmatrix inversion. For high dimensional problems ( D\\x1510, 000), it is\\nprobably faster to do gradient descent. For things in the middle, it’s\\nhard to say for sure.\\n7.7Support Vector Machines\\nAt the beginning of this chapter, you may have looked at the convex\\nsurrogate loss functions and asked yourself: where did these come\\nfrom?! They are all derived from different underlying principles,\\nwhich essentially correspond to different inductive biases.\\nFigure 7.10: picture of data points with\\nthree hyperplanes, RGB with G the bestLet’s start by thinking back to the original goal of linear classiﬁers:\\nto ﬁnd a hyperplane that separates the positive training examples\\nfrom the negative ones. Figure 7.10shows some data and three po-\\ntential hyperplanes: red, green and blue. Which one do you like best?\\nMost likely you chose the green hyperplane. And most likely you\\nchose it because it was furthest away from the closest training points.\\nIn other words, it had a large margin. The desire for hyperplanes\\nwith large margins is a perfect example of an inductive bias. The data\\ndoes not tell us which of the three hyperplanes is best: we have to\\nchoose one using some other source of information.\\nFollowing this line of thinking leads us to the support vectorma-\\nchine (SVM). This is simply a way of setting up an optimization\\nproblem that attempts to ﬁnd a separating hyperplane with as large\\na margin as possible. It is written as a constrained optimiza tion\\nprob lem:\\nmin\\nw,b1\\ng(w,b)(7.37)\\nsubj. to yn(w\\x01xn+b)\\x151 (8n)\\nIn this optimization, you are trying to ﬁnd parameters that maximize\\nthe margin, denoted g, (i.e., minimize the reciprocal of the margin)\\nsubject to the constraint that alltraining examples are correctly classi-\\nﬁed.\\nFigure 7.11: hyperplane with margins\\non sidesThe “odd” thing about this optimization problem is that we re-\\nquire the classiﬁcation of each point to be greater than onerather than\\nsimply greater than zero. However, the problem doesn’t fundamen-\\ntally change if you replace the “ 1” with any other positive constant\\n(see Exercise ??). As shown in Figure 7.11, the constant one can be\\ninterpreted visually as ensuring that there is a non-trivial margin\\nbetween the positive points and negative points.\\nThe difﬁculty with the optimization problem in Eq ( 7.37) is what\\nhappens with data that is not linearly separable. In that case, there\\nis no set of parameters w,bthat can simultaneously satisfy all thelinear models 101\\nconstraints. In optimization terms, you would say that the feasible\\nregion isempty . (The feasible region is simply the set of all parame-\\nters that satify the constraints.) For this reason, this is refered to as\\nthehard -marginSVM , because enforcing the margin is a hard con-\\nstraint. The question is: how to modify this optimization problem so\\nthat it can handle inseparable data.\\nFigure 7.12: one bad point with slackThe key idea is the use of slack parameters. The intuition behind\\nslack parameters is the following. Suppose we ﬁnd a set of param-\\neters w,bthat do a really good job on 9999 data points. The points\\nare perfectly classifed and you achieve a large margin. But there’s\\none pesky data point left that cannot be put on the proper side of the\\nmargin: perhaps it is noisy. (See Figure 7.12.) You want to be able\\nto pretend that you can “move” that point across the hyperplane on\\nto the proper side. You will have to pay a little bit to do so, but as\\nlong as you aren’t moving a lotof points around, it should be a good\\nidea to do this. In this picture, the amount that you move the point is\\ndenoted x(xi).\\nBy introducing one slack parameter for each training example,\\nand penalizing yourself for having to use slack, you can create an\\nobjective function like the following, soft-marginSVM :\\nmin\\nw,b,x1\\ng(w,b)|{z}\\nlarge margin+Cå\\nnxn\\n|{z}\\nsmall slack(7.38)\\nsubj. to yn(w\\x01xn+b)\\x151\\x00xn (8n)\\nxn\\x150 (8n)\\nThe goal of this objective function is to ensure that all points are\\ncorrectly classiﬁed (the ﬁrst constraint). But if a point ncannot be\\ncorrectly classiﬁed, then you can set the slack xnto something greater\\nthan zero to “move” it in the correct direction. However, for all non-\\nzero slacks, you have to pay in the objective function proportional to\\nthe amount of slack. The hyperparameter C>0 controls overﬁtting\\nversus underﬁtting. The second constraint simply says that you must\\nnot have negative slack. What values of Cwill lead to over-\\nﬁtting? What values will lead to\\nunderﬁtting?? One major advantage of the soft-margin SVM over the original\\nhard-margin SVM is that the feasible region is never empty . That is,\\nthere is always going to be some solution, regardless of whether your\\ntraining data is linearly separable or not.\\nSuppose I give you a data set.\\nWithout even looking at the data,\\nconstruct for me a feasible solution\\nto the soft-margin SVM. What is\\nthe value of the objective for this\\nsolution??It’s one thing to write down an optimization problem. It’s another\\nthing to try to solve it. There are a very large number of ways to\\noptimize SVMs, essentially because they are such a popular learning\\nmodel. Here, we will talk just about one, very simple way. More\\ncomplex methods will be discussed later in this book once you have a\\nbit more background.102 a course in machine learning\\nTo make progress, you need to be able to measure the size of the\\nmargin. Suppose someone gives you parameters w,bthat optimize\\nthe hard-margin SVM. We wish to measure the size of the margin.\\nThe ﬁrst observation is that the hyperplane will lie exactly halfway\\nbetween the nearest positive point and nearest negative point. If not,\\nthe margin could be made bigger by simply sliding it one way or the\\nother by adjusting the bias b.\\nFigure 7.13: copy of ﬁgure from p 5of\\ncs544svm tutorialBy this observation, there is some positive example that that lies\\nexactly 1 unit from the hyperplane. Call it x+, so that w\\x01x++b=1.\\nSimilarly, there is some negative example, x\\x00, that lies exactly on\\nthe other side of the margin: for which w\\x01x\\x00+b=\\x001. These two\\npoints, x+and x\\x00give us a way to measure the size of the margin.\\nAs shown in Figure 7.11, we can measure the size of the margin by\\nlooking at the difference between the lengths of projections of x+\\nand x\\x00onto the hyperplane. Since projection requires a normalized\\nvector, we can measure the distances as:\\nd+=1\\njjwjjw\\x01x++b\\x001 ( 7.39)\\nd\\x00=\\x001\\njjwjjw\\x01x\\x00\\x00b+1 ( 7.40)\\nWe can then compute the margin by algebra:\\ng=1\\n2\\x02\\nd+\\x00d\\x00\\x03\\n(7.41)\\n=1\\n2\\x141\\njjwjjw\\x01x++b\\x001\\x001\\njjwjjw\\x01x\\x00\\x00b+1\\x15\\n(7.42)\\n=1\\n2\\x141\\njjwjjw\\x01x+\\x001\\njjwjjw\\x01x\\x00\\x15\\n(7.43)\\n=1\\n2\\x141\\njjwjj(+1)\\x001\\njjwjj(\\x001)\\x15\\n(7.44)\\n=1\\njjwjj(7.45)\\nThis is a remarkable conclusion: the size of the margin is inversely\\nproportional to the norm of the weight vector. Thus, maximizing the\\nmargin is equivalent to minimizing jjwjj!This serves as an addi-\\ntional justiﬁcation of the 2-norm regularizer: having small weights\\nmeans having large margins!\\nHowever, our goal wasn’t to justify the regularizer: it was to un-\\nderstand hinge loss. So let us go back to the soft-margin SVM and\\nplug in our new knowledge about margins:\\nmin\\nw,b,x1\\n2jjwjj2\\n|{z}\\nlarge margin+Cå\\nnxn\\n|{z}\\nsmall slack(7.46)linear models 103\\nsubj. to yn(w\\x01xn+b)\\x151\\x00xn (8n)\\nxn\\x150 (8n)\\nNow, let’s play a thought experiment. Suppose someone handed\\nyou a solution to this optimization problem that consisted of weights\\n(w) and a bias ( b), but they forgot to give you the slacks. Could you\\nrecover the slacks from the information you have?\\nIn fact, the answer is yes! For simplicity, let’s consider positive\\nexamples. Suppose that you look at some positive example xn. You\\nneed to ﬁgure out what the slack, xn, would have been. There are two\\ncases. Either w\\x01xn+bis at least 1 or it is not. If it’s large enough,\\nthen you want to set xn=0. Why? It cannot be less than zero by the\\nsecond constraint. Moreover, if you set it greater than zero, you will\\n“pay” unnecessarily in the objective. So in this case, xn=0. Next,\\nsuppose that w\\x01xn+b=0.2, so it is not big enough. In order to\\nsatisfy the ﬁrst constraint, you’ll need to set xn\\x150.8. But because\\nof the objective, you’ll not want to set it any larger than necessary, so\\nyou’ll set xn=0.8 exactly.\\nFollowing this argument through for both positive and negative\\npoints, if someone gives you solutions for w,b, you can automatically\\ncompute the optimal xvariables as:\\nxn=(\\n0 if yn(w\\x01xn+b)\\x151\\n1\\x00yn(w\\x01xn+b)otherwise(7.47)\\nIn other words, the optimal value for a slack variable is exactly the\\nhinge loss on the corresponding example! Thus, we can write the\\nSVM objective as an unconstrained optimization problem:\\nmin\\nw,b1\\n2jjwjj2\\n|{z}\\nlarge margin+Cå\\nn`(hin)(yn,w\\x01xn+b)\\n|{z}\\nsmall slack(7.48)\\nMultiplying this objective through by l/C, we obtain exactly the reg-\\nularized objective from Eq ( 7.8) with hinge loss as the loss function\\nand the 2-norm as the regularizer!\\n7.8Further Reading\\nTODO further reading8 | B IAS AND FAIRNESS\\nDependencies: Chapter 1,Chap-\\nter3,Chapter 4,Chapter 5At the end of Chapter 1, you saw the “Russian Tank” example of\\na biased data set leading to a classiﬁer that seemed like it was doing\\nwell, but was really relying on some artifact of the data collection\\nprocess. As machine learning algorithms have a greater and greater\\nimpact on the real world, it is crucially important to ensure that they\\nare making decisions based on the “right” aspects of the input, rather\\nthan exploiting arbitrary idiosyncracies of a particular training set.\\nFor the rest of this chapter, we will consider two real world ex-\\namples of bias issues that have had signiﬁcant impact: the effect of\\ngender in speech recognition systems and the effect of race in pre-\\ndicting criminal recidivism (i.e., will a convicted criminal commit\\nfurther crimes if released).1The gender issue is that early speech\\n1See Autoblog and ProPublica for press\\ncoverage of these two issues.recognition systems in cars failed to recognized the voices of many\\npeople who were not men. The race issue is that a speciﬁc recidivism\\npredictor based on standard learning algorithms was biased against\\nminorities.\\n8.1Train/Test Mismatch\\nOne of the most common issues in bias is a mismatch between the\\ntraining distribution and the testing distribution. In the running\\nexample of speech recognition failing to work on many non-men\\nspeakers, a large part of this happened because most of the training\\ndata on which the speech recognition system was trained was spoken\\nby men. The learning algorithm learned—very well—how to recog-\\nnize men’s speech, but its accuracy dropped signiﬁcantly when faced\\nwith a different distribution of test data.\\nTo understand why this happens, recall the Bayes Optimal clas-\\nsiﬁer from Chapter 2. This was the classiﬁer than magically know\\nthe data distribution D, and then when presented with an example x\\npredicted argmaxyD(x,y). This was optimal, but only becauseDwas\\nthe correct distribution. Even if one has access to the true distribu-\\ntion for male speakers, say D(male), the Bayes Optimal classiﬁer underLearning Objectives:\\n•Science and everyday life cannot\\nand should not be separated. – Rosalind Franklinbias and fairness 105\\nD(male)will generally notbe optimal under the distribution for any\\nother gender.\\nAnother example occurs in sentiment analysis. It is common to\\ntrain sentiment analysis systems on data collected from reviews:\\nproduct reviews, restaurant reviews, movie reviews, etc. This data is\\nconvenient because it includes both text (the review itself) and also\\na rating (typically one to ﬁve stars). This yields a “free” dataset for\\ntraining a model to predict rating given text. However, one should\\nbe wary when running such a sentiment classiﬁer on text other than\\nthe types of reviews it was trained on. One can easily imagine that\\na sentiment analyzer trained on movie reviews may not work so\\nwell when trying to detect sentiment in a politician’s speech. Even\\nmoving between one type of product and another can fail wildly:\\n“very small” typically expresses positive sentiment for USB drives,\\nbut not for hotel rooms.\\nThe issue of train/test mismatch has been widely studied under\\nmany different names: covariateshift (in statistics, the input features\\nare called “covariates”), sampleselection bias and domain adap ta-\\ntion are the most common. We will refer to this problem simply as\\nadap tation. The adaptation challenge is: given training data from one\\n“old” distribution, learn a classiﬁer that does a good job on another\\nrelated, but different, “new” distribution.\\nIt’s important to recognize that in general, adaptation is im-\\npossible. For example, even if the task remains the same (posi-\\ntive/negative sentiment), if the old distribution is text reviews and\\nthe new distribution is images of text reviews, it’s hard to imagine\\nthat doing well on the old distribution says anything about the new.\\nAs a less extreme example, if the old distribution is movie reviews in\\nEnglish and the new distribution is movie reviews in Mandarin, it’s\\nunlikely that adaptation will be easy.\\nThese examples give rise to the tension in adaptation problems.\\n1. What does it mean for two distributions to be related? We might\\nbelieve that “reviews of DVDs” and “reviews of movies” are\\n“highly related” (though somewhat different: DVD reviews of-\\nten discuss bonus features, quality, etc.); while “reviews of hotels”\\nand “reviews of political policies” are “less related.” But how can\\nwe formalize this?\\n2. When two distributions arerelated, how can we build models that\\neffectively share information between them?106 a course in machine learning\\n8.2Unsupervised Adaptation\\nThe ﬁrst type of adaptation we will cover is unsupervised adap ta-\\ntion. The setting is the following. There are two distributions, Dold\\nandDnew. We have labeled training data from Dold, say (x1,y1), . . . ,(xN,yN)\\ntotalling Nexamples. We also have Mmany unlabeled examples from\\nDnew:z1, . . . , zM. We assume that the examples live in the same\\nspace, RD. This is called unsupervised adaptation because we do not\\nhave access to any labels in the new distribution.2 2Sometimes this is called semi -super-\\nvised adap tation in the literature.Our goal is to learn a classiﬁer fthat achieves low expected loss\\nunder the new distribution, Dnew. The challenge is that we do not have\\naccess to any labeled data from Dnew. As a warm-up, let’s suppose\\nthat we have a black box machine learning algorithm Athat takes\\ninweighted examples and produces a classiﬁer. At the very least,\\nthis can be achieved using either undersampling or oversampling\\n(see Section 6.1). We’re going to attempt to reweigh the (old distri-\\nbution) labeled examples based on how similar they are to the new\\ndistribution. This is justiﬁed using the importance sampling trick for\\nswitching expectations:\\ntest loss ( 8.1)\\n=E(x,y)\\x18Dnew[`(y,f(x))] deﬁnition (8.2)\\n=å\\n(x,y)Dnew(x,y)`(y,f(x)) expand expectation (8.3)\\n=å\\n(x,y)Dnew(x,y)Dold(x,y)\\nDold(x,y)`(y,f(x)) times one (8.4)\\n=å\\n(x,y)Dold(x,y)Dnew(x,y)\\nDold(x,y)`(y,f(x)) rearrange (8.5)\\n=E(x,y)\\x18Dold\\x14Dnew(x,y)\\nDold(x,y)`(y,f(x))\\x15\\ndeﬁnition (8.6)\\nWhat we have achieved here is rewriting the test loss, which is an\\nexpectation overDnew, as an expectation over Doldinstead.3This3In this example, we assumed a discrete\\ndistribution; if the distributions are con-\\ntinuous, the sums are simply replaced\\nwith integrals.is useful because we have access to labeled examples from Dold\\nbut notDnew. The implicit suggested algorithm by this analysis\\nto to train a classiﬁer using our learning algorithm A, but where\\neach training example (xn,yn)is weighted according to the ratio\\nDnew(xn,yn)/Dold(xn,yn). Intuitively, this makes sense: the classiﬁer\\nis being told to pay more attention to training examples that have\\nhigh probability under the new distribution, and less attention to\\ntraining that have low probability under the new distribution.\\nThe problem with this approach is that we do not have access to\\nDneworDold, so we cannot compute this ratio and therefore cannot\\nrun this algorithm. One approach to this problem is to try to explic-bias and fairness 107\\nitly estimate these distributions, a task known as density estimation.\\nThis is an incredibly difﬁcult problem; far harder than the original\\nadaptation problem.\\nA solution to this problem is to try to estimate the ratio directly,\\nrather than separately estimating the two probability distributions4.4Bickel et al. 2007\\nThe key idea is to think of the adaptation as follows. All examples\\nare drawn according to some ﬁxed base distribution Dbase. Some\\nof these are selected to go into the new distribution, and some of\\nthem are selected to go into the old distribution. The mechanism for\\ndeciding which ones are kept and which are thrown out is governed\\nby a selection variable , which we call s. The choice of selection-or-\\nnot, s, is based only on the input example xand not on it’s label. In What could go wrong if sgot to\\nlook at the label, too??particular, we deﬁne:\\nDold(x,y)_Dbase(x,y)p(s=1jx) (8.7)\\nDnew(x,y)_Dbase(x,y)p(s=0jx) (8.8)\\nThat is, the probability of drawing some pair (x,y)in the old distri-\\nbution is proportional to the probability of ﬁrst drawing that example\\naccording to the base distribution, and then the probability of se-\\nlecting that particular example into the old distribution. If we can\\nsuccessfully estimate p(s=1jx), then the ratio that we sought, then\\nwe can compute the importance ratio as:\\nDnew(x,y)\\nDold(x,y)=1\\nZnewDbase(x,y)p(s=0jx)\\n1\\nZoldDbase(x,y)p(s=1jx)deﬁnition (8.9)\\n=1\\nZnewp(s=0jx)\\n1\\nZoldp(s=1jx)cancel base (8.10)\\n=Zp(s=0jx)\\np(s=1jx)consolidate (8.11)\\n=Z1\\x00p(s=1jx)\\np(s=1jx)binary selection (8.12)\\n=Z\\x141\\np(s=1jx)\\x001\\x15\\nrearrange (8.13)\\nThis means that if we can estimate the selection probability p(s=\\n1jx), we’re done. We can therefore use 1/ p(s=1jxn)\\x001 as an\\nexample weight on example (xn,yn)when feeding these examples\\ninto our learning algorithm A. As a check: make sure that these\\nweights are always non-negative.\\nFurthermore, why is it okay to\\nignore the Zfactor??The remaining question is how to estimate p(s=1jxn). Recall\\nthat s=1 denotes the case that xis selected into the old distribution\\nand s=0 denotes the case that xis selected into the new distribution.\\nThis means that predicting sisexactly a binary classiﬁcation problem,\\nwhere the “positive” class is the set of Nexamples from the old\\ndistribution and the “negative” class is the set of Mexamples from\\nthe new distribution.108 a course in machine learning\\nAlgorithm 23Selection Adaptation (h(xn,yn)iN\\nn=1,hzmiM\\nm=1,A)\\n1:Ddist h(xn,+1)iN\\nn=1Sh(zm,\\x001)iM\\nm=1// assemble data for distinguishing\\n// between old and new distributions\\n2:ˆp train logistic regression on Ddist\\n3:Dweighted D\\n(xn,yn,1\\nˆp(xn)\\x001)EN\\nn=1// assemble weight classiﬁcation\\n// data using selector\\n4:returnA(Dweighted) // train classiﬁer\\nThis analysis gives rise to Algorithm 8.2, which consists of essen-\\ntially two steps. The ﬁrst is to train a logistic regression classiﬁer5to5The use of logistic regression is arbi-\\ntrary: it need only be a classiﬁcation\\nalgorithm that can produce probabili-\\nties.distinguish between old and new distributions. The second is to use\\nthat classiﬁer to produce weights on the labeled examples from the\\nold distribution and then train whatever learning algorithm you wish\\non that.\\nIn terms of the questions posed at the beginning of this chapter,\\nthis approach to adaptation measures nearness of the two distribu-\\ntions by the degree to which the selection probability is constant. In\\nparticular, if the selection probability is independent of x, then the\\ntwo distributions are identical. If the selection probabilities vary sigi-\\nﬁcantly as xchanges, then the two distributions are considered very\\ndifferent. More generally, if it is easy to train a classiﬁer to distin-\\nguish between the old and new distributions, then they are very\\ndifferent.\\nIn the case of speech recognition failing as a function of gender, a\\ncore issue is that speech from men was massively over-represented\\nin the training data but not the test data. When the selection logistic\\nregression is trained, it is likely to say “old” on speech from men and\\n“new” on other speakers, thereby downweighting the signiﬁcance of\\nmale speakers and upweighting the signiﬁcance of speakers of other\\ngenders on the ﬁnal learned model. This would (hopefully) address\\nmany of the issues confounding that system. Make up percentages for fraction\\nof speakers who are male in the\\nold and new distributions; estimate\\n(you’ll have to make some assump-\\ntions) what the importance weights\\nwould look like in this case.?8.3Supervised Adaptation\\nUnsupervised adaptation is very challenging because we never get to\\nsee try labels in the new distribution. In many ways, unsupervised\\nadaptation attempts to guard against bad things happening. That is,\\nif an old distribution training example looks very unlike the new\\ndistribution, it (and consequently it’s features) are downweighted so\\nmuch as to be ignored. In supervised adaptation, we can hope for\\nmore: we can hope to actually do better on the new distribution than\\nthe old because we have labeled data there.\\nThe typical setup is similar to the unsupervised case. There arebias and fairness 109\\nAlgorithm 24EasyAdapt (h(x(old)\\nn,y(old)\\nn)iN\\nn=1,h(x(new)\\nm,y(new)\\nm)iM\\nm=1,A)\\n1:D D\\n(hx(old)\\nn,x(old)\\nn,0i,y(old)\\nn)EN\\nn=1SD\\n(hx(new)\\nm,0,x(new)\\nmi,y(new)\\nm)EM\\nm=1// union\\n// of transformed data\\n2:returnA(D) // train classiﬁer\\ntwo distributions, DoldandDnew, and our goal is to learn a classi-\\nﬁer that does well in expectation on Dnew. However, now we have\\nlabeled data from both distributions: Nlabeled examples from Dold\\nand Mlabeled examples from Dnew. Call themhx(old)\\nn,y(old)\\nniN\\nn=1from\\nDoldandhx(new)\\nm,y(new)\\nmiM\\nm=1fromDnew. Again, suppose that both x(old)\\nn\\nand x(new)\\nm both live in RD.\\nOne way of answer the question of “how do we share informa-\\ntion between the two distributions” is to say: when the distributions\\nagree on the value of a feature, let them share it, but when they dis-\\nagree, allow them to learn separately. For instance, in a sentiment\\nanalysis task,Doldmight be reviews of electronics and Dnewmight\\nbe reviews of hotel rooms. In both cases, if the review contains the\\nword “awesome” then it’s probably a positive review, regardless of\\nwhich distribution we’re in. We would want to share this information\\nacross distributions. On the other hand, “small” might be positive\\nin electronics and negative in hotels, and we would like the learning\\nalgorithm to be able to learn separate information for that feature.\\nA very straightforward way to accomplish this is the feature aug-\\nmen tation approach6. This is a simple preprocessing step after which6Daumé III 2007\\none can apply any learning algorithm. The idea is to create three ver-\\nsions of every feature: one that’s shared (for words like “awesome”),\\none that’s old-distribution-speciﬁc and one that’s new-distribution-\\nspeciﬁc. The mapping is:\\nshared old-only new-only\\nx(old)\\nn7!D\\nx(old)\\nn ,x(old)\\nn , 0, 0, . . . , 0|{z}\\nD-manyE\\n(8.14)\\nx(new)\\nm7!D\\nx(new)\\nm , 0, 0, . . . , 0|{z}\\nD-many,x(new)\\nmE\\n(8.15)\\nOnce you’ve applied this transformation, you can take the union\\nof the (transformed) old and new labeled examples and feed the en-\\ntire set into your favorite classiﬁcation algorithm. That classiﬁcation\\nalgorithm can then choose to share strength between the two distri-\\nbutions by using the “shared” features, if possible; or, if not, it can\\nlearn distribution-speciﬁc properties on the old-only or new-only\\nparts. This is summarized in Algorithm 8.3.\\nNote that this algorithm can be combined with the instance weight-110 a course in machine learning\\ning (unsupervised) learning algorithm. In this case, the logistic re-\\ngression separator should be trained on the untransformed data, and\\nthen weights can be used exactly as in Algorithm 8.2. This is particu- Why is it crucial that the separator\\nbe trained on the untransformed\\ndata?? larly useful if you have access to way more old distribution data than\\nnew distribution data, and you don’t want the old distribution data\\nto “wash out” the new distribution data.\\nAlthough this approach is general, it is most effective when the\\ntwo distributions are “not too close but not too far”:\\n• If the distributions are too far, and there’s little information to\\nshare, you’re probably better off throwing out the old distribution\\ndata and training just on the (untransformed) new distribution\\ndata.\\n• If the distributions are too close, then you might as well just take\\nthe union of the (untransformed) old and new distribution data,\\nand training on that.\\nIn general, the interplay between how far the distributions are and\\nhow much new distribution data you have is complex, and you\\nshould always try “old only” and “new only” and “simple union”\\nas baselines.\\n8.4Fairness and Data Bias\\nAlmost any data set in existence is biased in some way, in the sense\\nthat it captures an imperfect view of the world. The degree to which\\nthis bias is obvious can vary greatly:\\n• There might be obvious bias in the labeling. For instance, in crimi-\\nnology, learning to predict sentence lengths by predicting the sen-\\ntences assigned by judges will simply learn to reproduce whatever\\nbias already exists in judicial sentencing.\\n• There might be sample selection bias, as discussed early. In the\\nsame criminology example, the only people for which we have\\ntraining data are those that have already been arrested, charged\\nwith and convicted of a crime; these processes are inherantly bi-\\nased, and so any model learned on this data may exhibit similar\\nbiases.\\n• The task itself might be biased because the designers had blindspots.\\nAn intelligent billboard that predicts the gender of the person\\nwalking toward it so as to show “better” advertisements may be\\ntrained as a binary classiﬁer between male/female and thereby\\nexcludes anyone who does not fall in the gender binary. A similar\\nexample holds for a classiﬁer that predicts political afﬁliation inbias and fairness 111\\nthe US as Democrat/Republican, when in fact there are far more\\npossibilities.\\n• There might be bias in the features or model structure. Machine\\ntranslation systems often exhibit incorrect gender stereotypes7 7For instance, many languages have\\nverbal marking that agrees with the\\ngender of the subject. In such cases,\\n“the doctor treats . . . ” puts masculine\\nmarkers on the translation of “treats”\\nbut “the nurse treats . . . ” uses feminine\\nmarkers.because the relevant context, which would tell them the correct\\ngender, is not encoded in the feature representation. Alone, or\\ncoupled with biased data, this leads to errors.\\n• The loss function may favor certain types of errors over others.\\nYou’ve already seen such an example: using zero/one loss on\\na highly imbalanced class problem will often lead to a learned\\nmodel that ignores the minority class entirely.\\n• A deployed system creates feedback loops when it begins to con-\\nsume it’s own output as new input. For instance, once a spam\\nﬁlter is in place, spammers will adjust their strategies, leading to\\ndistribution shift in the inputs. Or a car guidance system for pre-\\ndicting which roads are likely to be unoccupied may ﬁnd those\\nroads now occupied by other cars that it directed there.\\nThese are all difﬁcult questions, none of which has easy answers.\\nNonetheless, it’s important to be aware of how our systems may\\nfail, especially as they take over more and more of our lives. Most\\ncomputing, engineering and mathematical societies (like the ACM,\\nIEEE, BCS, etc.) have codes of ethics, all of which include a statement\\nabout avoiding harm; the following is taken from the ACM code8:8ACM Code of Ethics\\nTo minimize the possibility of indirectly harming others, computing\\nprofessionals must minimize malfunctions by following generally\\naccepted standards for system design and testing. Furthermore, it is\\noften necessary to assess the social consequences of systems to project\\nthe likelihood of any serious harm to others.\\nIn addition to ethical questions, there are often related legal ques-\\ntions. For example, US law prohibits discrimination by race and gen-\\nder (among other “protected attributes”) in processes like hiring and\\nhousing. The current legal mechanism for measuring discimination is\\ndisparate impact and the 80%rule. Informally, the 80% rule says that\\nyour rate of hiring women (for instance) must be at least 80% of your\\nrate of hiring men. Formally, the rule states:\\nPr(y= + 1jG6=male)\\x150.8\\x02Pr(y= + 1jG=male) (8.16)\\nOf course, gender/male can be replaced with any other protected\\nattribute.\\nOne non-solution to the disparate impact problem is to simply\\nthrow out protected attributes from your dataset. Importantly, yet un-\\nfortunately, this is notsufﬁcient. Many other features often correlate112 a course in machine learning\\nstrongly with gender, race, and other demographic information, and\\njust because you’ve removed explicit encodings of these factors does\\nnot imply that a model trained as such would satisfy the 80% rule. A\\nnatural question to ask is: can we build machine learning algorithms\\nthat have high accuracy but simultaneously avoid disparate impact?\\nDisparate impact is an imperfect measure of (un)fairness, and\\nthere are alternatives, each with it’s own pros and cons9. All of these9Friedler et al. 2016 , Hardt et al. 2016\\nrely on predeﬁned categories of protected attributes, and a natural\\nquestion is where these come from if not governmental regulation.\\nRegardless of the measure you choose, the most important thing to\\nkeep in mind is that just because something comes from data, or is\\nalgorithmically implemented, does not mean it’s fair.\\n8.5How Badly can it Go?\\nTo help better understand how badly things can go when the distri-\\nbution over inputs changes, it’s worth thinking about how to analyze\\nthis situation formally. Suppose we have two distributions Doldand\\nDnew, and let’s assume that the only thing that’s different about these\\nis the distribution they put on the inputs Xand not the outputs Y.\\n(We will return later to the usefulness of this assumption.) That is:\\nDold(x,y) =Dold(x)D(yjx)andDnew(x,y) =Dnew(x)D(yjx),\\nwhereD(yjx)is shared between them.\\nLet’s say that you’ve learned a classiﬁer fthat does really well on\\nDoldand achieves some test error ofe(old). That is:\\ne(old)=Ex\\x18Dold\\ny\\x18D(\\x01jx)\\x02\\n1[f(x)6=y]\\x03\\n(8.17)\\nThe question is: how badly can fdo on the new distribution?\\nWe can calculate this directly.\\ne(new)\\n=Ex\\x18Dnew\\ny\\x18D(\\x01jx)\\x02\\n1[f(x)6=y]\\x03\\n(8.18)\\n=Z\\nXZ\\nYDnew(x)D(yjx)1[f(x)6=y]dydx def. ofE(8.19)\\n=Z\\nX\\x10\\nDnew(x)\\x00Dold(x) +Dold(x)\\x11\\n\\x02\\nZ\\nYD(yjx)1[f(x)6=y]dydx add zero (8.20)\\n=e(old)+\\nZ\\nX\\x10\\nDnew(x)\\x00Dold(x)\\x11\\n\\x02\\nZ\\nYD(yjx)1[f(x)6=y]dydx def.eold(8.21)bias and fairness 113\\n\\x14e(old)+Z\\nX\\x0c\\x0c\\x0cDnew(x)\\x00Dold(x)\\x0c\\x0c\\x0c\\x10\\n1\\x11\\ndx worst caseZ\\nY(8.22)\\n=e(old)+2\\x0c\\x0c\\x0cDnew\\x00Dold\\x0c\\x0c\\x0c\\nVardef.j\\x01jVar(8.23)\\nHere,j\\x01jVaris the totalvariation distance (orvariational distance )\\nbetween two probability distributions, deﬁned as:\\njP\\x00QjVar=sup\\nejP(e)\\x00Q(e)j=1\\n2Z\\nXjP(x)\\x00Q(x)jdx (8.24)\\nIs a standard measure of dissimilarity between probability distribu-\\ntions. In particular, the variational distance is the largest difference\\nin probability that Pand Qassign to the same event (in this case, the\\nevent is an example x).\\nThe bound tells us that we might not be able to hope for very\\ngood error on the new distribution, even if we have very good error\\non the old distribution, when DoldandDneware very different (as-\\nsign very different probabilities to the same event). Of course, this is\\nanupper bound , and possibly a very loose one.\\nThe second observation is that we barely used the assumption that\\nDnewandDoldshare the same label distribution D(yjx)in the above\\nanalysis. (In fact, as an exercise, repeat the analysis without this as-\\nsumption. It will still go through.) In general, this assumption buys\\nus very little. In an extreme case, DnewandDoldcan essentially “en-\\ncode” which distribution a given xcame from in one of the features.\\nOnce the origin distribution is completely encoded in a feature, then\\nD(yjx)could look at the encoding, and completely ﬂip the label\\nbased on which distribution it’s from. How could DnewandDold\\nencode the distribution? One way would be to set the 29th decimal\\ndigit of feature 1to an odd value in the old distribution and an even\\nvalue in the new distribution. This tiny change will be essentially im-\\nperceptible if one looks at the data, but would give D(yjx)enough\\npower to make our lives miserable.\\nIf we want a way out of this dilemma, we need more technology.\\nThe core idea is that if we’re learning a function ffrom some hypoth-\\nesis classF, and this hypothesis class isn’t rich enough to peek at the\\n29th decimal digit of feature 1, then perhaps things are not as bad\\nas they could be. This motivates the idea of looking at a measure of\\ndistance between probability distributions that depends on the hypoth-\\nesis class . A popular measure is the dA-distance or the discrep ancy .\\nThe discrepancy measure distances between probability distributions\\nbased on how much two function fand f0in the hypothesis class can\\ndisagree on their labels. Let:\\neP(f,f0) =Ex\\x18Ph\\n1[f(x)6=f0(x)]i\\n(8.25)114 a course in machine learning\\nYou can think of eP(f,f0)as the error off0when the ground truth is\\ngiven by f, where the error is taken with repsect to examples drawn\\nfrom P. Given a hypothesis class F, the discrepancy between Pand\\nQis deﬁned as:\\ndA(P,Q) = max\\nf,f02F\\x0c\\x0ceP(f,f0)\\x00eQ(f,f0)\\x0c\\x0c (8.26)\\nThe discrepancy very much has the ﬂavor of a classiﬁer: if you think\\noffas providing the ground truth labeling, then the discrepancy is\\nlarge whenever there exists a function f0that agrees strongly with f\\non distribution Pbutdisagrees strongly with fonQ. This feels natu-\\nral: if all functions behave similarly on Pand Q, then the discrepancy\\nwill be small, and we also expect to be able to generalize across these\\ntwo distributions easily.\\nOne very attractive property of the discrepancy is that you can\\nestimate it from ﬁnite unlabeled samples fromDoldandDnew. Al-\\nthough not obvious at ﬁrst, the discrepancy is very closely related\\nto a quantity we saw earlier in unsupervised adaptation: a classiﬁer\\nthat distinguishes between DoldandDnew. In fact, the discrepancy is\\nprecisely twice the accuracy of the best classiﬁer from Hat separating\\nDoldfromDnew.\\nHow does this work in practice? Exactly as in the section on un-\\nsupervised adaptation, we train a classiﬁer to distinguish between\\nDoldandDnew. It needn’t be a probabilistic classiﬁer; any binary\\nclassiﬁer is ﬁne. This classiﬁer, the “domain separator,” will have\\nsome (heldout) accuracy, call it acc. The discrepancy is then exactly\\ndA=2(acc\\x000.5).\\nIntuitively the accuracy of the domain separator is a natural mea-\\nsure of how different the two distributions are. If the two distribu-\\ntions are identical, you shouldn’t expect to get very good accuracy at\\nseparating them. In particular, you expect the accuracy to be around\\n0.5, which puts the discrepancy at zero. On the other hand, if the two\\ndistributions are really far apart, separation is easy, and you expect\\nan accuracy of about 1, yielding a discrepancy also of about 1.\\nOne can, in fact, prove a generalization bound—generalizing from\\nﬁnite samples from Doldto expected loss on Dnew—based on the\\ndiscrepancy10.10Ben-David et al. 2007\\nTheorem 9(Unsupervised Adaptation Bound) .Given a ﬁxed rep-\\nresentation and a ﬁxed hypothesis space F, let f2F and let e(best)=\\nmin f\\x032F1\\n2\\x02\\ne(old)(f\\x03) +e(new)(f\\x03)\\x03\\n, then, for all f2F:\\ne(new)(f)|{z}\\nerror onDnew\\x14e(old)(f)|{z}\\nerror onDold+ e(best)\\n|{z}\\nminimal avg error+dA(Dold,Dnew)|{z}\\ndistance(8.27)\\nIn this bound, e(best)denotes the error rate of the best possible\\nclassiﬁer fromF, where the error rate is measured as the averagebias and fairness 115\\nerror rate onDnewandDold; this term ensures that at least some\\ngood classiﬁer exists that does well on both distributions.\\nThe main practical use of this result is that it suggests a way to\\nlook for representations that are good for adaptation: on the one\\nhand, we should try to get our training error (on Dold) as low as\\npossible; on the other hand, we want to make sure that it is hard to\\ndistinguish between DoldandDnewin this representation.\\n8.6Further Reading\\nTODO further reading9 | P ROBABILISTIC MODELING\\nDependencies:Many of the models and algorithms you have learned about\\nthus far are relatively disconnected . There is an alternative view of\\nmachine learning that unites and generalizes much of what you have\\nalready learned. This is the prob abilis ticmod elingframework, in\\nwhich you will explicitly think of learning as a problem of statistical\\ninference .\\nIn this chapter, you will learn about two ﬂavors of probabilistic\\nmodels: generative and conditional. You will see that many of the ap-\\nproaches (both supervised and unsupervised) we have seen already\\ncan be cast as probabilistic models. Through this new view, you will\\nbe able to develop learning algorithms that have inductive biases\\ncloser to what you, as a designer, believe. Moreover, the two chap-\\nters that follow will make heavy use of the probabilistic modeling\\napproach to open doors to other learning problems.\\n9.1Classiﬁcation by Density Estimation\\nRecall from Chapter 2that if we had access to the underlying prob-\\nability distribution D, then we could form a Bayes optimal classiﬁer\\nas:\\nf(BO)(ˆx) =arg max\\nˆy2YD(ˆx,ˆy) (9.1)\\nUnfortunately, no one gave you this distribution, but the optimality\\nof this approach suggests that good way to build a classiﬁer is to\\ntry to estimateD. In other words, you try to learn a distribution ˆD,\\nwhich you hope to very similar to D, and then use this distribution\\nfor classiﬁcation. Just as in the preceding chapters, you can try to\\nform your estimate of Dbased on a ﬁnite training set.\\nThe most direct way that you can attempt to construct such a\\nprobability distribution is to select a family of parametric distribu-\\ntions. For instance, a Gaussian (or Normal) distribution is parametric:\\nit’s parameters are its mean and covariance. The job of learning isLearning Objectives:\\n• Deﬁne the generative story for a\\nnaive Bayes classiﬁer.\\n• Derive relative frequency as the so-\\nlution to a constrained optimization\\nproblem.\\n• Compare and contrast generative,\\nconditional and discriminative\\nlearning.\\n• Explain when generative models are\\nlikely to fail.\\n• Derive logistic loss with an `2\\nregularizer from a probabilistic\\nperspective.The world is noisy and messy. Y ou need to deal with the noise\\nand uncertainty. – Daphne Kollerprobabilistic modeling 117\\nA probability distribution pspeciﬁes the likelihood of an event e, where p(e)2[0, 1]. It’s often con-\\nvenient to think of events as “conﬁgurations of the world”, so p(e)says “how likely is it that the\\nworld is in conﬁguration e.” Often world conﬁgurations are built up of smaller pieces, for instance you\\nmight say “ e= the conﬁguration in which it is rainy, windy and cold.” Formally, we might write this as\\n“e=fWeather =rainy, Wind =windy, Temperature =coldg”, where we’ve used a convention that\\nrandom variable s (like Temperature) are capitalized and their instantiations (like cold) are lower case.\\nConsidering this event, we want to evaluate p(Weather =rainy, Wind =windy, Temperature =cold),\\nor more generally p(A=a,B=b,C=c)for some random variables A,Band C, and some instantia-\\ntions of those random variables a,band crespectively.\\nThere are a few standard rules of probability that we will use regularly:\\nsum -to-one: if you sum over all possible conﬁgurations of the world, psums to one: åep(E=e) =1.\\nmarginal ization: you can sum out one random variable to remove it from the world: åap(A=a,B=\\nb) =p(B=b).\\nchain rule: if a world conﬁguration consists of two or more random variables, you can evaluate the\\nlikelihood of the world one step at a time: p(A=a,B=b) = p(A=a)p(B=bjA=a). Events are\\nunordered, so you can also get p(A=a,B=b) =p(B=b)p(A=ajB=b).\\nBayes rule: combining the two chain rule equalities and dividing, we can relate a conditional proba-\\nbility in one direction with that in the other direction: p(A=ajB=b) = p(A=a)p(B=bjA=\\na)/p(B=b).MATHREVIEW | RULES OF PROBABILITY\\nFigure 9.1:\\nthen to infer which parameters are “best” as far as the observed train-\\ning data is concerned, as well as whatever inductive bias you bring.\\nA key assumption that you will need to make is that the training data\\nyou have access to is drawn independently fromD. In particular, as\\nyou draw examples (x1,y1)\\x18D then(x2,y2)\\x18D and so on, the\\nnth draw (xn,yn)is drawn fromDand does not otherwise depend\\non the previous n\\x001 samples. This assumption is usually false, but\\nis also usually sufﬁciently close to being true to be useful. Together\\nwith the assumption that all the training data is drawn from the same\\ndistributionDleads to the i.i.d. assump tion orindependently and\\niden tically distributed assumption. This is a key assumption in al-\\nmost all of machine learning.\\n9.2Statistical Estimation\\nSuppose you need to model a coin that is possibly biased (you can\\nthink of this as modeling the label in a binary classiﬁcation problem),\\nand that you observe data HHTH (where Hmeans a ﬂip came up heads118 a course in machine learning\\nand Tmeans it came up tails). You can assume that all the ﬂips came\\nfrom the same coin, and that each ﬂip was independent (hence, the\\ndata was i.i.d.). Further, you may choose to believe that the coin has\\na ﬁxed probability bof coming up heads (and hence 1 \\x00bof coming\\nup tails). Thus, the parameter of your model is simply the scalar b. Describe a case in which at least\\none of the assumptions we are\\nmaking about the coin ﬂip is false.? The most basic computation you might perform is max imum like-\\nlihood estimation: namely, select the paramter bthe maximizes the\\nprobability of the data under that parameter. In order to do so, you\\nneed to compute the probability of the data:\\npb(D) =pb(HHTH) deﬁnition of D (9.2)\\n=pb(H)pb(H)pb(T)pb(H) data is independent (9.3)\\n=bb(1\\x00b)b (9.4)\\n=b3(1\\x00b) (9.5)\\n=b3\\x00b4(9.6)\\nThus, if you want the parameter bthat maximizes the probability of\\nthe data, you can take the derivative of b3\\x00b4with respect to b, set\\nit equal to zero and solve for b:\\n¶\\n¶bh\\nb3\\x00b4i\\n=3b2\\x004b3(9.7)\\n4b3=3b2(9.8)\\n() 4b=3 ( 9.9)\\n()b=3\\n4(9.10)\\nThus, the maximum likelihood bis 0.75, which is probably what\\nyou would have selected by intuition. You can solve this problem\\nmore generally as follows. If you have H-many heads and T-many\\ntails, the probability of your data sequence is bH(1\\x00b)T. You can\\ntry to take the derivative of this with respect to band follow the\\nsame recipe, but all of the products make things difﬁcult. A more\\nfriendly solution is to work with the loglikelihood orlogprob a-\\nbilityinstead. The log likelihood of this data sequence is Hlogb+\\nTlog(1\\x00b). Differentiating with respect to b, you get H/b\\x00T/(1\\x00\\nb). To solve, you obtain H/b=T/(1\\x00b)soH(1\\x00b) = Tb.\\nThus H\\x00Hb=Tband so H= (H+T)b, ﬁnally yeilding that\\nb=H/(H+T)or, simply, the fraction of observed data that came up\\nheads. In this case, the maximum likelihood estimate is nothing but\\nthe relative frequency of observing heads! How do you know that the solution\\nofb= H/(H+T)is actually a\\nmaximum?? Now, suppose that instead of ﬂipping a coin, you’re rolling a K-\\nsided die (for instance, to pick the label for a multiclass classiﬁcation\\nproblem). You might model this by saying that there are parameters\\nq1,q2, . . . , qKspecifying, respectively, the probabilities that any givenprobabilistic modeling 119\\nside comes up on a role. Since these are themselves probabilities,\\neach qkshould be at least zero, and the sum of the qks should be one.\\nGiven a data set that consists of x1rolls of 1, x2rolls of 2 and so on,\\nthe probability of this data is Õkqxk\\nk, yielding a log probability of\\nåkxklogqk. If you pick some particular parameter, say q3, the deriva-\\ntive of this with respect to q3isx3/q3, which you want to equate to\\nzero. This leads to. . . q3!¥.\\nThis is obviously “wrong.” From the mathematical formulation,\\nit’s correct: in fact, setting all of the qks to¥does maximize Õkqxk\\nkfor\\nany (non-negative) xks. The problem is that you need to constrain the\\nqs to sum to one. In particular, you have a constraint that åkqk=1\\nthat you forgot to enforce. A convenient way to enforce such con-\\nstraints is through the technique of Lagrange multipliers. To make\\nthis problem consistent with standard minimization problems, it is\\nconvenient to minimize negative log probabilities, instead of maxi-\\nmizing log probabilities. Thus, the constrainted optimization problem\\nis:\\nmin\\nq\\x00å\\nkxklogqk (9.11)\\nsubj. to å\\nkqk\\x001=0\\nThe Lagrange multiplier approach involves adding a new variable l\\nto the problem (called the Lagrange variable ) corresponding to the\\nconstraint, and to use that to move the constraint into the objective.\\nThe result, in this case, is:\\nmax\\nlmin\\nq\\x00å\\nkxklogqk\\x00l \\nå\\nkqk\\x001!\\n(9.12)\\nTurning a constrained optimization problem into it’s corresponding\\nLagrangian is straightforward. The mystical aspect is why it works.\\nIn this case, the idea is as follows. Think of las an adversary: lis\\ntrying to maximize this function (you’re trying to minimize it). If\\nyou pick some parameters qthat actually satisfy the constraint, then\\nthe green term in Eq ( 9.12) goes to zero, and therefore ldoes not\\nmatter: the adversary cannot do anything. On the other hand, if the\\nconstraint is even slightly unsatisﬁed, then lcan tend toward +¥\\nor\\x00¥to blow up the objective. So, in order to have a non-inﬁnite\\nobjective value, the optimizer must ﬁnd values of qthat satisfy the\\nconstraint.\\nIf we solve the inner optimization of Eq ( 9.12) by differentiating\\nwith respect to q1, we get x1/q1=l, yielding q1=x1/l. In general,\\nthe solution is qk=xk/l. Remembering that the goal of lis to\\nenforce the sums-to-one constraint, we can set l=åkxkand verify120 a course in machine learning\\nthat this is a solution. Thus, our optimal qk=xk/åkxk, which again\\ncompletely corresponds to intuition.\\n9.3Naive Bayes Models\\nNow, consider the binary classiﬁcation problem. You are looking for\\naparameterized probability distribution that can describe the training\\ndata you have. To be concrete, your task might be to predict whether\\na movie review is positive or negative (label) based on what words\\n(features) appear in that review. Thus, the probability for a single data\\npoint can be written as:\\npq((y,x)) = pq(y,x1,x2, . . . , xD) (9.13)\\nThe challenge in working with a probability distribution like Eq ( 9.13)\\nis that it’s a distribution over a lotof variables. You can try to sim-\\nplify it by applying the chain rule of probabilities:\\npq(x1,x2, . . . , xD,y) =pq(y)pq(x1jy)pq(x2jy,x1)pq(x3jy,x1,x2)\\n\\x01\\x01\\x01pq(xDjy,x1,x2, . . . , xD\\x001) (9.14)\\n=pq(y)Õ\\ndpq(xdjy,x1, . . . , xd\\x001) (9.15)\\nAt this point, this equality is exact for any probability distribution.\\nHowever, it might be difﬁcult to craft a probability distribution for\\nthe 10000th feature, given the previous 9999. Even if you could, it\\nmight be difﬁcult to accurately estimate it. At this point, you can\\nmake assumptions. A classic assumption, called the naive Bayes as-\\nsump tion, is that the features are independent, conditioned on the label.\\nIn the movie review example, this is saying that once you know that\\nit’s a positive review , the probability that the word “excellent” appears\\nis independent of whether “amazing” also appeared. (Note that\\nthis does notimply that these words are independent when you\\ndon’t know the label—they most certainly are not.) Formally this\\nassumption states that:\\nAssumption: p(xdjy,xd0) =p(xdjy),8d6=d0(9.16)\\nUnder this assumption, you can simplify Eq ( 9.15) to:\\npq((y,x)) = pq(y)Õ\\ndpq(xdjy) naive Bayes assumption (9.17)\\nAt this point, you can start parameterizing p. Suppose, for now,\\nthat your labels are binary andyour features are also binary. In this\\ncase, you could model the label as a biased coin, with probability of\\nheads (e.g., positive review) given by q0. Then, for each label, youprobabilistic modeling 121\\ncan imagine having one (biased) coin for each feature. So if there are\\nD-many features, you’ll have 1 +2Dtotal coins: one for the label\\n(call it q0) and one for each label/feature combination (call these q+1\\nand as q\\x001). In the movie review example, we might expect q0\\x190.4\\n(forty percent of movie reviews are positive) and also that q+1might\\ngive high probability to words like “excellent” and “amazing” and\\n“good” and q\\x001might give high probability to words like “terrible”\\nand “boring” and “hate”. You can rewrite the probability of a single\\nexample as follows, eventually leading to the log probability of the\\nentire data set:\\npq((y,x)) = pq(y)Õ\\ndpq(xdjy) naive Bayes assumption\\n(9.18)\\n=\\x10\\nq[y=+1]\\n0(1\\x00q0)[y=\\x001]\\x11\\nÕ\\ndq[xd=1]\\n(y),d(1\\x00q(y),d)[xd=0]model assumptions\\n(9.19)\\nSolving for q0is identical to solving for the biased coin case from\\nbefore: it is just the relative frequency of positive labels in your data\\n(because q0doesn’t depend on xat all). For the other parameters,\\nyou can repeat the same exercise as before for each of the 2 Dcoins\\nindependently. This yields:\\nˆq0=1\\nNå\\nn[yn= + 1] (9.20)\\nˆq(+1),d=ån[yn= + 1^xn,d=1]\\nån[yn= + 1](9.21)\\nˆq(\\x001),d=ån[yn=\\x001^xn,d=1]\\nån[yn=\\x001](9.22)\\nIn the case that the features are notbinary, you need to choose a dif-\\nferent model for p(xdjy). The model we chose here is the Bernouilli\\ndistribution, which is effectively a distribution over independent\\ncoin ﬂips. For other types of data, other distributions become more\\nappropriate. The die example from before corresponds to a discrete\\ndistribution. If the data is continuous, you might choose to use a\\nGaus sian distribution (aka Normal distribution). The choice of dis-\\ntribution is a form of inductive bias by which you can inject your\\nknowledge of the problem into the learning algorithm.\\n9.4Prediction\\nConsider the predictions made by the naive Bayes model with Bernoulli\\nfeatures in Eq ( 9.18). You can better understand this model by con-\\nsidering its decision boundary. In the case of probabilistic models,122 a course in machine learning\\nThere are a few common probability distributions that we use in this book. The ﬁrst is the Bernouilli\\ndistribution, which models binary outcomes (like coin ﬂips). A Bernouilli distribution, Ber(q)is pa-\\nrameterized by a single scalar value q2[0, 1]that represents the probability of heads. The likelihood\\nfunction isBer(xjq) = qx(1\\x00q)1\\x00x. The generalization of the Bernouilli to more than two possible\\noutcomes (like rolls of a die) is the Discrete distribution, Disc(th). If the die has Ksides, then q2RK\\nwith all entries non-negative and åkqk= 1.qkis the probabability that the die comes up on side k.\\nThe likelihood function is Disc(xjq) = Õkq1[x=k]\\nk. The Binomial distribution is just like the Bernouilli\\ndistribution but for multiple ﬂips of the rather than a single ﬂip; it’s likelihood is (Bin(kjn,q)=n\\nkqk(1\\x00q)n\\x00k), where n\\nis the number of ﬂips and kis the number of heads. The Multinomial distribution extends the Discrete\\ndistribution also to multiple rolls; it’s likelihood is Mult(xjn,q) =n!\\nÕkxk!Õkqxk\\nk, where nis the total\\nnumber of rolls and xkis the number of times the die came up on side k(soåkxk=n). The preceding\\ndistributions are all discrete.\\nThere are two common continuous distributions we need. The ﬁrst is the Uniform distribution,\\nUni(a,b)which is uniform over the closed range [a,b]. It’s density function is Uni(xja,b) =1\\nb\\x00a1[x2\\n[a,b]]. Finally, the Gaussian distribution is parameterized by a mean mand variance s2and has density\\nNor(xjm,s2) = ( 2ps2)\\x001\\n2exph\\n\\x001\\n2s2(x\\x00m)2i\\n.MATHREVIEW | COMMON PROBABILITY DISTRIBUTIONS\\nFigure 9.2:\\nthe decision boundary is the set of inputs for which the likelihood of\\ny= + 1 is precisely 50%. Or, in other words, the set of inputs xfor\\nwhich p(y= + 1jx)/p(y=\\x001jx) = 1. In order to do this, the\\nﬁrst thing to notice is that p(yjx) = p(y,x)/p(x). In the ratio, the\\np(x)terms cancel, leaving p(y= + 1,x)/p(y=\\x001,x). Instead of\\ncomputing this ratio, it is easier to compute the log-likelihood ratio\\n(or LLR), log p(y= + 1,x)\\x00logp(y=\\x001,x), computed below:\\nLLR=log\"\\nq0Õ\\ndq[xd=1]\\n(+1),d(1\\x00q(+1),d)[xd=0]#\\n\\x00log\"\\n(1\\x00q0)Õ\\ndq[xd=1]\\n(\\x001),d(1\\x00q(\\x001),d)[xd=0]#\\nmodel assumptions\\n(9.23)\\n=logq0\\x00log(1\\x00q0) +å\\nd[xd=1]\\x10\\nlogq(+1),d\\x00logq(\\x001),d\\x11\\n+å\\nd[xd=0]\\x10\\nlog(1\\x00q(+1),d)\\x00log(1\\x00q(\\x001),d)\\x11\\ntake logs and rearrange\\n(9.24)\\n=å\\ndxdlogq(+1),d\\nq(\\x001),d+å\\nd(1\\x00xd)log1\\x00q(+1),d\\n1\\x00q(\\x001),d+logq0\\n1\\x00q0simplify log terms\\n(9.25)probabilistic modeling 123\\n=å\\ndxd\"\\nlogq(+1),d\\nq(\\x001),d\\x00log1\\x00q(+1),d\\n1\\x00q(\\x001),d#\\n+å\\ndlog1\\x00q(+1),d\\n1\\x00q(\\x001),d+logq0\\n1\\x00q0group x-terms\\n(9.26)\\n=x\\x01w+b (9.27)\\nwd=logq(+1),d(1\\x00q(\\x001),d)\\nq(\\x001),d(1\\x00q(+1),d),b=å\\ndlog1\\x00q(+1),d\\n1\\x00q(\\x001),d+logq0\\n1\\x00q0\\n(9.28)\\nThe result of the algebra is that the naive Bayes model has precisely\\nthe form of a linear model! Thus, like perceptron and many of the\\nother models you’ve previous studied, the decision boundary is\\nlinear.\\n9.5Generative Stories\\nA useful way to develop probabilistic models is to tell a generative\\nstory . This is a ﬁctional story that explains how you believe your\\ntraining data came into existence. To make things interesting, con-\\nsider a multiclass classiﬁcation problem, with continuous features\\nmodeled by independent Gaussians. Since the label can take values\\n1 . . . K, you can use a discrete distribution (die roll) to model it (as\\nopposed to the Bernoilli distribution from before):\\n1. For each example n=1 . . . N:\\n(a) Choose a label yn\\x18Disc(q)\\n(b) For each feature d=1 . . . D:\\ni. Choose feature value xn,d\\x18N or(myn,d,s2\\nyn,d)\\nThis generative story can be directly translated into a likelihood\\nfunction by replacing the “for each”s with products:\\np(D) =for each examplez }| {\\nÕ\\nnqyn|{z}\\nchoose labelÕ\\nd1q\\n2ps2\\nyn,dexp\"\\n\\x001\\n2s2\\nyn,d(xn,d\\x00myn,d)2#\\n| {z }\\nchoose feature value| {z }\\nfor each feature\\n(9.29)\\nYou can take logs to arrive at the log-likelihood:\\nlogp(D) =å\\nn\"\\nlogqyn+å\\nd\\x001\\n2log(s2\\nyn,d)\\x001\\n2s2\\nyn,d(xn,d\\x00myn,d)2#\\n+const\\n(9.30)124 a course in machine learning\\nTo optimize for q, you need to add a “sums to one” constraint as\\nbefore. This leads to the previous solution where the qks are propor-\\ntional to the number of examples with label k. In the case of the ms\\nyou can take a derivative with respect to, say mk,iand obtain:\\n¶logp(D)\\n¶mk,i=¶\\n¶mk,i\\x00å\\nnå\\nd1\\n2s2\\nyn,d(xn,d\\x00myn,d)2ignore irrelevant terms\\n(9.31)\\n=¶\\n¶mk,i\\x00å\\nn:yn=k1\\n2s2\\nk,d(xn,i\\x00mk,i)2ignore irrelevant terms\\n(9.32)\\n=å\\nn:yn=k1\\ns2\\nk,d(xn,i\\x00mk,i) take derivative\\n(9.33)\\nSetting this equal to zero and solving yields:\\nmk,i=ån:yn=kxn,i\\nån:yn=k1(9.34)\\nNamely, the sample mean of the ith feature of the data points that fall\\nin class k. A similar analysis for s2\\nk,iyields:\\n¶logp(D)\\n¶s2\\nk,i=¶\\n¶s2\\nk,i\\x00å\\ny:yn=k\"\\n1\\n2log(s2\\nk,i) +1\\n2s2\\nk,i(xn,i\\x00mk,i)2#\\nignore irrelevant terms\\n(9.35)\\n=\\x00å\\ny:yn=k\"\\n1\\n2s2\\nk,i\\x001\\n2(s2\\nk,i)2(xn,i\\x00mk,i)2#\\ntake derivative\\n(9.36)\\n=1\\n2s4\\nk,iå\\ny:yn=kh\\n(xn,i\\x00mk,i)2\\x00s2\\nk,ii\\nsimplify\\n(9.37)\\nYou can now set this equal to zero and solve, yielding:\\ns2\\nk,i=ån:yn=k(xn,i\\x00mk,i)2\\nån:yn=k1(9.38)\\nWhich is just the sample variance of feature ifor class k. What would the estimate be if you\\ndecided that, for a given class k, all\\nfeatures had equal variance? What\\nif you assumed feature ihad equal\\nvariance for each class? Under what\\ncircumstances might it be a good\\nidea to make such assumptions?? 9.6Conditional Models\\nIn the foregoing examples, the task was formulated as attempting to\\nmodel the joint distribution of (x,y)pairs. This may seem wasteful:\\nat prediction time, all you care about is p(yjx), so why not model it\\ndirectly?probabilistic modeling 125\\nStarting with the case of regression is actually somewhat simpler\\nthan starting with classiﬁcation in this case. Suppose you “believe”\\nthat the relationship between the real value yand the vector xshould\\nbe linear. That is, you expect that y=w\\x01x+b should hold for some\\nparameters (w,b). Of course, the data that you get does not exactly\\nobey this: that’s ﬁne, you can think of deviations from y=w\\x01x+\\nbasnoise . To form a probabilistic model, you must assume some\\ndistribution over noise; a convenient choice is zero-mean Gaussian\\nnoise. This leads to the following generative story:\\n1. For each example n=1 . . . N:\\n(a) Compute tn=w\\x01xn+b\\n(b) Choose noise en\\x18N or(0,s2)\\n(c) Return yn=tn+en\\nIn this story, the variable tnstands for “target.” It is the noiseless\\nvariable that you do not get to observe. Similarly enis the error\\n(noise) on example n. The value that you actually get to observe is\\nyn=tn+en. See Figure 9.3.\\nFigure 9.3: pictorial view of targets\\nversus labelsA basic property of the Gaussian distribution is additivity. Namely,\\nthat if a\\x18N or(m,s2)and b=a+c, then b\\x18N or(m+c,s2). Given\\nthis, from the generative story above, you can derive a shorter gener-\\native story:\\n1. For each example n=1 . . . N:\\n(a) Choose yn\\x18N or(w\\x01xn+b,s2)\\nReading off the log likelihood of a dataset from this generative story,\\nyou obtain:\\nlogp(D) =å\\nn\\x14\\n\\x001\\n2log(s2)\\x001\\n2s2(w\\x01xn+b\\x00yn)2\\x15\\nmodel assumptions\\n(9.39)\\n=\\x001\\n2s2å\\nn(w\\x01xn+b\\x00yn)2+const remove constants\\n(9.40)\\nThis is precisely the linear regression model you encountered in\\nSection 7.6! To minimizing the negative log probability, you need only\\nsolve for the regression coefﬁcients w,bas before.\\nIn the case of binary classiﬁcation, using a Gaussian noise model\\ndoes not make sense. Switching to a Bernoulli model, which de-\\nscribes binary outcomes, makes more sense. The only remaining\\ndifﬁculty is that the parameter of a Bernoulli is a value between zero\\nand one (the probability of “heads”) so your model must produce126 a course in machine learning\\nsuch values. A classic approach is to produce a real-valued target, as\\nbefore, and then transform this target into a value between zero and\\none, so that\\x00¥maps to 0 and +¥maps to 1. A function that does\\nthis is the logistic function1, deﬁned below and plotted in Figure 9.4:1Also called the sigmoid func tion\\nbecause of it’s “S”-shape.\\nFigure 9.4: sketch of logistic functionLogistic function: s(z) =1\\n1+exp[\\x00z]=expz\\n1+expz(9.41)\\nThe logistic function has several nice properties that you can verify\\nfor yourself: s(\\x00z) =1\\x00s(z)and¶s/¶z=zs2(z).\\nUsing the logistic function, you can write down a generative story\\nfor binary classiﬁcation:\\n1. For each example n=1 . . . N:\\n(a) Compute tn=s(w\\x01xn+b)\\n(b) Compute zn\\x18Ber(tn)\\n(c) Return yn=2zn\\x001 (to make it\\x061)\\nThe log-likelihood for this model is:\\nlogp(D) =å\\nnh\\n[yn= + 1]logs(w\\x01xn+b)\\n+ [yn=\\x001]logs(\\x00w\\x01xn+b)i\\nmodel and properties of s\\n(9.42)\\n=å\\nnlogs(yn(w\\x01xn+b)) join terms\\n(9.43)\\n=\\x00å\\nnlog[1+exp(\\x00yn(w\\x01xn+b))] deﬁnition of s\\n(9.44)\\n=\\x00å\\nn`(log)(yn,w\\x01xn+b) deﬁnition of `(log)\\n(9.45)\\nAs you can see, the log-likelihood is precisely the negative of (a\\nscaled version of) the logistic loss from Chapter 7. This model is the\\nlogisticregres sion model, and this is where logisitic loss originally\\nderived from.\\nTODO: conditional versus joint\\n9.7Regularization via Priors\\nIn the foregoing discussion, parameters of the model were selected\\naccording to the maximum likelihood criteria: ﬁnd the parameters\\nqthat maximize pq(D). The trouble with this approach is easy toprobabilistic modeling 127\\nsee even in a simple coin ﬂipping example. If you ﬂip a coin twice\\nand it comes up heads both times, the maximum likelihood estimate\\nfor the bias of the coin is 100%: it will always come up heads. This is\\ntrue even if you had only ﬂipped it once! If course if you had ﬂipped\\nit one million times and it had come up heads every time, then you\\nmight ﬁnd this to be a reasonable solution.\\nThis is clearly undesirable behavior, especially since data is expen-\\nsive in a machine learning setting. One solution (there are others!) is\\nto seek parameters that balance a tradeoff between the likelihood of\\nthe data and some prior belief you have about what values of those\\nparameters are likely. Taking the case of the logistic regression, you\\nmight a priori believe that small values of ware more likely than\\nlarge values, and choose to represent this as a Gaussian prior on each\\ncomponent of w.\\nThe max imum aposterioriprinciple is a method for incoporat-\\ning both data and prior beliefs to obtain a more balanced parameter\\nestimate. In abstract terms, consider a probabilistic model over data\\nDthat is parameterized by parameters q. If you think of the pa-\\nrameters as just another random variable, then you can write this\\nmodel as p(Djq), and maximum likelihood amounts to choosing q\\nto maximize p(Djq). However, you might instead with to maximize\\nthe probability of the parameters , given the data. Namely, maximize\\np(qjD). This term is known as the posterior distribution on q, and\\ncan be computed by Bayes’ rule:\\np(qjD)|{z}\\nposterior=priorz}|{\\np(q)likelihoodz}|{\\np(Djq)\\np(D)|{z}\\nevidence, where p(D) =Z\\ndqp(q)p(Djq)\\n(9.46)\\nThis reads: the posterior is equal to the prior times the likelihood di-\\nvided by the evidence .2The evidence is a scary-looking term (it has2The evidence is sometimes called the\\nmarginal likelihood .an integral!) but note that from the perspective of seeking parameters\\nqthan maximize the posterior, the evidence is just a constant (it does\\nnot depend on q) and therefore can be ignored.\\nReturning to the logistic regression example with Gaussian priors\\non the weights, the logposterior looks like:\\nlogp(qjD) =\\x00å\\nn`(log)(yn,w\\x01xn+b)\\x00å\\nd1\\n2s2w2\\nd+const model deﬁnition\\n(9.47)\\n=\\x00å\\nn`(log)(yn,w\\x01xn+b)\\x001\\n2s2jjwjj2(9.48)\\nand therefore reduces to a regularized logistic function, with a128 a course in machine learning\\nsquared 2-norm regularizer on the weights. (A 1-norm regularizer\\ncan be obtained by using a Laplace prior on wrather than a Gaussian\\nprior on w.)\\n9.8Further Reading\\nTODO10 | N EURAL NETWORKS\\nDependencies:The first learning models you learned about (decision trees\\nand nearest neighbor models) created complex, non-lineardecision\\nboundaries. We moved from there to the perceptron, perhaps the\\nmost classic linear model. At this point, we will move back to non-\\nlinear learning models, but using all that we have learned about\\nlinear learning thus far.\\nThis chapter presents an extension of perceptron learning to non-\\nlinear decision boundaries, taking the biological inspiration of neu-\\nrons even further. In the perceptron, we thought of the input data\\npoint (e.g., an image) as being directly connected to an output (e.g.,\\nlabel). This is often called a single-layer network because there is one\\nlayer of weights. Now, instead of directly connecting the inputs to\\nthe outputs, we will insert a layer of “hidden” nodes, moving from\\na single-layer network to a multi -layer network . But introducing\\na non-linearity at inner layers, this will give us non-linear decision\\nboundaires. In fact, such networks are able to express almost any\\nfunction we want, not just linear functions. The trade-off for this ﬂex-\\nibility is increased complexity in parameter tuning and model design.\\n10.1Bio-inspired Multi-Layer Networks\\nOne of the major weaknesses of linear models, like perceptron and\\nthe regularized linear models from the previous chapter, is that they\\nare linear! Namely, they are unable to learn arbitrary decision bound-\\naries. In contrast, decision trees and KNN could learn arbitrarily\\ncomplicated decision boundaries.\\nFigure 10.1: picture of a two-layer\\nnetwork with 5inputs and two hidden\\nunitsOne approach to doing this is to chain together a collection of\\nperceptrons to build more complex neuralnetworks . An example of\\natwo-layer network is shown in Figure 10.1. Here, you can see ﬁve\\ninputs (features) that are fed into two hidden units . These hidden\\nunits are then fed in to a single outputunit . Each edge in this ﬁgure\\ncorresponds to a different weight. (Even though it looks like there are\\nthree layers, this is called a two-layer network because we don’t countLearning Objectives:\\n• Explain the biological inspiration for\\nmulti-layer neural networks.\\n• Construct a two-layer network that\\ncan solve the XOR problem.\\n• Implement the back-propogation\\nalgorithm for training multi-layer\\nnetworks.\\n• Explain the trade-off between depth\\nand breadth in network structure.\\n• Contrast neural networks with ra-\\ndial basis functions with k-nearest\\nneighbor learning.TODO –130 a course in machine learning\\nthe inputs as a real layer. That is, it’s two layers of trained weights.)\\nPrediction with a neural network is a straightforward generaliza-\\ntion of prediction with a perceptron. First you compute activations\\nof the nodes in the hidden unit based on the inputs and the input\\nweights. Then you compute activations of the output unit given the\\nhidden unit activations and the second layer of weights.\\nThe only major difference between this computation and the per-\\nceptron computation is that the hidden units compute a non-linear\\nfunction of their inputs. This is usually called the activation func tion\\norlink func tion. More formally, if wi,dis the weights on the edge\\nconnecting input dto hidden unit i, then the activation of hidden unit\\niis computed as:\\nhi=f(wi\\x01x) (10.1)\\nWhere fis the link function and wirefers to the vector of weights\\nfeeding in to node i.\\nOne example link function is the sign function. That is, if the\\nincoming signal is negative, the activation is \\x001. Otherwise the\\nactivation is +1. This is a potentially useful activiation function,\\nbut you might already have guessed the problem with it: it is non-\\ndifferentiable.\\nFigure 10.2: picture of sign versus tanhEXPLAIN BIAS!!!\\nA more popular link function is the hyperbolic tangent function,\\ntanh. A comparison between the sign function and the tanh function\\nis in Figure 10.2. As you can see, it is a reasonable approximation\\nto the sign function, but is convenient in that it is differentiable.1\\n1It’s derivative is just 1 \\x00tanh2(x).Because it looks like an “S” and because the Greek character for “S”\\nis “Sigma,” such functions are usually called sigmoid functions.\\nAssuming for now that we are using tanh as the link function, the\\noverall prediction made by a two-layer network can be computed\\nusing Algorithm 10.1. This function takes a matrix of weights W\\ncorresponding to the ﬁrst layer weights and a vector of weights vcor-\\nresponding to the second layer. You can write this entire computation\\nout in one line as:\\nˆy=å\\nivitanh(wi\\x01ˆx) (10.2)\\n=v\\x01tanh(Wˆx) (10.3)\\nWhere the second line is short hand assuming that tanh can take a\\nvector as input and product a vector as output. Is it necessary to use a link function\\nat all? What would happen if you\\njust used the identify function as a\\nlink??neural networks 131\\nAlgorithm 25TwoLayer Network Predict (W,v, ˆx)\\n1:fori=1tonumber of hidden units do\\n2:hi tanh(wi\\x01ˆx) // compute activation of hidden unit i\\n3:end for\\n4:return v\\x01h // compute output unit\\nyx0x1x2\\n+1+1+1+1\\n+1+1-1-1\\n-1+1+1-1\\n-1+1-1+1\\nTable 10.1: Small XOR data set.The claim is that two-layer neural networks are more expressive\\nthan single layer networks (i.e., perceptrons). To see this, you can\\nconstruct a very small two-layer network for solving the XOR prob-\\nlem. For simplicity, suppose that the data set consists of four data\\npoints, given in Table 10.1. The classiﬁcation rule is that y= + 1 if an\\nonly if x1=x2, where the features are just \\x061.\\nYou can solve this problem using a two layer network with two\\nhidden units. The key idea is to make the ﬁrst hidden unit compute\\nan “or” function: x1_x2. The second hidden unit can compute an\\n“and” function: x1^x2. The the output can combine these into a\\nsingle prediction that mimics XOR. Once you have the ﬁrst hidden\\nunit activate for “or” and the second for “and,” you need only set the\\noutput weights as \\x002 and +1, respectively. Verify that these output weights\\nwill actually give you XOR.?To achieve the “or” behavior, you can start by setting the bias to\\n\\x000.5 and the weights for the two “real” features as both being 1. You\\ncan check for yourself that this will do the “right thing” if the link\\nfunction were the sign function. Of course it’s not, it’s tanh. To get\\ntanh to mimic sign, you need to make the dot product either really\\nreally large or really really small. You can accomplish this by set-\\nting the bias to\\x00500, 000 and both of the two weights to 1, 000, 000.\\nNow, the activation of this unit will be just slightly above \\x001 for\\nx=h\\x001,\\x001iand just slightly below +1 for the other three examples. This shows how to create an “or”\\nfunction. How can you create an\\n“and” function?? At this point you’ve seen that one-layer networks (aka percep-\\ntrons) can represent any linear function and only linear functions.\\nYou’ve also seen that two-layer networks can represent non-linear\\nfunctions like XOR. A natural question is: do you get additional\\nrepresentational power by moving beyond two layers? The answer\\nis partially provided in the following Theorem, due originally to\\nGeorge Cybenko for one particular type of link function, and ex-\\ntended later by Kurt Hornik to arbitrary link functions.\\nTheorem 10(Two-Layer Networks are Universal Function Approx-\\nimators) .Let F be a continuous function on a bounded subset of D-\\ndimensional space. Then there exists a two-layer neural network ˆF with a\\nﬁnite number of hidden units that approximate F arbitrarily well. Namely,\\nfor all xin the domain of F,\\x0c\\x0cF(x)\\x00ˆF(x)\\x0c\\x0c<e.\\nOr, in colloquial terms “two-layer networks can approximate any132 a course in machine learning\\nfunction.”\\nThis is a remarkable theorem. Practically, it says that if you give\\nme a function Fand some error tolerance parameter e, I can construct\\na two layer network that computes F. In a sense, it says that going\\nfrom one layer to two layers completely changes the representational\\ncapacity of your model.\\nWhen working with two-layer networks, the key question is: how\\nmany hidden units should I have? If your data is Ddimensional\\nand you have Khidden units, then the total number of parameters\\nis(D+2)K. (The ﬁrst +1 is from the bias, the second is from the\\nsecond layer of weights.) Following on from the heuristic that you\\nshould have one to two examples for each parameter you are trying\\nto estimate, this suggests a method for choosing the number of hid-\\nden units as roughly bN\\nDc. In other words, if you have tons and tons\\nof examples, you can safely have lots of hidden units. If you only\\nhave a few examples, you should probably restrict the number of\\nhidden units in your network.\\nThe number of units is both a form of inductive bias and a form\\nof regularization. In both view, the number of hidden units controls\\nhow complex your function will be. Lots of hidden units )very\\ncomplicated function. As the number increases, training performance\\ncontinues to get better. But at some point, test performance gets\\nworse because the network has overﬁt the data.\\n10.2The Back-propagation Algorithm\\nThe back-propagation algorithm is a classic approach to training\\nneural networks. Although it was not originally seen this way, based\\non what you know from the last chapter, you can summarize back-\\npropagation as:\\nback-propagation =gradient descent +chain rule ( 10.4)\\nMore speciﬁcally, the set up is exactly the same as before. You are\\ngoing to optimize the weights in the network to minimize some ob-\\njective function. The only difference is that the predictor is no longer\\nlinear (i.e., ˆy=w\\x01x+b) but now non-linear (i.e., v\\x01tanh(Wˆx)).\\nThe only question is how to do gradient descent on this more compli-\\ncated objective.\\nFor now, we will ignore the idea of regularization. This is for two\\nreasons. The ﬁrst is that you already know how to deal with regular-\\nization, so everything you’ve learned before applies. The second is\\nthat historically , neural networks have not been regularized. Instead,\\npeople have used early stop ping as a method for controlling overﬁt-\\nting. Presently, it’s not obvious which is a better solution: both areneural networks 133\\nvalid options.\\nTo be completely explicit, we will focus on optimizing squared\\nerror. Again, this is mostly for historic reasons. You could easily\\nreplace squared error with your loss function of choice. Our overall\\nobjective is:\\nmin\\nW,vå\\nn1\\n2 \\nyn\\x00å\\nivif(wi\\x01xn)!2\\n(10.5)\\nHere, fis some link function like tanh.\\nThe easy case is to differentiate this with respect to v: the weights\\nfor the output unit. Without even doing any math, you should be\\nable to guess what this looks like. The way to think about it is that\\nfrom vs perspective, it is just a linear model, attempting to minimize\\nsquared error. The only “funny” thing is that its inputs are the activa-\\ntions hrather than the examples x. So the gradient with respect to v\\nis just as for the linear case.\\nTo make things notationally more convenient, let endenote the\\nerror on the nth example (i.e., the blue term above), and let hndenote\\nthe vector of hidden unit activations on that example. Then:\\nrv=\\x00å\\nnenhn (10.6)\\nThis is exactly like the linear case. One way of interpreting this is:\\nhow would the output weights have to change to make the prediction\\nbetter? This is an easy question to answer because they can easily\\nmeasure how their changes affect the output.\\nThe more complicated aspect to deal with is the weights corre-\\nsponding to the ﬁrst layer. The reason this is difﬁcult is because the\\nweights in the ﬁrst layer aren’t necessarily trying to produce speciﬁc\\nvalues, say 0 or 5 or \\x002.1. They are simply trying to produce acti-\\nvations that get fed to the output layer. So the change they want to\\nmake depends crucially on how the output layer interprets them.\\nThankfully, the chain rule of calculus saves us. Ignoring the sum\\nover data points, we can compute:\\nL(W) =1\\n2 \\ny\\x00å\\nivif(wi\\x01x)!2\\n(10.7)\\n¶L\\n¶wi=¶L\\n¶fi¶fi\\n¶wi(10.8)\\n¶L\\n¶fi=\\x00 \\ny\\x00å\\nivif(wi\\x01x)!\\nvi=\\x00evi (10.9)\\n¶fi\\n¶wi=f0(wi\\x01x)x (10.10)134 a course in machine learning\\nAlgorithm 26TwoLayer Network Train (D,h,K,MaxIter )\\n1:W D\\x02Kmatrix of small random values // initialize input layer weights\\n2:v K-vector of small random values // initialize output layer weights\\n3:foriter=1. . .MaxIter do\\n4:G D\\x02Kmatrix of zeros // initialize input layer gradient\\n5:g K-vector of zeros // initialize output layer gradient\\n6:for all (x,y)2Ddo\\n7: fori=1toKdo\\n8: ai wi\\x01ˆx\\n9: hi tanh(ai) // compute activation of hidden unit i\\n10: end for\\n11: ˆy v\\x01h // compute output unit\\n12: e y\\x00ˆy // compute error\\n13: g g\\x00eh // update gradient for output layer\\n14: fori=1toKdo\\n15: Gi Gi\\x00evi(1\\x00tanh2(ai))x // update gradient for input layer\\n16: end for\\n17:end for\\n18:W W\\x00hG // update input layer weights\\n19:v v\\x00hg // update output layer weights\\n20:end for\\n21:return W,v\\nPutting this together, we get that the gradient with respect to wiis:\\nrwi=\\x00evif0(wi\\x01x)x (10.11)\\nIntuitively you can make sense of this. If the overall error of the\\npredictor ( e) is small, you want to make small steps. If viis small\\nfor hidden unit i, then this means that the output is not particularly\\nsensitive to the activation of the ith hidden unit. Thus, its gradient\\nshould be small. If viﬂips sign, the gradient at wishould also ﬂip\\nsigns. The name back -prop agation comes from the fact that you\\npropagate gradients backward through the network, starting at the\\nend.\\nThe complete instantiation of gradient descent for a two layer\\nnetwork with Khidden units is sketched in Algorithm 10.2. Note that\\nthis really is exactly a gradient descent algorithm; the only different is\\nthat the computation of the gradients of the input layer is moderately\\ncomplicated. What would happen to this algo-\\nrithm if you wanted to optimize\\nexponential loss instead of squared\\nerror? What if you wanted to add in\\nweight regularization??As a bit of practical advice, implementing the back-propagation\\nalgorithm can be a bit tricky. Sign errors often abound. A useful trick\\nis ﬁrst to keep Wﬁxed and work on just training v. Then keep v\\nﬁxed and work on training W. Then put them together.\\nIf you like matrix calculus, derive\\nthe same algorithm starting from\\nEq (10.3).?neural networks 135\\n10.3Initialization and Convergence of Neural Networks\\nBased on what you know about linear models, you might be tempted\\nto initialize all the weights in a neural network to zero. You might\\nalso have noticed that in Algorithm 10.2, this is not what’s done:\\nthey’re initialized to small random values. The question is why?\\nThe answer is because an initialization of W=0and v=0will\\nlead to “uninteresting” solutions. In other words, if you initialize the\\nmodel in this way, it will eventually get stuck in a bad local optimum.\\nTo see this, ﬁrst realize that on any example x, the activation hiof the\\nhidden units will all be zero since W=0. This means that on the ﬁrst\\niteration, the gradient on the output weights ( v) will be zero, so they\\nwill stay put. Furthermore, the gradient w1,dfor the dth feature on\\ntheith unit will be exactly the same as the gradient w2,dfor the same\\nfeature on the second unit. This means that the weight matrix, after\\na gradient step, will change in exactly the same way for every hidden\\nunit. Thinking through this example for iterations 2 . . . , the values of\\nthe hidden units will always be exactly the same, which means that\\nthe weights feeding in to any of the hidden units will be exactly the\\nsame. Eventually the model will converge, but it will converge to a\\nsolution that does not take advantage of having access to the hidden\\nunits.\\nThis shows that neural networks are sensitive to their initialization.\\nIn particular, the function that they optimize is non-convex, meaning\\nthat it might have plentiful local optima. (One of which is the trivial\\nlocal optimum described in the preceding paragraph.) In a sense,\\nneural networks must have local optima. Suppose you have a two\\nlayer network with two hidden units that’s been optimized. You have\\nweights w1from inputs to the ﬁrst hidden unit, weights w2from in-\\nputs to the second hidden unit and weights (v1,v2)from the hidden\\nunits to the output. If I give you back another network with w1and\\nw2swapped, and v1and v2swapped, the network computes exactly\\nthe same thing, but with a markedly different weight structure. This\\nphenomena is known as sym metricmodes (“mode” referring to an\\noptima) meaning that there are symmetries in the weight space. It\\nwould be one thing if there were lots of modes and they were all\\nsymmetric: then ﬁnding one of them would be as good as ﬁnding\\nany other. Unfortunately there are additional local optima that are\\nnotglobal optima.\\nFigure 10.3: convergence of randomly\\ninitialized networksRandom initialization of the weights of a network is a way to\\naddress both of these problems. By initializing a network with small\\nrandom weights (say, uniform between \\x000.1 and 0.1), the network is\\nunlikely to fall into the trivial, symmetric local optimum. Moreover,\\nby training a collection of networks, each with a different random136 a course in machine learning\\ninitialization, you can often obtain better solutions that with just\\none initialization. In other words, you can train ten networks with\\ndifferent random seeds, and then pick the one that does best on held-\\nout data. Figure 10.3shows prototypical test-set performance for ten\\nnetworks with different random initialization, plus an eleventh plot\\nfor the trivial symmetric network initialized with zeros.\\nOne of the typical complaints about neural networks is that they\\nare ﬁnicky. In particular, they have a rather large number of knobs to\\ntune:\\n1. The number of layers\\n2. The number of hidden units per layer\\n3. The gradient descent learning rate h\\n4. The initialization\\n5. The stopping iteration or weight regularization\\nThe last of these is minor (early stopping is an easy regularization\\nmethod that does not require much effort to tune), but the others\\nare somewhat signiﬁcant. Even for two layer networks, having to\\nchoose the number of hidden units, and then get the learning rate\\nand initialization “right” can take a bit of work. Clearly it can be\\nautomated, but nonetheless it takes time.\\nAnother difﬁculty of neural networks is that their weights can\\nbe difﬁcult to interpret. You’ve seen that, for linear networks, you\\ncan often interpret high weights as indicative of positive examples\\nand low weights as indicative of negative examples. In multilayer\\nnetworks, it becomes very difﬁcult to try to understand what the\\ndifferent hidden units are doing.\\n10.4Beyond Two Layers\\nFigure 10.4: multi-layer networkThe deﬁnition of neural networks and the back-propagation algo-\\nrithm can be generalized beyond two layers to any arbitrary directed\\nacyclic graph. In practice, it is most common to use a layered net-\\nwork like that shown in Figure 10.4unless one has a very strong\\nreason (aka inductive bias) to do something different. However, the\\nview as a directed graph sheds a different sort of insight on the back-\\npropagation algorithm.\\nFigure 10.5: DAG networkSuppose that your network structure is stored in some directed\\nacyclic graph, like that in Figure 10.5. We index nodes in this graph\\nasu,v. The activation before applying non-linearity at a node is au\\nand after non-linearity is hu. The graph has a single sink, which is\\nthe output node ywith activation ay(no non-linearity is performedneural networks 137\\nAlgorithm 27Forward Propagation (x)\\n1:for all input nodes udo\\n2:hu corresponding feature of x\\n3:end for\\n4:for all nodes vin the network whose parent’s are computed do\\n5:av åu2par(v)w(u,v)hu\\n6:hv tanh(av)\\n7:end for\\n8:return ay\\nAlgorithm 28BackPropagation (x,y)\\n1:run F orward Propagation (x) to compute activations\\n2:ey y\\x00ay // compute overall network error\\n3:for all nodes vin the network whose error evis computed do\\n4:for all u2par(v)do\\n5: gu,v \\x00 evhu // compute gradient of this edge\\n6: eu eu+evwu,v(1\\x00tanh2(au))// compute the “error” of the parent node\\n7:end for\\n8:end for\\n9:return all gradients ge\\non the output unit). The graph has D-many inputs (i.e., nodes with\\nno parent), whose activations huare given by an input example. An\\nedge (u,v)is from a parent to a child (i.e., from an input to a hidden\\nunit, or from a hidden unit to the sink). Each edge has a weight wu,v.\\nWe say that par(u)is the set of parents of u.\\nThere are two relevant algorithms: forward-propagation and back-\\npropagation. Forward-propagation tells you how to compute the\\nactivation of the sink ygiven the inputs. Back-propagation computes\\nderivatives of the edge weights for a given input.\\nFigure 10.6: picture of forward propThe key aspect of the forward -prop agation algorithm is to iter-\\natively compute activations, going deeper and deeper in the DAG.\\nOnce the activations of all the parents of a node uhave been com-\\nputed, you can compute the activation of node u. This is spelled out\\nin Algorithm 10.4. This is also explained pictorially in Figure 10.6.\\nFigure 10.7: picture of back propBack-propagation (see Algorithm 10.4) does the opposite: it com-\\nputes gradients top-down in the network. The key idea is to compute\\nanerror for each node in the network. The error at the output unit is\\nthe “true error.” For any input unit, the error is the amount of gradi-\\nent that we see coming from our children (i.e., higher in the network).\\nThese errors are computed backwards in the network (hence the\\nname back -prop agation) along with the gradients themselves. This is\\nalso explained pictorially in Figure 10.7.\\nGiven the back-propagation algorithm, you can directly run gradi-\\nent descent, using it as a subroutine for computing the gradients.138 a course in machine learning\\n10.5Breadth versus Depth\\nAt this point, you’ve seen how to train two-layer networks and how\\nto train arbitrary networks. You’ve also seen a theorem that says\\nthat two-layer networks are universal function approximators. This\\nbegs the question: if two-layer networks are so great, why do we care\\nabout deeper networks?\\nTo understand the answer, we can borrow some ideas from CS\\ntheory, namely the idea of circuit com plex ity. The goal is to show\\nthat there are functions for which it might be a “good idea” to use a\\ndeep network. In other words, there are functions that will require a\\nhuge number of hidden units if you force the network to be shallow,\\nbut can be done in a small number of units if you allow it to be deep.\\nThe example that we’ll use is the parityfunc tion which, ironically\\nenough, is just a generalization of the XOR problem. The function is\\ndeﬁned over binary inputs as:\\nparity (x) =å\\ndxdmod 2 ( 10.12)\\n=(\\n1 if the number of 1s in xis odd\\n0 if the number of 1s in xis even(10.13)\\nFigure 10.8:nnet:paritydeep : deep\\nfunction for computing parityIt is easy to deﬁne a circuit of depth O(log2D)withO(D)-many\\ngates for computing the parity function. Each gate is an XOR, ar-\\nranged in a complete binary tree, as shown in Figure 10.8. (If you\\nwant to disallow XOR as a gate, you can ﬁx this by allowing the\\ndepth to be doubled and replacing each XOR with an AND, OR and\\nNOT combination, like you did at the beginning of this chapter.)\\nThis shows that if you are allowed to be deep, you can construct a\\ncircuit with that computes parity using a number of hidden units that\\nis linear in the dimensionality. So can you do the same with shallow\\ncircuits? The answer is no. It’s a famous result of circuit complexity\\nthat parity requires exponentially many gates to compute in constant\\ndepth. The formal theorem is below:\\nTheorem 11(Parity Function Complexity) .Any circuit of depth K <\\nlog2D that computes the parity function of D input bits must contain OeD\\ngates.\\nThis is a very famous result because it shows that constant-depth\\ncircuits are less powerful that deep circuits. Although a neural net-\\nwork isn’t exactly the same as a circuit, the is generally believed that\\nthe same result holds for neural networks. At the very least, this\\ngives a strong indication that depth might be an important considera-\\ntion in neural networks. What is it about neural networks\\nthat makes it so that the theorem\\nabout circuits does not apply di-\\nrectly??One way of thinking about the issue of breadth versus depth has\\nto do with the number of parameters that need to be estimated. Byneural networks 139\\nthe heuristic that you need roughly one or two examples for every\\nparameter, a deep model could potentially require exponentially\\nfewer examples to train than a shallow model!\\nThis now ﬂips the question: if deep is potentially so much better,\\nwhy doesn’t everyone use deep networks? There are at least two\\nanswers. First, it makes the architecture selection problem more\\nsigniﬁcant. Namely, when you use a two-layer network, the only\\nhyperparameter to choose is how many hidden units should go in\\nthe middle layer. When you choose a deep network, you need to\\nchoose how many layers, and what is the width of all those layers.\\nThis can be somewhat daunting.\\nA second issue has to do with training deep models with back-\\npropagation. In general, as back-propagation works its way down\\nthrough the model, the sizes of the gradients shrink. You can work\\nthis out mathematically, but the intuition is simpler. If you are the\\nbeginning of a very deep network, changing one single weight is\\nunlikely to have a signiﬁcant effect on the output, since it has to\\ngo through so many other units before getting there. This directly\\nimplies that the derivatives are small. This, in turn, means that back-\\npropagation essentially never moves far from its initialization when\\nrun on very deep networks. While these small derivatives might\\nmake training difﬁcult, they might\\nbegood for other reasons: what\\nreasons??Finding good ways to train deep networks is an active research\\narea. There are two general strategies. The ﬁrst is to attempt to ini-\\ntialize the weights better, often by a layer -wise initialization strategy.\\nThis can be often done using unlabeled data. After this initializa-\\ntion, back-propagation can be run to tweak the weights for whatever\\nclassiﬁcation problem you care about. A second approach is to use a\\nmore complex optimization procedure, rather than gradient descent.\\nYou will learn about some such procedures later in this book.\\n10.6Basis Functions\\nAt this point, we’ve seen that: (a) neural networks can mimic linear\\nfunctions and (b) they can learn more complex functions. A rea-\\nsonable question is whether they can mimic a KNN classiﬁer, and\\nwhether they can do it efﬁciently (i.e., with not-too-many hidden\\nunits).\\nA natural way to train a neural network to mimic a KNN classiﬁer\\nis to replace the sigmoid link function with a radial basisfunc tion\\n(RBF). In a sigmoid network (i.e., a network with sigmoid links),\\nthe hidden units were computed as hi=tanh(wi,x\\x01). In an RBF\\nnetwork , the hidden units are computed as:\\nhi=exph\\n\\x00gijjwi\\x00xjj2i\\n(10.14)140 a course in machine learning\\nFigure 10.9:nnet:rbfpicture : a one-D\\npicture of RBF bumps\\nFigure 10.10:nnet:unitsymbols : picture\\nof nnet with sigmoid/rbf unitsIn other words, the hidden units behave like little Gaussian “bumps”\\ncentered around locations speciﬁed by the vectors wi. A one-dimensional\\nexample is shown in Figure 10.9. The parameter gispeciﬁes the width\\nof the Gaussian bump. If giis large, then only data points that are\\nreally close to wihave non-zero activations. To distinguish sigmoid\\nnetworks from RBF networks, the hidden units are typically drawn\\nwith sigmoids or with Gaussian bumps, as in Figure 10.10.\\nTraining RBF networks involves ﬁnding good values for the Gas-\\nsian widths, gi, the centers of the Gaussian bumps, wiand the con-\\nnections between the Gaussian bumps and the output unit, v. This\\ncan all be done using back-propagation. The gradient terms for vre-\\nmain unchanged from before, the the derivates for the other variables\\ndiffer (see Exercise ??).\\nOne of the big questions with RBF networks is: where should\\nthe Gaussian bumps be centered? One can, of course, apply back-\\npropagation to attempt to ﬁnd the centers. Another option is to spec-\\nify them ahead of time. For instance, one potential approach is to\\nhave one RBF unit per data point, centered on that data point. If you\\ncarefully choose the gs and vs, you can obtain something that looks\\nnearly identical to distance-weighted KNN by doing so. This has the\\nadded advantage that you can go futher, and use back-propagation\\ntolearn good Gaussian widths ( g) and “voting” factors ( v) for the\\nnearest neighbor algorithm.\\nConsider an RBF network with\\none hidden unit per training point,\\ncentered at that point. What bad\\nthing might happen if you use back-\\npropagation to estimate the gs and\\nvon this data if you’re not careful?\\nHow could you be careful??10.7Further Reading\\nTODO further reading11 | K ERNEL METHODS\\nDependencies:Linear models are great because they are easy to understand\\nand easy to optimize. They suffer because they can only learn very\\nsimple decision boundaries. Neural networks can learn more com-\\nplex decision boundaries, but lose the nice convexity properties of\\nmany linear models.\\nOne way of getting a linear model to behave non-linearly is to\\ntransform the input. For instance, by adding feature pairs as addi-\\ntional inputs. Learning a linear model on such a representation is\\nconvex, but is computationally prohibitive in all but very low dimen-\\nsional spaces. You might ask: instead of explicitly expanding the fea-\\nture space, is it possible to stay with our original data representation\\nand do all the feature blow up implicitly? Surprisingly, the answer is\\noften “yes” and the family of techniques that makes this possible are\\nknown as kernelapproaches.\\n11.1From Feature Combinations to Kernels\\nIn Section 5.4, you learned one method for increasing the expressive\\npower of linear models: explode the feature space. For instance,\\na “quadratic” feature explosion might map a feature vector x=\\nhx1,x2,x3, . . . , xDito an expanded version denoted f(x):\\nf(x) =h1, 2x1, 2x2, 2x3, . . . , 2 xD,\\nx2\\n1,x1x2,x1x3, . . . , x1xD,\\nx2x1,x2\\n2,x2x3, . . . , x2xD,\\nx3x1,x3x2,x2\\n3, . . . , x2xD,\\n. . . ,\\nxDx1,xDx2,xDx3, . . . , x2\\nDi (11.1)\\n(Note that there are repetitions here, but hopefully most learning\\nalgorithms can deal well with redundant features; in particular, the\\n2x1terms are due to collapsing some repetitions.)Learning Objectives:\\n• Explain how kernels generalize\\nboth feature combinations and basis\\nfunctions.\\n• Contrast dot products with kernel\\nproducts.\\n• Implement kernelized perceptron.\\n• Derive a kernelized version of\\nregularized least squares regression.\\n• Implement a kernelized version of\\nthe perceptron.\\n• Derive the dual formulation of the\\nsupport vector machine.Many who have had an opportunity of knowing any more about\\nmathematics confuse it with arithmetic, and consider it an arid\\nscience. In reality, however, it is a science which requires a great\\namount of imagination. – Soﬁa Kovalevskaya142 a course in machine learning\\nYou could then train a classiﬁer on this expanded feature space.\\nThere are two primary concerns in doing so. The ﬁrst is computa-\\ntional: if your learning algorithm scales linearly in the number of fea-\\ntures, then you’ve just squared the amount of computation you need\\nto perform; you’ve also squared the amount of memory you’ll need.\\nThe second is statistical: if you go by the heuristic that you should\\nhave about two examples for every feature, then you will now need\\nquadratically many training examples in order to avoid overﬁtting.\\nThis chapter is all about dealing with the computational issue. It\\nwill turn out in Chapter 12that you can also deal with the statistical\\nissue: for now, you can just hope that regularization will be sufﬁcient\\nto attenuate overﬁtting.\\nThe key insight in kernel-based learning is that you can rewrite\\nmany linear models in a way that doesn’t require you to ever ex-\\nplicitly compute f(x). To start with, you can think of this purely\\nas a computational “trick” that enables you to use the power of a\\nquadratic feature mapping without actually having to compute and\\nstore the mapped vectors. Later, you will see that it’s actually quite a\\nbit deeper. Most algorithms we discuss involve a product of the form\\nw\\x01f(x), after performing the feature mapping. The goal is to rewrite\\nthese algorithms so that they only ever depend on dot products be-\\ntween two examples, say xand z; namely, they depend on f(x)\\x01f(z).\\nTo understand why this is helpful, consider the quadratic expansion\\nfrom above, and the dot-product between two vectors. You get:\\nf(x)\\x01f(z) =1+x1z1+x2z2+\\x01\\x01\\x01+xDzD+x2\\n1z2\\n1+\\x01\\x01\\x01+x1xDz1zD+\\n\\x01\\x01\\x01+xDx1zDz1+xDx2zDz2+\\x01\\x01\\x01+x2\\nDz2\\nD (11.2)\\n=1+2å\\ndxdzd+å\\ndå\\nexdxezdze (11.3)\\n=1+2x\\x01z+(x\\x01z)2(11.4)\\n= (1+x\\x01z)2(11.5)\\nThus, you can compute f(x)\\x01f(z)in exactly the same amount of\\ntime as you can compute x\\x01z(plus the time it takes to perform an\\naddition and a multiply, about 0.02 nanoseconds on a circa 2011\\nprocessor).\\nThe rest of the practical challenge is to rewrite your algorithms so\\nthat they only depend on dot products between examples and not on\\nany explicit weight vectors.\\n11.2Kernelized Perceptron\\nConsider the original perceptron algorithm from Chapter 4, re-\\npeated in Algorithm 11.2using linear algebra notation and using fea-\\nture expansion notation f(x). In this algorithm, there are two placeskernel methods 143\\nAlgorithm 29Perceptron Train (D,MaxIter )\\n1:w 0,b 0 // initialize weights and bias\\n2:foriter=1. . .MaxIter do\\n3:for all (x,y)2Ddo\\n4: a w\\x01f(x) +b // compute activation for this example\\n5: ifya\\x140then\\n6: w w+yf(x) // update weights\\n7: b b+y // update bias\\n8: end if\\n9:end for\\n10:end for\\n11:return w,b\\nIfU=fuigI\\ni=1is a set of vectors in RD, then the span ofUis the set of vectors that can be written as\\nlinear combinations of uis; namely: span(U) =fåiaiui:a12R, . . . , aI2Rg. If all of the uis are\\nlinearly independent, then the dimension of span(U)isI; in particular, if there are D-many linearly\\nindependent vectors then they span RD.MATHREVIEW | SPANS\\nFigure 11.1:\\nwhere f(x)is used explicitly. The ﬁrst is in computing the activation\\n(line 4) and the second is in updating the weights (line 6). The goal is\\nto remove the explicit dependence of this algorithm on fand on the\\nweight vector.\\nTo do so, you can observe that at any point in the algorithm, the\\nweight vector wcan be written as a linear combination of expanded\\ntraining data. In particular, at any point, w=ånanf(xn)for some\\nparameters a. Initially, w=0so choosing a=0 yields this. If the\\nﬁrst update occurs on the nth training example, then the resolution\\nweight vector is simply ynf(xn), which is equivalent to setting an=\\nyn. If the second update occurs on the mth training example, then all\\nyou need to do is update am am+ym. This is true, even if you\\nmake multiple passes over the data. This observation leads to the\\nfollowing representertheorem, which states that the weight vector of\\nthe perceptron lies in the span of the training data.\\nTheorem 12(Perceptron Representer Theorem) .During a run of\\nthe perceptron algorithm, the weight vector wis always in the span of the\\n(assumed non-empty) training data, f(x1), . . . , f(xN).\\nProof of Theorem 12.By induction. Base case: the span of any non-\\nempty set contains the zero vector, which is the initial weight vec-\\ntor. Inductive case: suppose that the theorem is true before the kth\\nupdate, and suppose that the kth update happens on example n.\\nBy the inductive hypothesis, you can write w=åiaif(xi)before144 a course in machine learning\\nAlgorithm 30Kernelized Perceptron Train (D,MaxIter )\\n1:a 0,b 0 // initialize coefﬁcients and bias\\n2:foriter=1. . .MaxIter do\\n3:for all (xn,yn)2Ddo\\n4: a åmamf(xm)\\x01f(xn) +b // compute activation for this example\\n5: ifyna\\x140then\\n6: an an+yn // update coefﬁcients\\n7: b b+y // update bias\\n8: end if\\n9:end for\\n10:end for\\n11:return a,b\\nthe update. The new weight vector is [åiaif(xi)]+ynf(xn) =\\nåi(ai+yn[i=n])f(xi), which is still in the span of the training\\ndata.\\nNow that you know that you can always write w=ånanf(xn)for\\nsome ais, you can additionall compute the activations (line 4) as:\\nw\\x01f(x) +b= \\nå\\nnanf(xn)!\\n\\x01f(x) +b deﬁnition of w\\n(11.6)\\n=å\\nnanh\\nf(xn)\\x01f(x)i\\n+b dot products are linear\\n(11.7)\\nThis now depends only on dot-products between data points, and\\nnever explicitly requires a weight vector. You can now rewrite the\\nentire perceptron algorithm so that it never refers explicitly to the\\nweights and only ever depends on pairwise dot products between\\nexamples. This is shown in Algorithm 11.2.\\nThe advantage to this “kernelized” algorithm is that you can per-\\nform feature expansions like the quadratic feature expansion from\\nthe introduction for “free.” For example, for exactly the same cost as\\nthe quadratic features, you can use a cubicfeature map , computed\\nas¨f(x)f(z) = ( 1+x\\x01z)3, which corresponds to three-way inter-\\nactions between variables. (And, in general, you can do so for any\\npolynomial degree pat the same computational complexity.)\\n11.3Kernelized K-means\\nFor a complete change of pace, consider the K-means algorithm from\\nSection 3. This algorithm is for clustering where there is no notion of\\n“training labels.” Instead, you want to partition the data into coher-\\nent clusters. For data in RD, it involves randomly initializing K-manykernel methods 145\\ncluster means m(1), . . . ,m(K). The algorithm then alternates between the\\nfollowing two steps until convergence, with xreplaced by f(x)since\\nthat is the eventual goal:\\n1. For each example n, set cluster label zn=arg min k\\x0c\\x0c\\x0c\\x0cf(xn)\\x00m(k)\\x0c\\x0c\\x0c\\x0c2.\\n2. For each cluster k, update m(k)=1\\nNkån:zn=kf(xn), where Nkis the\\nnumber of nwith zn=k.\\nThe question is whether you can perform these steps without ex-\\nplicitly computing f(xn). The representertheorem is more straight-\\nforward here than in the perceptron. The mean of a set of data is,\\nalmost by deﬁnition, in the span of that data (choose the ais all to be\\nequal to 1/ N). Thus, so long as you initialize the means in the span\\nof the data, you are guaranteed always to have the means in the span\\nof the data. Given this, you know that you can write each mean as an\\nexpansion of the data; say that m(k)=åna(k)\\nnf(xn)for some parame-\\ntersa(k)\\nn(there are N\\x02K-many such parameters).\\nGiven this expansion, in order to execute step ( 1), you need to\\ncompute norms. This can be done as follows:\\nzn=arg min\\nk\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cf(xn)\\x00m(k)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\ndeﬁnition of zn\\n(11.8)\\n=arg min\\nk\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cf(xn)\\x00å\\nma(k)\\nmf(xm)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\ndeﬁnition of m(k)\\n(11.9)\\n=arg min\\nkjjf(xn)jj2+\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cå\\nma(k)\\nmf(xm)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n+f(xn)\\x01\"\\nå\\nma(k)\\nmf(xm)#\\nexpand quadratic term\\n(11.10)\\n=arg min\\nkå\\nmå\\nm0a(k)\\nma(k)\\nm0f(xm)\\x01f(xm0) +å\\nma(k)\\nmf(xm)\\x01f(xn) +const linearity and constant\\n(11.11)\\nThis computation can replace the assignments in step ( 1) ofK-means.\\nThe mean updates are more direct in step ( 2):\\nm(k)=1\\nNkå\\nn:zn=kf(xn)() a(k)\\nn=(1\\nNkifzn=k\\n0 otherwise(11.12)\\n11.4What Makes a Kernel\\nAkernelis just a form of generalized dot product. You can also\\nthink of it as simply shorthand for f(x)\\x01f(z), which is commonly\\nwritten Kf(x,z). Or, when fis clear from context, simply K(x,z).146 a course in machine learning\\nThis is often refered to as the kernel product between xand z(under\\nthe mapping f).\\nIn this view, what you’ve seen in the preceding two sections is\\nthat you can rewrite both the perceptron algorithm and the K-means\\nalgorithm so that they only ever depend on kernel products between data\\npoints, and never on the actual datapoints themselves . This is a very pow-\\nerful notion, as it has enabled the development of a large number of\\nnon-linear algorithms essentially “for free” (by applying the so-called\\nkerneltrick , that you’ve just seen twice).\\nThis raises an interesting question. If you have rewritten these\\nalgorithms so that they only depend on the data through a function\\nK:X\\x02X ! R, can you stick anyfunction Kin these algorithms,\\nor are there some Kthat are “forbidden?” In one sense, you “could”\\nuse any K, but the real question is: for what types of functions Kdo\\nthese algorithms retain the properties that we expect them to have\\n(like convergence, optimality, etc.)?\\nOne way to answer this question is to say that K(\\x01,\\x01)is a valid\\nkernel if it corresponds to the inner product between two vectors.\\nThat is, Kis valid if there exists a function fsuch that K(x,z) =\\nf(x)\\x01f(z). This is a direct deﬁnition and it should be clear that if K\\nsatisﬁes this, then the algorithms go through as expected (because\\nthis is how we derived them).\\nYou’ve already seen the general class of poly nomial kernels ,\\nwhich have the form:\\nK(poly)\\nd(x,z) =\\x10\\n1+x\\x01z\\x11d\\n(11.13)\\nwhere dis a hyperparameter of the kernel. These kernels correspond\\nto polynomial feature expansions.\\nThere is an alternative characterization of a valid kernel function\\nthat is more mathematical. It states that K:X\\x02X ! Ris a kernel if\\nKispositive semi -definite (or, in shorthand, psd). This property is\\nalso sometimes called Mer cer’s condition. In this context, this means\\nthefor all functions fthat are square integrable (i.e.,R\\nf(x)2dx<¥),\\nother than the zero function, the following property holds:\\nZZ\\nf(x)K(x,z)f(z)dxdz>0 ( 11.14)\\nThis likely seems like it came out of nowhere. Unfortunately, the\\nconnection is well beyond the scope of this book, but is covered well\\nis external sources. For now, simply take it as a given that this is an\\nequivalent requirement. (For those so inclined, the appendix of this\\nbook gives a proof, but it requires a bit of knowledge of function\\nspaces to understand.)\\nThe question is: why is this alternative characterization useful? It\\nis useful because it gives you an alternative way to construct kernelkernel methods 147\\nfunctions. For instance, using it you can easily prove the following,\\nwhich would be difﬁcult from the deﬁnition of kernels as inner prod-\\nucts after feature mappings.\\nTheorem 13(Kernel Addition) .If K 1and K 2are kernels, the K deﬁned\\nby K(x,z) =K1(x,z) +K2(x,z)is also a kernel.\\nProof of Theorem 13.You need to verify the positive semi-deﬁnite\\nproperty on K. You can do this as follows:\\nZZ\\nf(x)K(x,z)f(z)dxdz=ZZ\\nf(x)[K1(x,z) +K2(x,z)]f(z)dxdz deﬁnition of K\\n(11.15)\\n=ZZ\\nf(x)K1(x,z)f(z)dxdz\\n+ZZ\\nf(x)K2(x,z)f(z)dxdz distributive rule\\n(11.16)\\n>0+0 K1andK2are psd\\n(11.17)\\nMore generally, any positive linear combination of kernels is still a\\nkernel. Speciﬁcally, if K1, . . . , KMare all kernels, and a1, . . . , aM\\x150,\\nthen K(x,z) =åmamKm(x,z)is also a kernel.\\nYou can also use this property to show that the following Gaus -\\nsian kernel(also called the RBF kernel) is also psd:\\nK(RBF)\\ng(x,z) =exph\\n\\x00gjjx\\x00zjj2i\\n(11.18)\\nHere gis a hyperparameter that controls the width of this Gaussian-\\nlike bumps. To gain an intuition for what the RBF kernel is doing,\\nconsider what prediction looks like in the perceptron:\\nf(x) =å\\nnanK(xn,x) +b (11.19)\\n=å\\nnanexph\\n\\x00gjjxn\\x00zjj2i\\n(11.20)\\nIn this computation, each training example is getting to “vote” on the\\nlabel of the test point x. The amount of “vote” that the nth training\\nexample gets is proportional to the negative exponential of the dis-\\ntance between the test point and itself. This is very much like an RBF\\nneural network, in which there is a Gaussian “bump” at each training\\nexample, with variance 1/ (2g), and where the ans act as the weights\\nconnecting these RBF bumps to the output.\\nShowing that this kernel is positive deﬁnite is a bit of an exercise\\nin analysis (particularly, integration by parts), but otherwise not\\ndifﬁcult. Again, the proof is provided in the appendix.148 a course in machine learning\\nSo far, you have seen two bsaic classes of kernels: polynomial\\nkernels ( K(x,z) = ( 1+x\\x01z)d), which includes the linear kernel\\n(K(x,z) = x\\x01z) and RBF kernels ( K(x,z) = exp[\\x00gjjx\\x00zjj2]). The\\nformer have a direct connection to feature expansion; the latter to\\nRBF networks. You also know how to combine kernels to get new\\nkernels by addition. In fact, you can do more than that: the product\\nof two kernels is also a kernel.\\nAs far as a “library of kernels” goes, there are many. Polynomial\\nand RBF are by far the most popular. A commonly used, but techni-\\ncally invalid kernel, is the hyperbolic-tangent kernel, which mimics\\nthe behavior of a two-layer neural network. It is deﬁned as:\\nK(tanh)=tanh(1+x\\x01z) Warning: not psd (11.21)\\nA ﬁnal example, which is not very common, but is nonetheless\\ninteresting, is the all-subsets kernel. Suppose that your Dfeatures\\nare all binary : all take values 0 or 1. Let A\\x12f1, 2, . . . Dgbe a subset\\nof features, and let fA(x) =V\\nd2Axdbe the conjunction of all the\\nfeatures in A. Let f(x)be a feature vector over allsuch As, so that\\nthere are 2Dfeatures in the vector f. You can compute the kernel\\nassociated with this feature mapping as:\\nK(subs)(x,z) =Õ\\nd\\x10\\n1+xdzd\\x11\\n(11.22)\\nVerifying the relationship between this kernel and the all-subsets\\nfeature mapping is left as an exercise (but closely resembles the ex-\\npansion for the quadratic kernel).\\n11.5Support Vector Machines\\nKernelization predated support vector machines, but SVMs are def-\\ninitely the model that popularized the idea. Recall the deﬁnition of\\nthe soft-margin SVM from Chapter 7.7and in particular the opti-\\nmization problem ( 7.38), which attempts to balance a large margin\\n(smalljjwjj2) with a small loss (small xns, where xnis the slack on\\nthenth training example). This problem is repeated below:\\nmin\\nw,b,x1\\n2jjwjj2+Cå\\nnxn (11.23)\\nsubj. to yn(w\\x01xn+b)\\x151\\x00xn (8n)\\nxn\\x150 (8n)\\nPreviously, you optimized this by explicitly computing the slack\\nvariables xn, given a solution to the decision boundary, wand b.\\nHowever, you are now an expert with using Lagrange multiplierskernel methods 149\\nto optimize constrained problems! The overall goal is going to be to\\nrewrite the SVM optimization problem in a way that it no longer ex-\\nplicitly depends on the weights wand only depends on the examples\\nxnthrough kernel products.\\nThere are 2 Nconstraints in this optimization, one for each slack\\nconstraint and one for the requirement that the slacks are non-\\nnegative. Unlike the last time, these constraints are now inequalities ,\\nwhich require a slightly different solution. First, you rewrite all the\\ninequalities so that they read as something\\x150 and then add cor-\\nresponding Lagrange multipliers. The main difference is that the\\nLagrange multipliers are now constrained to be non-negative, and\\ntheir sign in the augmented objective function matters.\\nThe second set of constraints is already in the proper form; the\\nﬁrst set can be rewritten as yn(w\\x01xn+b)\\x001+xn\\x150. You’re now\\nready to construct the Lagrangian, using multipliers anfor the ﬁrst\\nset of constraints and bnfor the second set.\\nL(w,b,x,a,b) =1\\n2jjwjj2+Cå\\nnxn\\x00å\\nnbnxn (11.24)\\n\\x00å\\nnan[yn(w\\x01xn+b)\\x001+xn] (11.25)\\nThe new optimization problem is:\\nmin\\nw,b,xmax\\na\\x150max\\nb\\x150L(w,b,x,a,b) (11.26)\\nThe intuition is exactly the same as before. If you are able to ﬁnd a\\nsolution that satisﬁes the constraints (e.g., the purple term is prop-\\nerly non-negative), then the bns cannot do anything to “hurt” the\\nsolution. On the other hand, if the purple term isnegative, then the\\ncorresponding bncan go to +¥, breaking the solution.\\nYou can solve this problem by taking gradients. This is a bit te-\\ndious, but and important step to realize how everything ﬁts together.\\nSince your goal is to remove the dependence on w, the ﬁrst step is to\\ntake a gradient with respect to w, set it equal to zero, and solve for w\\nin terms of the other variables.\\nrwL=w\\x00å\\nnanynxn=0() w=å\\nnanynxn (11.27)\\nAt this point, you should immediately recognize a similarity to the\\nkernelized perceptron: the optimal weight vector takes exactly the\\nsame form in both algorithms.\\nYou can now take this new expression for wand plug it back in to\\nthe expression for L, thus removing wfrom consideration. To avoid\\nsubscript overloading, you should replace the nin the expression for150 a course in machine learning\\nwwith, say, m. This yields:\\nL(b,x,a,b) =1\\n2\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cå\\nmamymxm\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n+Cå\\nnxn\\x00å\\nnbnxn (11.28)\\n\\x00å\\nnan\"\\nyn \"\\nå\\nmamymxm#\\n\\x01xn+b!\\n\\x001+xn#\\n(11.29)\\nAt this point, it’s convenient to rewrite these terms; be sure you un-\\nderstand where the following comes from:\\nL(b,x,a,b) =1\\n2å\\nnå\\nmanamynymxn\\x01xm+å\\nn(C\\x00bn)xn (11.30)\\n\\x00å\\nnå\\nmanamynymxn\\x01xm\\x00å\\nnan(ynb\\x001+xn)\\n(11.31)\\n=\\x001\\n2å\\nnå\\nmanamynymxn\\x01xm+å\\nn(C\\x00bn)xn (11.32)\\n\\x00bå\\nnanyn\\x00å\\nnan(xn\\x001) (11.33)\\nThings are starting to look good: you’ve successfully removed the de-\\npendence on w,andeverything is now written in terms of dot prod-\\nucts between input vectors! This might still be a difﬁcult problem to\\nsolve, so you need to continue and attempt to remove the remaining\\nvariables bandx.\\nThe derivative with respect to bis:\\n¶L\\n¶b=\\x00å\\nnanyn=0 ( 11.34)\\nThis doesn’t allow you to substitute b with something (as you did\\nwith w), but it does mean that the fourth term ( bånanyn) goes to\\nzero at the optimum.\\nThe last of the original variables is xn; the derivatives in this case\\nlook like:\\n¶L\\n¶xn=C\\x00bn\\x00an() C\\x00bn=an (11.35)\\nAgain, this doesn’t allow you to substitute, but it does mean that you\\ncan rewrite the second term, which as ån(C\\x00bn)xnasånanxn. This\\nthen cancels with (most of) the ﬁnal term. However, you need to be\\ncareful to remember something. When we optimize, both anandbn\\nare constrained to be non-negative. What this means is that since we\\nare dropping bfrom the optimization, we need to ensure that an\\x14C,\\notherwise the corresponding bwill need to be negative, which is notkernel methods 151\\nallowed. You ﬁnally wind up with the following, where xn\\x01xmhas\\nbeen replaced by K(xn,xm):\\nL(a) =å\\nnan\\x001\\n2å\\nnå\\nmanamynymK(xn,xm) (11.36)\\nIf you are comfortable with matrix notation, this has a very compact\\nform. Let 1denote the N-dimensional vector of all 1s, let ydenote\\nthe vector of labels and let Gbe the N\\x02Nmatrix, where Gn,m=\\nynymK(xn,xm), then this has the following form:\\nL(a) =a>1\\x001\\n2a>Ga (11.37)\\nThe resulting optimization problem is to maximizeL(a)as a function\\nofa, subject to the constraint that the ans are all non-negative and\\nless than C(because of the constraint added when removing the b\\nvariables). Thus, your problem is:\\nmina\\x00L(a) =1\\n2å\\nnå\\nmanamynymK(xn,xm)\\x00å\\nnan (11.38)\\nsubj. to 0\\x14an\\x14C (8n)\\nOne way to solve this problem is gradient descent on a. The only\\ncomplication is making sure that the as satisfy the constraints. In\\nthis case, you can use a projected gradientalgorithm: after each\\ngradient update, you adjust your parameters to satisfy the constraints\\nbyprojecting them into the feasible region. In this case, the projection\\nis trivial: if, after a gradient step, any an<0, simply set it to 0; if any\\nan>C, set it to C.\\n11.6Understanding Support Vector Machines\\nThe prior discussion involved quite a bit of math to derive a repre-\\nsentation of the support vector machine in terms of the Lagrange\\nvariables. This mapping is actually sufﬁciently standard that every-\\nthing in it has a name. The original problem variables ( w,b,x) are\\ncalled the primal variables ; the Lagrange variables are called the\\ndual variables . The optimization problem that results after removing\\nall of the primal variables is called the dual prob lem.\\nA succinct way of saying what you’ve done is: you found that after\\nconverting the SVM into its dual, it is possible to kernelize.\\nTo understand SVMs, a ﬁrst step is to peek into the dual formula-\\ntion, Eq ( 11.38). The objective has two terms: the ﬁrst depends on the\\ndata, and the second depends only on the dual variables. The ﬁrst\\nthing to notice is that, because of the second term, the as “want” to152 a course in machine learning\\nget as large as possible. The constraint ensures that they cannot ex-\\nceed C, which means that the general tendency is for the as to grow\\nas close to Cas possible.\\nTo further understand the dual optimization problem, it is useful\\nto think of the kernel as being a measure of similarity between two\\ndata points. This analogy is most clear in the case of RBF kernels,\\nbut even in the case of linear kernels, if your examples all have unit\\nnorm, then their dot product is still a measure of similarity. Since you\\ncan write the prediction function as f(ˆx) = sign(ånanynK(xn, ˆx)), it\\nis natural to think of anas the “importance” of training example n,\\nwhere an=0 means that it is not used at all at test time.\\nConsider two data points that have the same label; namely, yn=\\nym. This means that ynym= + 1 and the objective function has a term\\nthat looks like anamK(xn,xm). Since the goal is to make this term\\nsmall, then one of two things has to happen: either Khas to be small,\\noranamhas to be small. If Kis already small, then this doesn’t affect\\nthe setting of the corresponding as. But if Kis large, then this strongly\\nencourages at least one of anoramto go to zero. So if you have two\\ndata points that are very similar andhave the same label, at least one\\nof the corresponding as will be small. This makes intuitive sense: if\\nyou have two data points that are basically the same (both in the x\\nand ysense) then you only need to “keep” one of them around.\\nSuppose that you have two data points with different labels:\\nynym=\\x001. Again, if K(xn,xm)is small, nothing happens. But if\\nit is large, then the corresponding as are encouraged to be as large as\\npossible. In other words, if you have two similar examples with dif-\\nferent labels, you are strongly encouraged to keep the corresponding\\nas as large as C.\\nAn alternative way of understanding the SVM dual problem is\\ngeometrically. Remember that the whole point of introducing the\\nvariable anwas to ensure that the nth training example was correctly\\nclassiﬁed, modulo slack. More formally, the goal of anis to ensure\\nthat yn(w\\x01xn+b)\\x001+xn\\x150. Suppose that this constraint it\\nnotsatisﬁed. There is an important result in optimization theory,\\ncalled the Karush -Kuhn -T ucker conditions (orKKT conditions , for\\nshort) that states that at the optimum, the product of the Lagrange\\nmultiplier for a constraint, and the value of that constraint, will equal\\nzero. In this case, this says that at the optimum, you have:\\nanh\\nyn(w\\x01xn+b)\\x001+xni\\n=0 ( 11.39)\\nIn order for this to be true, it means that (at least) one of the follow-\\ning must be true:\\nan=0 or y n(w\\x01xn+b)\\x001+xn=0 ( 11.40)kernel methods 153\\nA reasonable question to ask is: under what circumstances will an\\nbenon-zero? From the KKT conditions, you can discern that ancan\\nbe non-zero only when the constraint holds exactly ; namely, that\\nyn(w\\x01xn+b)\\x001+xn=0. When does that constraint hold ex-\\nactly ? It holds exactly only for those points precisely on the margin of\\nthe hyperplane.\\nIn other words, the only training examples for which an6=0\\nare those that lie precisely 1 unit away from the maximum margin\\ndecision boundary! (Or those that are “moved” there by the corre-\\nsponding slack.) These points are called the support vectors because\\nthey “support” the decision boundary. In general, the number of sup-\\nport vectors is far smaller than the number of training examples, and\\ntherefore you naturally end up with a solution that only uses a subset\\nof the training data.\\nFrom the ﬁrst discussion, you know that the points that wind up\\nbeing support vectors are exactly those that are “confusable” in the\\nsense that you have to examples that are nearby, but have different la-\\nbels. This is a completely in line with the previous discussion. If you\\nhave a decision boundary, it will pass between these “confusable”\\npoints, and therefore they will end up being part of the set of support\\nvectors.\\n11.7Further Reading\\nTODO further reading12 | L EARNING THEORY\\nDependencies:By now ,you are an expert at building learning algorithms. You\\nprobably understand how they work, intuitively. And you under-\\nstand why they should generalize. However, there are several basic\\nquestions you might want to know the answer to. Is learning always\\npossible? How many training examples will I need to do a good job\\nlearning? Is my test performance going to be much worse than my\\ntraining performance? The key idea that underlies all these answer is\\nthat simple functions generalize well.\\nThe amazing thing is that you can actually prove strong results\\nthat address the above questions. In this chapter, you will learn\\nsome of the most important results in learning theory that attempt\\nto answer these questions. The goal of this chapter is not theory for\\ntheory’s sake, but rather as a way to better understand why learning\\nmodels work, and how to use this theory to build better algorithms.\\nAs a concrete example, we will see how 2-norm regularization prov-\\nably leads to better generalization performance, thus justifying our\\ncommon practice!\\n12.1The Role of Theory\\nIn contrast to the quote at the start of this chapter, a practitioner\\nfriend once said “I would happily give up a few percent perfor-\\nmance for an algorithm that I can understand.” Both perspectives\\nare completely valid, and are actually not contradictory. The second\\nstatement is presupposing that theory helps you understand, which\\nhopefully you’ll ﬁnd to be the case in this chapter.\\nTheory can serve two roles. It can justify and help understand\\nwhy common practice works. This is the “theory after” view. It can\\nalso serve to suggest new algorithms and approaches that turn out to\\nwork well in practice. This is the “theory before” view. Often, it turns\\nout to be a mix. Practitioners discover something that works surpris-\\ningly well. Theorists ﬁgure out why it works and prove something\\nabout it. And in the process, they make it better or ﬁnd new algo-Learning Objectives:\\n• Explain why inductive bias is\\nnecessary.\\n• Deﬁne the PAC model and explain\\nwhy both the “P” and “A” are\\nnecessary.\\n• Explain the relationship between\\ncomplexity measures and regulariz-\\ners.\\n• Identify the role of complexity in\\ngeneralization.\\n• Formalize the relationship between\\nmargins and complexity.The Universe is under no obligation to make sense to you. –\\nNeil deGrasse Tysonlearning theory 155\\nrithms that more directly exploit whatever property it is that made\\nthe theory go through.\\nTheory can also help you understand what’s possible and what’s\\nnot possible. One of the ﬁrst things we’ll see is that, in general, ma-\\nchine learning can not work. Of course it does work, so this means\\nthat we need to think harder about what it means for learning algo-\\nrithms to work. By understanding what’s not possible, you can focus\\nour energy on things that are.\\nProbably the biggest practical success story for theoretical machine\\nlearning is the theory of boost ing, which you won’t actually see in\\nthis chapter. (You’ll have to wait for Chapter 13.) Boosting is a very\\nsimple style of algorithm that came out of theoretical machine learn-\\ning, and has proven to be incredibly successful in practice. So much\\nso that it is one of the de facto algorithms to run when someone gives\\nyou a new data set. In fact, in 2004 , Yoav Freund and Rob Schapire\\nwon the ACM’s Paris Kanellakis Award for their boosting algorithm\\nAdaBoost. This award is given for theoretical accomplishments that\\nhave had a signiﬁcant and demonstrable effect on the practice of\\ncomputing.1 1In2008 , Corinna Cortes and Vladimir\\nVapnik won it for support vector\\nmachines.\\n12.2Induction is Impossible\\nOne nice thing about theory is that it forces you to be precise about\\nwhat you are trying to do. You’ve already seen a formal deﬁnition\\nof binary classiﬁcation in Chapter 6. But let’s take a step back and\\nre-analyze what it means to learn to do binary classiﬁcation.\\nFrom an algorithmic perspective, a natural question is whether\\nthere is an “ultimate” learning algorithm, Aawesome, that solves the\\nBinary Classiﬁcation problem above. In other words, have you been\\nwasting your time learning about KNN and Perceptron and decision\\ntrees, whenAawesomeis out there.\\nWhat would such an ultimate learning algorithm do? You would\\nlike it to take in a data set Dand produce a function f. No matter\\nwhat Dlooks like, this function fshould get perfect classiﬁcation on\\nall future examples drawn from the same distribution that produced\\nD.\\nA little bit of introspection should demonstrate that this is impos-\\nsible. For instance, there might be label noise in our distribution. As\\na very simple example, let X=f\\x001,+1g(i.e., a one-dimensional,\\nbinary distribution. Deﬁne the data distribution as:\\nD(h+1i,+1) =0.4 D(h\\x001i,\\x001) =0.4 ( 12.1)\\nD(h+1i,\\x001) =0.1 D(h\\x001i,+1) =0.1 ( 12.2)\\nIn other words, 80% of data points in this distrubtion have x=y156 a course in machine learning\\nand 20% don’t. No matter what function your learning algorithm\\nproduces, there’s no way that it can do better than 20% error on this\\ndata. It’s clear that if your algorithm pro-\\nduces a deterministic function that\\nit cannot do better than 20% error.\\nWhat if it produces a stochastic (aka\\nrandomized) function??Given this, it seems hopeless to have an algorithm Aawesomethat\\nalways achieves an error rate of zero. The best that we can hope is\\nthat the error rate is not “too large.”\\nUnfortunately, simply weakening our requirement on the error\\nrate is not enough to make learning possible. The second source of\\ndifﬁculty comes from the fact that the only access we have to the\\ndata distribution is through sampling. In particular, when trying to\\nlearn about a distribution like that in 12.1, you only get to see data\\npoints drawn from that distribution. You know that “eventually” you\\nwill see enough data points that your sample is representative of the\\ndistribution, but it might not happen immediately. For instance, even\\nthough a fair coin will come up heads only with probability 1/2, it’s\\ncompletely plausible that in a sequence of four coin ﬂips you never\\nsee a tails, or perhaps only see one tails.\\nSo the second thing that we have to give up is the hope that\\nAawesomewill always work. In particular, if we happen to get a lousy\\nsample of data from D, we need to allow Aawesometo do something\\ncompletely unreasonable.\\nThus, we cannot hope that Aawesomewill do perfectly, every time.\\nWe cannot even hope that it will do pretty well, all of the time. Nor\\ncan we hope that it will do perfectly, most of the time. The best best\\nwe can reasonably hope of Aawesomeis that it it will do pretty well,\\nmost of the time.\\n12.3Probably Approximately Correct Learning\\nProb ablyApprox imately Correct (PAC ) learning is a formalism\\nof inductive learning based on the realization that the best we can\\nhope of an algorithm is that it does a good job (i.e., is approximately\\ncorrect), most of the time (i.e., it is probably appoximately correct).2 2Leslie Valiant invented the notion\\nof PAC learning in 1984 . In2011 ,\\nhe received the Turing Award, the\\nhighest honor in computing for his\\nwork in learning theory, computational\\ncomplexity and parallel systems.Consider a hypothetical learning algorithm. You run it on ten dif-\\nferent binary classiﬁcation data sets. For each one, it comes back with\\nfunctions f1,f2, . . . , f10. For some reason, whenever you run f4on a\\ntest point, it crashes your computer. For the other learned functions,\\ntheir performance on test data is always at most 5% error. If this\\nsitutation is guaranteed to happen, then this hypothetical learning\\nalgorithm is a PAC learning algorithm. It satisﬁes “probably” because\\nit only failed in one out of ten cases, and it’s “approximate” because\\nit achieved low, but non-zero, error on the remainder of the cases.\\nThis leads to the formal deﬁnition of an (e,d)PAC-learning algo-\\nrithm. In this deﬁnition, eplays the role of measuring accuracy (inlearning theory 157\\nthe previous example, e=0.05) and dplays the role of measuring\\nfailure (in the previous, d=0.1).\\nDeﬁnitions 1.An algorithmAis an(e,d)-P AC learning algorithm if, for\\nalldistributionsD: given samples from D, the probability that it returns a\\n“bad function” is at most d; where a “bad” function is one with test error\\nrate more than eonD.\\nThere are two notions of efﬁciency that matter in PAC learning. The\\nﬁrst is the usual notion of computational complexity . You would prefer\\nan algorithm that runs quickly to one that takes forever. The second\\nis the notion of samplecom plex ity: the number of examples required\\nfor your algorithm to achieve its goals. Note that the goal of both\\nof these measure of complexity is to bound how much of a scarse\\nresource your algorithm uses. In the computational case, the resource\\nis CPU cycles. In the sample case, the resource is labeled examples.\\nDeﬁnition: An algorithmAis an efﬁcient (e,d)-PAC learning al-\\ngorithm if it is an (e,d)-PAC learning algorithm whose runtime is\\npolynomial in1\\neand1\\nd.\\nIn other words, suppose that you want your algorithm to achieve\\n4% error rate rather than 5%. The runtime required to do so should\\nno go up by an exponential factor.\\n12.4P AC Learning of Conjunctions\\nTo get a better sense of PAC learning, we will start with a completely\\nirrelevant and uninteresting example. The purpose of this example is\\nonly to help understand how PAC learning works.\\nThe setting is learning conjunctions. Your data points are binary\\nvectors, for instance x=h0, 1, 1, 0, 1i. Someone guarantees for you\\nthat there is some boolean conjunction that deﬁnes the true labeling\\nof this data. For instance, x1^:x2^x5(“or” is not allowed). In\\nformal terms, we often call the true underlying classiﬁcation function\\ntheconcept . So this is saying that the concept you are trying to learn\\nis a conjunction. In this case, the boolean function would assign a\\nnegative label to the example above.\\nSince you know that the concept you are trying to learn is a con-\\njunction, it makes sense that you would represent your function as\\na conjunction as well. For historical reasons, the function that you\\nlearn is often called a hypoth esisand is often denoted h. However,\\nin keeping with the other notation in this book, we will continue to\\ndenote it f.\\nFormally, the set up is as follows. There is some distribution DX\\nover binary data points (vectors) x=hx1,x2, . . . , xDi. There is a ﬁxed158 a course in machine learning\\nconcept conjunction cthat we are trying to learn. There is no noise,\\nso for any example x, its true label is simply y=c(x).y x1x2x3x4\\n+10 0 1 1\\n+10 1 1 1\\n-11 1 0 1\\nTable 12.1: Data set for learning con-\\njunctions.What is a reasonable algorithm in this case? Suppose that you\\nobserve the example in Table 12.1. From the ﬁrst example, we know\\nthat the true formula cannot include the term x1. If it did, this exam-\\nple would have to be negative, which it is not. By the same reason-\\ning, it cannot include x2. By analogous reasoning, it also can neither\\ninclude the term:x3nor the term:x4.\\nThis suggests the algorithm in Algorithm 12.4, colloquially the\\n“Throw Out Bad Terms” algorithm. In this algorithm, you begin with\\na function that includes all possible 2 Dterms. Note that this function\\nwill initially classify everything as negative. You then process each\\nexample in sequence. On a negative example, you do nothing. On\\na positive example, you throw out terms from fthat contradict the\\ngiven positive example. Verify that Algorithm 12.4main-\\ntains an invariant that it always errs\\non the side of classifying examples\\nnegative and never errs the other\\nway.?If you run this algorithm on the data in Table 12.1, the sequence of\\nfs that you cycle through are:\\nf0(x) =x1^:x1^x2^:x2^x3^:x3^x4^:x4 (12.3)\\nf1(x) =:x1^:x2^x3^x4 (12.4)\\nf2(x) =:x1^x3^x4 (12.5)\\nf3(x) =:x1^x3^x4 (12.6)\\nThe ﬁrst thing to notice about this algorithm is that after processing\\nan example, it is guaranteed to classify that example correctly. This\\nobservation requires that there is no noise in the data.\\nThe second thing to notice is that it’s very computationally ef-\\nﬁcient. Given a data set of Nexamples in Ddimensions, it takes\\nO(ND)time to process the data. This is linear in the size of the data\\nset.\\nHowever, in order to be an efﬁcient (e,d)-PAC learning algorithm,\\nyou need to be able to get a bound on the samplecom plex ityof this\\nalgorithm. Sure, you know that its run time is linear in the number\\nof example N. But how many examples Ndo you need to see in order\\nto guarantee that it achieves an error rate of at most e(in all but d-\\nmany cases)? Perhaps Nhas to be gigantic (like 22D/e) to (probably)\\nguarantee a small error.\\nThe goal is to prove that the number of samples Nrequired to\\n(probably) achieve a small error is not-too-big. The general proof\\ntechnique for this has essentially the same ﬂavor as almost every PAC\\nlearning proof around. First, you deﬁne a “bad thing.” In this case,\\na “bad thing” is that there is some term (say :x8) that should have\\nbeen thrown out, but wasn’t. Then you say: well, bad things happen.\\nThen you notice that if this bad thing happened, you must not havelearning theory 159\\nAlgorithm 31Binary Conjunction Train (D)\\n1:f x1^:x1^x2^:x2^\\x01\\x01\\x01^ xD^:xD // initialize function\\n2:for all positive examples (x,+1)inDdo\\n3:ford=1. . .Ddo\\n4: ifxd=0then\\n5: f fwithout term “ xd”\\n6: else\\n7: f fwithout term “:xd”\\n8: end if\\n9:end for\\n10:end for\\n11:return f\\nseen any positive training examples with x8=0. So example with\\nx8=0 must have low probability (otherwise you would have seen\\nthem). So bad things must not be that common.\\nTheorem 14.With probability at least (1\\x00d): Algorithm 12.4requires at\\nmost N =. . .examples to achieve an error rate \\x14e.\\nProof of Theorem 14.Letcbe the concept you are trying to learn and\\nletDbe the distribution that generates the data.\\nA learned function fcan make a mistake if it contains anyterm t\\nthat is not in c. There are initially 2 Dmany terms in f, and any (or\\nall!) of them might not be in c. We want to ensure that the probability\\nthat fmakes an error is at most e. It is sufﬁcient to ensure that\\nFor a term t(e.g.,:x5), we say that t“negates” an example xif\\nt(x) = 0. Call a term t“bad” if (a) it does not appear in cand (b) has\\nprobability at least e/2Dof appearing (with respect to the unknown\\ndistributionDover data points).\\nFirst, we show that if we have no bad terms left in f, then fhas an\\nerror rate at most e.\\nWe know that fcontains at most 2Dterms, since is begins with 2 D\\nterms and throws them out.\\nThe algorithm begins with 2 Dterms (one for each variable and\\none for each negated variable). Note that fwill only make one type\\nof error: it can call positive examples negative, but can never call a\\nnegative example positive. Let cbe the true concept (true boolean\\nformula) and call a term “bad” if it does not appear in c. A speciﬁc\\nbad term (e.g.,:x5) will cause fto err only on positive examples\\nthat contain a corresponding bad value (e.g., x5=1). TODO... ﬁnish\\nthis\\nWhat we’ve shown in this theorem is that: ifthe true underly-\\ning concept is a boolean conjunction, andthere is no noise, then the\\n“Throw Out Bad Terms” algorithm needs N\\x14. . . examples in order160 a course in machine learning\\nto learn a boolean conjunction that is (1\\x00d)-likely to achieve an er-\\nror of at most e. That is to say, that the samplecom plex ityof “Throw\\nOut Bad Terms” is . . . . Moreover, since the algorithm’s runtime is\\nlinear in N, it is an efﬁcient PAC learning algorithm.\\n12.5Occam’s Razor: Simple Solutions Generalize\\nThe previous example of boolean conjunctions is mostly just a warm-\\nup exercise to understand PAC-style proofs in a concrete setting.\\nIn this section, you get to generalize the above argument to a much\\nlarger range of learning problems. We will still assume that there is\\nno noise, because it makes the analysis much simpler. (Don’t worry:\\nnoise will be added eventually.)\\nWilliam of Occam (c. 1288 – c.1348 ) was an English friar and\\nphilosopher is is most famous for what later became known as Oc-\\ncam’s razor and popularized by Bertrand Russell. The principle ba-\\nsically states that you should only assume as much as you need. Or,\\nmore verbosely, “if one can explain a phenomenon without assuming\\nthis or that hypothetical entity, then there is no ground for assuming\\nit i.e. that one should always opt for an explanation in terms of the\\nfewest possible number of causes, factors, or variables.” What Occam\\nactually wrote is the quote that began this chapter.\\nIn a machine learning context, a reasonable paraphrase is “simple\\nsolutions generalize well.” In other words, you have 10, 000 features\\nyou could be looking at. If you’re able to explain your predictions\\nusing just 5 of them, or using all 10, 000 of them, then you should just\\nuse the 5.\\nThe Occam’s razor theorem states that this is a good idea, theo-\\nretically. It essentially states that if you are learning some unknown\\nconcept, and if you are able to ﬁt your training data perfectly, but you\\ndon’t need to resort to a huge class of possible functions to do so,\\nthen your learned function will generalize well. It’s an amazing theo-\\nrem, due partly to the simplicity of its proof. In some ways, the proof\\nis actually easier than the proof of the boolean conjunctions, though it\\nfollows the same basic argument.\\nIn order to state the theorem explicitly, you need to be able to\\nthink about a hypoth esisclass . This is the set of possible hypotheses\\nthat your algorithm searches through to ﬁnd the “best” one. In the\\ncase of the boolean conjunctions example, the hypothesis class, H,\\nis the set of all boolean formulae over D-many variables. In the case\\nof a perceptron, your hypothesis class is the set of all possible linear\\nclassiﬁers. The hypothesis class for boolean conjunctions is ﬁnite ; the\\nhypothesis class for linear classiﬁers is inﬁnite . For Occam’s razor, we\\ncan only work with ﬁnite hypothesis classes.learning theory 161\\nTheorem 15(Occam’s Bound) .SupposeAis an algorithm that learns\\na function f from some ﬁnite hypothesis classH. Suppose the learned\\nfunction always gets zero error on the training data. Then, the sample com-\\nplexity of f is at most logjHj.\\nTODO COMMENTS\\nProof of Theorem 15.TODO\\nThis theorem applies directly to the “Throw Out Bad Terms” algo-\\nrithm, since (a) the hypothesis class is ﬁnite and (b) the learned func-\\ntion always achieves zero error on the training data. To apply Oc-\\ncam’s Bound, you need only compute the size of the hypothesis class\\nHof boolean conjunctions. You can compute this by noticing that\\nthere are a total of 2 Dpossible terms in any formula in H. Moreover,\\neach term may or may not be in a formula. So there are 22D=4D\\npossible formulae; thus, jHj=4D. Applying Occam’s Bound, we see\\nthat the sample complexity of this algorithm is N\\x14. . . .\\nOf course, Occam’s Bound is general enough to capture other\\nlearning algorithms as well. In particular, it can capture decision\\ntrees! In the no-noise setting, a decision tree will always ﬁt the train-\\ning data perfectly. The only remaining difﬁculty is to compute the\\nsize of the hypothesis class of a decision tree learner.\\nFigure 12.1:thy:dt : picture of full\\ndecision treeFor simplicity’s sake, suppose that our decision tree algorithm\\nalways learns complete trees: i.e., every branch from root to leaf\\nis length D. So the number of split points in the tree (i.e., places\\nwhere a feature is queried) is 2D\\x001. (See Figure 12.1.) Each split\\npoint needs to be assigned a feature: there D-many choices here.\\nThis gives D2D\\x001trees. The last thing is that there are 2Dleaves\\nof the tree, each of which can take two possible values, depending\\non whether this leaf is classiﬁed as +1 or\\x001: this is 2\\x022D=2D+1\\npossibilities. Putting this all togeter gives a total number of trees\\njHj=D2D\\x0012D+1=D22D=D4D. Applying Occam’s Bound, we see\\nthat TODO examples is enough to learn a decision tree!\\n12.6Complexity of Inﬁnite Hypothesis Spaces\\nOccam’s Bound is a fantastic result for learning over ﬁnite hypothesis\\nspaces. Unfortunately, it is completely useless when jHj=¥. This is\\nbecause the proof works by using each of the Ntraining examples to\\n“throw out” bad hypotheses until only a small number are left. But if\\njHj=¥, and you’re throwing out a ﬁnite number at each step, there\\nwill always be an inﬁnite number remaining.\\nThis means that, if you want to establish sample complexity results\\nfor inﬁnite hypothesis spaces, you need some new way of measuring162 a course in machine learning\\ntheir “size” or “complexity.” A prototypical way of doing this is to\\nmeasure the complexity of a hypothesis class as the number of different\\nthings it can do .\\nAs a silly example, consider boolean conjunctions again. Your\\ninput is a vector of binary features. However, instead of representing\\nyour hypothesis as a boolean conjunction, you choose to represent\\nit as a conjunction of inequalities. That is, instead of writing x1^\\n:x2^x5, you write [x1>0.2]^[x2<0.77]^[x5<p/4]. In this\\nrepresentation, for each feature, you need to choose an inequality\\n(<or>) and a threshold. Since the thresholds can be arbitrary real\\nvalues, there are now inﬁnitely many possibilities: jHj=2D\\x02¥=¥.\\nHowever, you can immediately recognize that on binary features,\\nthere really is no difference between [x2<0.77]and[x2<0.12]and\\nany other number of inﬁnitely many possibilities. In other words,\\neven though there are inﬁnitely many hypotheses, there are only ﬁnitely\\nmany behaviors.\\nFigure 12.2:thy:vcex : ﬁgure with three\\nand four examplesThe Vapnik-Cher novenkis dimen sion (orVCdimen sion ) is a\\nclassic measure of complexity of inﬁnite hypothesis classes based on\\nthis intuition3. The VC dimension is a very classiﬁcation-oriented no-\\n3Yes, this is the same Vapnik who\\nis credited with the creation of the\\nsupport vector machine.tion of complexity. The idea is to look at a ﬁnite set of unlabeled ex-\\namples, such as those in Figure 12.2. The question is: no matter how\\nthese points were labeled, would we be able to ﬁnd a hypothesis that\\ncorrectly classiﬁes them. The idea is that as you add more points,\\nbeing able to represent an arbitrary labeling becomes harder and\\nharder. For instance, regardless of how the three points are labeled,\\nyou can ﬁnd a linear classiﬁer that agrees with that classiﬁcation.\\nHowever, for the four points, there exists a labeling for which you\\ncannot ﬁnd a perfect classiﬁer. The VC dimension is the maximum\\nnumber of points for which you can always ﬁnd such a classiﬁer. What is that labeling? What is it’s\\nname??You can think of VC dimension as a game between you and an\\nadversary. To play this game, youchoose Kunlabeled points however\\nyou want. Then your adversary looks at those Kpoints and assigns\\nbinary labels to them them however they want. You must then ﬁnd\\na hypothesis (classiﬁer) that agrees with their labeling. You win if\\nyou can ﬁnd such a hypothesis; they win if you cannot. The VC\\ndimension of your hypothesis class is the maximum number of points\\nKso that you can always win this game. This leads to the following\\nformal deﬁnition, where you can interpret there exists as your move\\nand for all as adversary’s move.\\nDeﬁnitions 2.For data drawn from some space X, the VC dimension of\\na hypothesis spaceHoverXis the maximal K such that: there exists a set\\nX\\x12X of sizejXj=K, such that for all binary labelings of X, there exists\\na function f2H that matches this labeling.learning theory 163\\nIn general, it is much easier to show that the VC dimension is at\\nleast some value; it is much harder to show that it is at most some\\nvalue. For example, following on the example from Figure 12.2, the\\nimage of three points (plus a little argumentation) is enough to show\\nthat the VC dimension of linear classiﬁers in two dimension is at least\\nthree .\\nTo show that the VC dimension is exactly three it sufﬁces to show\\nthat you cannot ﬁnd a set of four points such that you win this game\\nagainst the adversary. This is much more difﬁcult. In the proof that\\nthe VC dimension is at least three, you simply need to provide an\\nexample of three points, and then work through the small number of\\npossible labelings of that data. To show that it is at most three, you\\nneed to argue that no matter what set of four point you pick, you\\ncannot win the game.\\n12.7Further Reading\\nTODO13 | E NSEMBLE METHODS\\nDependencies:Groups of people can often make better decisions than\\nindividuals, especially when group members each come in with\\ntheir own biases. The same is true in machine learning. Ensemble\\nmethods are learning models that achieve performance by combining\\nthe opinions of multiple learners. In doing so, you can often get away\\nwith using much simpler learners and still achieve great performance.\\nMoreover, ensembles are inherantly parallel, which can make them\\nmuch more efﬁcient at training and test time, if you have access to\\nmultiple processors.\\nIn this chapter, you will learn about various ways of combining\\nbase learn ersinto ensembles . One of the shocking results we will\\nsee is that you can take a learning model that only ever does slightly\\nbetter than chance, and turn it into an arbitrarily good learning\\nmodel, though a technique known as boost ing. You will also learn\\nhow ensembles can decrease the variance of predictors as well as\\nperform regularization.\\n13.1Voting Multiple Classiﬁers\\nAll of the learning algorithms you have seen so far are deterministic.\\nIf you train a decision tree multiple times on the same data set, you\\nwill always get the same tree back. In order to get an effect out of\\nvoting multiple classiﬁers, they need to differ. There are two primary\\nways to get variability. You can either change the learning algorithm\\nor change the data set.\\nBuilding an emsemble by training different classiﬁers is the most\\nstraightforward approach. As in single-model learning, you are given\\na data set (say, for classiﬁcation). Instead of learning a single classiﬁer\\n(e.g., a decision tree) on this data set, you learn multiple different\\nclassiﬁers. For instance, you might train a decision tree, a perceptron,\\naKNN, and multiple neural networks with different architectures.\\nCall these classiﬁers f1, . . . , fM. At test time, you can make a predic-\\ntion by voting . On a test example ˆx, you compute ˆy1=f1(ˆx), . . . ,Learning Objectives:\\n• Implement bagging and explain how\\nit reduces variance in a predictor.\\n• Explain the difference between a\\nweak learner and a strong learner.\\n• Derive the AdaBoost algorithm.\\n• Understand the relationship between\\nboosting decision stumps and linear\\nclassiﬁcation.This is the central illusion in life: that randomness is a risk, that it\\nis a bad thing. . . – Nassim Nicholas Talebensemble methods 165\\nˆyM=fM(ˆx). If there are more +1s in the listhy1, . . . , yMthen you\\npredict +1; otherwise you predict \\x001.\\nThe main advantage of ensembles of different classiﬁers is that it\\nis unlikely that all classiﬁers will make the same mistake. In fact, as\\nlong as every error is made by a minority of the classiﬁers, you will\\nachieve optimal classiﬁcation! Unfortunately, the inductive biases of\\ndifferent learning algorithms are highly correlated. This means that\\ndifferent algorithms are prone to similar types of errors. In particular,\\nensembles tend to reduce the variance of classiﬁers. So if you have\\na classiﬁcation algorithm that tends to be very sensitive to small\\nchanges in the training data, ensembles are likely to be useful. Which of the classiﬁers you’ve\\nlearned about so far have high\\nvariance?? Note that the voting scheme naturally extends to multiclass clas-\\nsiﬁcation. However, it does not make sense in the contexts of regres-\\nsion, ranking or collective classiﬁcation. This is because you will\\nrarely see the same exact output predicted twice by two different\\nregression models (or ranking models or collective classiﬁcation mod-\\nels). For regression, a simple solution is to take the mean ormedian\\nprediction from the different models. For ranking and collective clas-\\nsiﬁcation, different approaches are required.\\nInstead of training different types of classiﬁers on the same data\\nset, you can train a single type of classiﬁer (e.g., decision tree) on\\nmultiple data sets. The question is: where do these multiple data sets\\ncome from, since you’re only given one at training time?\\nOne option is to fragment your original data set. For instance, you\\ncould break it into 10pieces and build decision trees on each of these\\npieces individually. Unfortunately, this means that each decision tree\\nis trained on only a very small part of the entire data set and is likely\\nto perform poorly.\\nFigure 13.1: picture of sampling with\\nreplacementA better solution is to use boot strap resampling . This is a tech-\\nnique from the statistics literature based on the following observa-\\ntion. The data set we are given, D, is a sample drawn i.i.d. from an\\nunknown distribution D. If we draw a new data set ˜Dby random\\nsampling from Dwith replacement1, then ˜Disalsoa sample fromD.\\n1To sample with replacement, imagine\\nputting all items from Din a hat. To\\ndraw a single sample, pick an element\\nat random from that hat, write it down,\\nand then put it back .Figure 13.1shows the process of bootstrap resampling of ten objects.\\nApplying this idea to ensemble methods yields a technique known\\nasbagging . You start with a single data set Dthat contains Ntrain-\\ning examples. From this single data set, you create M-many “boot-\\nstrapped training sets” ˜D1, . . . ˜DM. Each of these bootstrapped sets\\nalso contains Ntraining examples, drawn randomly from Dwith\\nreplacement. You can then train a decision tree (or other model)\\nseperately on each of these data sets to obtain classiﬁers f1, . . . , fM.\\nAs before, you can use these classiﬁers to vote on new test points.\\nNote that the bootstrapped data sets will be similar. However, they\\nwill not be toosimilar. For example, if Nis large then the number of166 a course in machine learning\\nexamples that are not present in any particular bootstrapped sample\\nis relatively large. The probability that the ﬁrst training example is\\nnot selected once is (1\\x001/N). The probability that it is not selected\\nat all is (1\\x001/N)N. As N!¥, this tends to 1/ e\\x190.3679. (Already\\nforN=1000 this is correct to four decimal points.) So only about\\n63% of the original training examples will be represented in any\\ngiven bootstrapped set.\\nFigure 13.2: graph depicting overﬁtting\\nusing regularization versus baggingSince bagging tends to reduce variance , it provides an alternative\\napproach to regularization. That is, even if each of the learned clas-\\nsiﬁers f1, . . . , fMare individually overﬁt, they are likely to be overﬁt\\nto different things. Through voting, you are able to overcome a sig-\\nniﬁcant portion of this overﬁtting. Figure 13.2shows this effect by\\ncomparing regularization via hyperparameters to regularization via\\nbagging.\\n13.2Boosting Weak Learners\\nBoosting is the process of taking a crummy learning algorithm (tech-\\nnically called a weak learner ) and turning it into a great learning\\nalgorithm (technically, a strong learner ). Of all the ideas that origi-\\nnated in the theoretical machine learning community, boosting has\\nhad—perhaps—the greatest practical impact. The idea of boosting\\nis reminiscent of what you (like me!) might have thought when you\\nﬁrst learned about ﬁle compression. If I compress a ﬁle, and then\\nre-compress it, and then re-compress it, eventually I’ll end up with a\\nﬁnal that’s only one byte in size!\\nTo be more formal, let’s deﬁne a strong learn ingalgorithmLas\\nfollows. When given a desired error rate e, a failure probability d\\nand access to “enough” labeled examples from some distribution D,\\nthen, with high probability (at least 1 \\x00d),Llearns a classiﬁer fthat\\nhas error at most e. This is precisely the deﬁnition of PAC learning\\nthat you learned about in Chapter 12. Building a strong learning\\nalgorithm might be difﬁcult. We can as if, instead, it is possible to\\nbuild a weak learn ingalgorithmWthat only has to achieve an error\\nrate of 49%, rather than some arbitrary user-deﬁned parameter e.\\n(49% is arbitrary: anything strictly less than 50% would be ﬁne.)\\nBoosting is more of a “framework” than an algorithm. It’s a frame-\\nwork for taking a weak learning algorithm Wand turning it into a\\nstrong learning algorithm. The particular boosting algorithm dis-\\ncussed here is AdaBoost , short for “adaptive boosting algorithm.”\\nAdaBoost is famous because it was one of the ﬁrst practical boosting\\nalgorithms: it runs in polynomial time and does not require you to\\ndeﬁne a large number of hyperparameters. It gets its name from the\\nlatter beneﬁt: it automatically adapts to the data that you give it.ensemble methods 167\\nAlgorithm 32AdaBoost (W,D,K)\\n1:d(0) h1\\nN,1\\nN, . . . ,1\\nNi // Initialize uniform importance to each example\\n2:fork=1. . .Kdo\\n3: f(k) W (D,d(k-1)) // Train kth classiﬁer on weighted data\\n4: ˆyn f(k)(xn),8n // Make predictions on training data\\n5:ˆe(k) ånd(k-1)\\nn[yn6=ˆyn] // Compute weighted training error\\n6:a(k) 1\\n2log\\x10\\n1\\x00ˆe(k)\\nˆe(k)\\x11\\n// Compute “adaptive” parameter\\n7:d(k)\\nn 1\\nZd(k-1)\\nn exp[\\x00a(k)ynˆyn],8n // Re-weight examples and normalize\\n8:end for\\n9:return f(ˆx) =sgn\\x02\\nåka(k)f(k)(ˆx)\\x03\\n// Return (weighted) voted classiﬁer\\nThe intuition behind AdaBoost is like studying for an exam by\\nusing a past exam. You take the past exam and grade yourself. The\\nquestions that you got right, you pay less attention to. Those that you\\ngotwrong , you study more. Then you take the exam again and repeat\\nthis process. You continually down-weight the importance of questions\\nyou routinely answer correctly and up-weight the importance of ques-\\ntions you routinely answer incorrectly. After going over the exam\\nmultiple times, you hope to have mastered everything.\\nThe precise AdaBoost training algorithm is shown in Algorithm 13.2.\\nThe basic functioning of the algorithm is to maintain a weight dis-\\ntribution d, over data points. A weak learner, f(k)is trained on this\\nweighted data. (Note that we implicitly assume that our weak learner\\ncan accept weighted training data, a relatively mild assumption that\\nis nearly always true.) The (weighted) error rate of f(k)is used to de-\\ntermine the adaptive parameter a, which controls how “important” f(k)\\nis. As long as the weak learner does, indeed, achieve <50% error,\\nthen awill be greater than zero. As the error drops to zero, agrows\\nwithout bound. What happens if the weak learn-\\ning assumption is violated and ˆeis\\nequal to 50%? What if it is worse\\nthan 50%? What does this mean, in\\npractice??After the adaptive parameter is computed, the weight distibution\\nis updated for the next iteration. As desired, examples that are cor-\\nrectly classiﬁed (for which ynˆyn= + 1) have their weight decreased\\nmultiplicatively. Examples that are incorrectly classiﬁed ( ynˆyn=\\x001)\\nhave their weight increased multiplicatively. The Zterm is a nom-\\nralization constant to ensure that the sum of dis one (i.e., dcan be\\ninterpreted as a distribution). The ﬁnal classiﬁer returned by Ad-\\naBoost is a weighted vote of the individual classiﬁers, with weights\\ngiven by the adaptive parameters.\\nTo better understand why ais deﬁned as it is, suppose that our\\nweak learner simply returns a constant function that returns the\\n(weighted) majority class. So if the total weight of positive exam-\\nples exceeds that of negative examples, f(x) = + 1 for all x; otherwise\\nf(x) =\\x001 for all x. To make the problem moderately interesting,\\nsuppose that in the original training set, there are 80 positive ex-168 a course in machine learning\\namples and 20 negative examples. In this case, f(1)(x) = + 1. It’s\\nweighted error rate will be ˆe(1)=0.2 because it gets every negative\\nexample wrong. Computing, we get a(1)=1\\n2log 4. Before normaliza-\\ntion, we get the new weight for each positive (correct) example to be\\n1 exp[\\x001\\n2log 4] =1\\n2. The weight for each negative (incorrect) example\\nbecomes 1 exp [1\\n2log 4] =2. We can compute Z=80\\x021\\n2+20\\x022=80.\\nTherefore, after normalization, the weight distribution on any single\\npositive example is1\\n160and the weight on any negative example is1\\n40.\\nHowever, since there are 80 positive examples and 20 negative exam-\\nples, the cumulative weight on all positive examples is 80 \\x021\\n160=1\\n2;\\nthe cumulative weight on all negative examples is 20 \\x021\\n40=1\\n2. Thus,\\nafter a single boosting iteration, the data has become precisely evenly\\nweighted. This guarantees that in the next iteration, our weak learner\\nmust do something more interesting than majority voting if it is to\\nachieve an error rate less than 50%, as required. This example uses concrete num-\\nbers, but the same result holds no\\nmatter what the data distribution\\nlooks like nor how many examples\\nthere are. Write out the general case\\nto see that you will still arrive at an\\neven weighting after one iteration.?\\nFigure 13.3: perf comparison of depth\\nvs # boostOne of the major attractions of boosting is that it is perhaps easy\\nto design computationally efﬁcient weak learners. A very popular\\ntype of weak learner is a shal low decision tree: a decision tree with a\\nsmall depth limit. Figure 13.3shows test error rates for decision trees\\nof different maximum depths (the different curves) run for differing\\nnumbers of boosting iterations (the x-axis). As you can see, if you\\nare willing to boost for many iterations, very shallow trees are quite\\neffective.\\nIn fact, a very popular weak learner is a decision decision stump :\\na decision tree that can only ask onequestion. This may seem like a\\nsilly model (and, in fact, it is on it’s own), but when combined with\\nboosting, it becomes very effective. To understand why, suppose for\\na moment that our data consists only of binary features, so that any\\nquestion that a decision tree might ask is of the form “is feature 5\\non?” By concentrating on decision stumps, all weak functions must\\nhave the form f(x) =s(2xd\\x001), where s2f\\x06 1gand dindexes some\\nfeature.\\nWhy do the functions have this\\nform??Now, consider the ﬁnal form of a function learned by AdaBoost.\\nWe can expand it as follow, where we let fkdenote the single feature\\nselected by the kth decision stump and let skdenote its sign:\\nf(x) =sgn\"\\nå\\nkakf(k)(x)#\\n(13.1)\\n=sgn\"\\nå\\nkaksk(2xfk\\x001)#\\n(13.2)\\n=sgn\"\\nå\\nk2akskxfk\\x00å\\nkaksk#\\n(13.3)\\n=sgn[w\\x01x+b] (13.4)ensemble methods 169\\nAlgorithm 33Random Forest Train (D,depth ,K)\\n1:fork=1. . .Kdo\\n2:t(k) complete binary tree of depth depth with random feature splits\\n3: f(k) the function computed by t(k), with leaves ﬁlled in by D\\n4:end for\\n5:return f(ˆx) =sgn\\x02\\nåkf(k)(ˆx)\\x03\\n// Return voted classiﬁer\\nwhere wd=å\\nk:fk=d2akskand b=\\x00å\\nkaksk (13.5)\\nThus, when working with decision stumps, AdaBoost actually pro-\\nvides an algorithm for learning linearclassiﬁers ! In fact, this con-\\nnection has recently been strengthened: you can show that AdaBoost\\nprovides an algorithm for optimizing exponentialloss. (However,\\nthis connection is beyond the scope of this book.)\\nAs a further example, consider the case of boosting a linearclassi-\\nﬁer. In this case, if we let the kth weak classiﬁer be parameterized by\\nw(k)and b(k), the overall predictor will have the form:\\nf(x) =sgn\"\\nå\\nkaksgn\\x10\\nw(k)\\x01x+b(k)\\x11#\\n(13.6)\\nYou can notice that this is nothing but a two-layer neuralnetwork ,\\nwith K-many hidden units! Of course it’s not a classiﬁcally trained\\nneural network (once you learn w(k)you never go back and update\\nit), but the structure is identical.\\n13.3Random Ensembles\\nOne of the most computationally expensive aspects of ensembles of\\ndecision trees is training the decision trees. This is very fast for de-\\ncision stumps, but for deeper trees it can be prohibitively expensive.\\nThe expensive part is choosing the tree structure. Once the tree struc-\\nture is chosen, it is very cheap to ﬁll in the leaves (i.e., the predictions\\nof the trees) using the training data.\\nAn efﬁcient and surprisingly effective alternative is to use trees\\nwith ﬁxed structures and random features. Collections of trees are\\ncalled forests, and so classiﬁers built like this are called random\\nforests . The random forest training algorithm, shown in Algo-\\nrithm 13.3is quite short. It takes three arguments: the data, a desired\\ndepth of the decision trees, and a number Kof total decision trees to\\nbuild.\\nThe algorithm generates each of the Ktrees independently, which\\nmakes it very easy to parallelize. For each trees, it constructs a full\\nbinary tree of depth depth . The features used at the branches of this170 a course in machine learning\\ntree are selected randomly, typically with replacement , meaning that\\nthe same feature can appear multiple times, even in one branch. The\\nleaves of this tree, where predictions are made, are ﬁlled in based on\\nthe training data. This last step is the only point at which the training\\ndata is used. The resulting classiﬁer is then just a voting of the K-\\nmany random trees.\\nThe most amazing thing about this approach is that it actually\\nworks remarkably well. It tends to work best when all of the features\\nare at least marginally relevant, since the number of features selected\\nfor any given tree is small. An intuitive reason that it works well\\nis the following. Some of the trees will query on useless features.\\nThese trees will essentially make random predictions. But some\\nof the trees will happen to query on good features and will make\\ngood predictions (because the leaves are estimated based on the\\ntraining data). If you have enough trees, the random ones will wash\\nout as noise, and only the good trees will have an effect on the ﬁnal\\nclassiﬁcation.\\n13.4Further Reading\\nTODO further reading14 | E FFICIENT LEARNING\\nDependencies:So far ,our focus has been on models of learning and basic al-\\ngorithms for those models. We have not placed much emphasis on\\nhow to learn quickly . The basic techniques you learned about so far\\nare enough to get learning algorithms running on tens or hundreds\\nof thousands of examples. But if you want to build an algorithm for\\nweb page ranking, you will need to deal with millions or billions\\nof examples, in hundreds of thousands of dimensions. The basic\\napproaches you have seen so far are insufﬁcient to achieve such a\\nmassive scale.\\nIn this chapter, you will learn some techniques for scaling learning\\nalgorithms. This are useful even when you do not have billions of\\ntraining examples, because it’s always nice to have a program that\\nruns quickly. You will see techniques for speeding up both model\\ntraining and model prediction. The focus in this chapter is on linear\\nmodels (for simplicity), but most of what you will learn applies more\\ngenerally.\\n14.1What Does it Mean to be Fast?\\nEveryone always wants fast algorithms. In the context of machine\\nlearning, this can mean many things. You might want fast training\\nalgorithms, or perhaps training algorithms that scale to very large\\ndata sets (for instance, ones that will not ﬁt in main memory). You\\nmight want training algorithms that can be easily parallelized. Or,\\nyou might not care about training efﬁciency, since it is an ofﬂine\\nprocess, and only care about how quickly your learned functions can\\nmake classiﬁcation decisions.\\nIt is important to separate out these desires. If you care about\\nefﬁciency at training time, then what you are really asking for are\\nmore efﬁcient learning algorithms. On the other hand, if you care\\nabout efﬁciency at test time, then you are asking for models that can\\nbe quickly evaluated.\\nOne issue that is not covered in this chapter is parallel learning.Learning Objectives:\\n• Understand and be able to imple-\\nment stochastic gradient descent\\nalgorithms.\\n• Compare and contrast small ver-\\nsus large batch sizes in stochastic\\noptimization.\\n• Derive subgradients for sparse\\nregularizers.\\n• Implement feature hashing.One essential object is to choose that arrangement which shall\\ntend to reduce to a minimum the time necessary for completing\\nthe calculation. – Ada Lovelace172 a course in machine learning\\nThis is largely because it is currently not a well-understood area in\\nmachine learning. There are many aspects of parallelism that come\\ninto play, such as the speed of communication across the network,\\nwhether you have shared memory, etc. Right now, this the general,\\npoor-man’s approach to parallelization, is to employ ensembles.\\n14.2Stochastic Optimization\\nDuring training of most learning algorithms, you consider the entire\\ndata set simultaneously. This is certainly true of gradient descent\\nalgorithms for regularized linear classiﬁers (recall Algorithm 7.4), in\\nwhich you ﬁrst compute a gradient over the entire training data (for\\nsimplicity, consider the unbiased case):\\ng=å\\nnrw`(yn,w\\x01xn) +lw (14.1)\\nwhere `(y,ˆy)is some loss function. Then you update the weights by\\nw w\\x00hg. In this algorithm, in order to make a single update, you\\nhave to look at every training example.\\nWhen there are billions of training examples, it is a bit silly to look\\nat every one before doing anything. Perhaps just on the basis of the\\nﬁrst few examples, you can already start learning something!\\nStochastic optimization involves thinking of your training data\\nas a big distribution over examples. A draw from this distribution\\ncorresponds to picking some example (uniformly at random) from\\nyour data set. Viewed this way, the optimization problem becomes a\\nstochas ticoptimiza tion problem, because you are trying to optimize\\nsome function (say, a regularized linear classiﬁer) over a probability\\ndistribution. You can derive this intepretation directly as follows:\\nw\\x03=arg max\\nwå\\nn`(yn,w\\x01xn) +R(w) deﬁnition\\n(14.2)\\n=arg max\\nwå\\nn\\x14\\n`(yn,w\\x01xn) +1\\nNR(w)\\x15\\nmove Rinside sum\\n(14.3)\\n=arg max\\nwå\\nn\\x141\\nN`(yn,w\\x01xn) +1\\nN2R(w)\\x15\\ndivide through by N\\n(14.4)\\n=arg max\\nwE(y,x)\\x18D\\x14\\n`(y,w\\x01x) +1\\nNR(w)\\x15\\nwrite as expectation\\n(14.5)\\nwhere Dis the training data distribution ( 14.6)\\nGiven this framework, you have the following general form of anefficient learning 173\\nAlgorithm 34Stochastic Gradient Descent (F,D,S,K,h1, . . . )\\n1:z(0) h0,0, . . . , 0i // initialize variable we are optimizing\\n2:fork=1. . .Kdo\\n3:D(k) S-many random data points from D\\n4:g(k) r zF(D(k))\\x0c\\x0c\\nz(k-1) // compute gradient on sample\\n5:z(k) z(k-1)\\x00h(k)g(k)// take a step down the gradient\\n6:end for\\n7:return z(K)\\noptimization problem:\\nminzEz[F(z,z)] (14.7)\\nIn the example, zdenotes the random choice of examples over the\\ndataset, zdenotes the weight vector and F(w,z)denotes the loss on\\nthat example plus a fraction of the regularizer.\\nStochastic optimization problems are formally harder than regu-\\nlar (deterministic) optimization problems because you do not even\\nget access to exact function values and gradients. The only access\\nyou have to the function Fthat you wish to optimize are noisy mea-\\nsurements, governed by the distribution over z. Despite this lack of\\ninformation, you can still run a gradient-based algorithm, where you\\nsimply compute local gradients on a current sample of data.\\nMore precisely, you can draw a data point at random from your\\ndata set. This is analogous to drawing a single value zfrom its\\ndistribution. You can compute the gradient of Fjust at that point.\\nIn this case of a 2-norm regularized linear model, this is simply\\ng=rw`(y,w\\x01x) +1\\nNw, where (y,x)is the random point you\\nselected. Given this estimate of the gradient (it’s an estimate because\\nit’s based on a single random draw), you can take a small gradient\\nstep w w\\x00hg.\\nThis is the stochas ticgradientdescent algorithm ( SGD ). In prac-\\ntice, taking gradients with respect to a single data point might be\\ntoo myopic. In such cases, it is useful to use a small batch of data.\\nHere, you can draw 10 random examples from the training data\\nand compute a small gradient (estimate) based on those examples:\\ng=å10\\nm=1rw`(ym,w\\x01xm) +10\\nNw, where you need to include 10\\ncounts of the regularizer. Popular batch sizes are 1 (single points)\\nand 10. The generic SGD algorithm is depicted in Algorithm 14.2,\\nwhich takes K-many steps over batches of S-many examples.\\nIn stochastic gradient descent, it is imperative to choose good step\\nsizes. It is also very important that the steps get smaller over time at\\na reasonable slow rate. In particular, convergence can be guaranteed\\nfor learning rates of the form: h(k)=h0p\\nk, where h0is a ﬁxed, initial\\nstep size, typically 0.01, 0.1 or 1 depending on how quickly you ex-174 a course in machine learning\\npect the algorithm to converge. Unfortunately, in comparisong to\\ngradient descent, stochastic gradient is quite sensitive to the selection\\nof a good learning rate.\\nThere is one more practical issues related to the use of SGD as a\\nlearning algorithm: do you really select a random point (or subset\\nof random points) at each step, or do you stream through the data\\nin order. The answer is akin to the answer of the same question for\\nthe perceptron algorithm (Chapter 4). If you do not permute your\\ndata at all, very bad things can happen. If you dopermute your data\\nonce and then do multiple passes over that same permutation, it\\nwill converge, but more slowly. In theory, you really should permute\\nevery iteration. If your data is small enough to ﬁt in memory, this\\nis not a big deal: you will only pay for cache misses. However, if\\nyour data is too large for memory and resides on a magnetic disk\\nthat has a slow seek time, randomly seeking to new data points for\\neach example is prohibitivly slow, and you will likely need to forgo\\npermuting the data. The speed hit in convergence speed will almost\\ncertainly be recovered by the speed gain in not having to seek on disk\\nroutinely. (Note that the story is very different for solid state disks,\\non which random accesses really are quite efﬁcient.)\\n14.3Sparse Regularization\\nFor many learning algorithms, the test-time efﬁciency is governed\\nby how many features are used for prediction. This is one reason de-\\ncision trees tend to be among the fastest predictors: they only use a\\nsmall number of features. Especially in cases where the actual com-\\nputation of these features is expensive, cutting down on the number\\nthat are used at test time can yield huge gains in efﬁciency. Moreover,\\nthe amount of memory used to make predictions is also typically\\ngoverned by the number of features. (Note: this is nottrue of kernel\\nmethods like support vector machines, in which the dominant cost is\\nthe number of support vectors.) Furthermore, you may simply believe\\nthat your learning problem can be solved with a very small number\\nof features: this is a very reasonable form of inductive bias.\\nThis is the idea behind sparse models, and in particular, sparse\\nregularizers. One of the disadvantages of a 2-norm regularizer for\\nlinear models is that they tend to never produce weights that are\\nexactly zero. They get close to zero, but never hit it. To understand\\nwhy, as a weight wdapproaches zero, its gradient alsoapproaches\\nzero. Thus, even if the weight should be zero, it will essentially never\\nget there because of the constantly shrinking gradient.\\nThis suggests that an alternative regularizer is required to yield a\\nsparse inductive bias. An ideal case would be the zero-norm regular-efficient learning 175\\nizer, which simply counts the number of non-zero values in a vector:\\njjwjj0=åd[wd6=0]. If you could minimize this regularizer, you\\nwould be explicitly minimizing the number of non-zero features. Un-\\nfortunately, not only is the zero-norm non-convex, it’s also discrete.\\nOptimizing it is NP-hard.\\nA reasonable middle-ground is the one-norm: jjwjj1=ådjwdj.\\nIt is indeed convex: in fact, it is the tighest `pnorm that isconvex.\\nMoreover, its gradients do not go to zero as in the two-norm. Just as\\nhinge-loss is the tightest convex upper bound on zero-one error, the\\none-norm is the tighest convex upper bound on the zero-norm.\\nAt this point, you should be content. You can take your subgradi-\\nent optimizer for arbitrary functions and plug in the one-norm as a\\nregularizer. The one-norm is surely non-differentiable at wd=0, but\\nyou can simply choose any value in the range [\\x001,+1]as a subgradi-\\nent at that point. (You should choose zero.)\\nUnfortunately, this does not quite work the way you might expect.\\nThe issue is that the gradient might “overstep” zero and you will\\nnever end up with a solution that is particularly sparse. For example,\\nat the end of one gradient step, you might have w3=0.6. Your\\ngradient might have g6=0.8 and your gradient step (assuming\\nh=1) will update so that the new w3=\\x000.2. In the subsequent\\niteration, you might have g6=\\x000.3 and step to w3=0.1.\\nThis observation leads to the idea of trucated gradients . The idea\\nis simple: if you have a gradient that would step you over wd=0,\\nthen just set wd=0. In the easy case when the learning rate is 1, this\\nmeans that if the sign ofwd\\x00gdis different than the sign of wdthen\\nyou truncate the gradient step and simply set wd=0. In other words,\\ngdshould never be larger than wdOnce you incorporate learning\\nrates, you can express this as:\\ngd 8\\n><\\n>:gdifwd>0 and gd\\x141\\nh(k)wd\\ngdifwd<0 and gd\\x151\\nh(k)wd\\n0 otherwise(14.8)\\nThis works quite well in the case of subgradient descent. It works\\nsomewhat less well in the case of stochastic subgradient descent. The\\nproblem that arises in the stochastic case is that wherever you choose\\nto stop optimizing, you will have just touched a single example (or\\nsmall batch of examples), which will increase the weights for a lot of\\nfeatures, before the regularizer “has time” to shrink them back down\\nto zero. You will still end up with somewhat sparse solutions, but not\\nas sparse as they could be. There are algorithms for dealing with this\\nsituation, but they all have a heuristic ﬂavor to them and are beyond\\nthe scope of this book.176 a course in machine learning\\n14.4Feature Hashing\\nAs much as speed is a bottleneck in prediction, so often is memory\\nusage. If you have a very large number of features, the amount of\\nmemory that it takes to store weights for all of them can become\\nprohibitive, especially if you wish to run your algorithm on small de-\\nvices. Feature hashing is an incredibly simple technique for reducing\\nthe memory footprint of linear models, with very small sacriﬁces in\\naccuracy.\\nThe basic idea is to replace all of your features with hashed ver-\\nsions of those features, thus reducing your space from D-many fea-\\nture weights to P-many feature weights, where Pis the range of\\nthe hash function. You can actually think of hashing as a (random-\\nized) feature mapping f:RD!RP, for some P\\x1cD. The idea\\nis as follows. First, you choose a hash function hwhose domain is\\n[D] =f1, 2, . . . , Dgand whose range is [P]. Then, when you receive a\\nfeature vector x2RD, you map it to a shorter feature vector ˆ x2RP.\\nAlgorithmically, you can think of this mapping as follows:\\n1. Initialize ˆ x=h0, 0, . . . , 0i\\n2. For each d=1 . . . D:\\n(a) Hash dto position p=h(d)\\n(b) Update the pth position by adding xd:ˆxp ˆxp+xd\\n3. Return ˆ x\\nMathematically, the mapping looks like:\\nf(x)p=å\\nd[h(d) =p]xd=å\\nd2h\\x001(p)xd (14.9)\\nwhere h\\x001(p) =fd:h(d) =pg.\\nIn the (unrealistic) case where P=Dand hsimply encodes a per-\\nmutation, then this mapping does not change the learning problem\\nat all. All it does is rename all of the features. In practice, P\\x1cD\\nand there will be collisions. In this context, a collision means that\\ntwo features, which are really different, end up looking the same to\\nthe learning algorithm. For instance, “is it sunny today?” and “did\\nmy favorite sports team win last night?” might get mapped to the\\nsame location after hashing. The hope is that the learning algorithm\\nis sufﬁciently robust to noise that it can handle this case well.efficient learning 177\\nConsider the kernel deﬁned by this hash mapping. Namely:\\nK(hash)(x,z) =f(x)\\x01f(z) (14.10)\\n=å\\np \\nå\\nd[h(d) =p]xd! \\nå\\nd[h(d) =p]zd!\\n(14.11)\\n=å\\npå\\nd,e[h(d) =p][h(e) =p]xdze (14.12)\\n=å\\ndå\\ne2h\\x001(h(d))xdze (14.13)\\n=x\\x01z+å\\ndå\\ne6=d,\\ne2h\\x001(h(d))xdze (14.14)\\nThis hash kernelhas the form of a linear kernel plus a small number\\nof quadratic terms. The particular quadratic terms are exactly those\\ngiven by collisions of the hash function.\\nThere are two things to notice about this. The ﬁrst is that collisions\\nmight not actually be bad things! In a sense, they’re giving you a\\nlittle extra representational power. In particular, if the hash function\\nhappens to select out feature pairs that beneﬁt from being paired,\\nthen you now have a better representation. The second is that even if\\nthis doesn’t happen, the quadratic term in the kernel has only a small\\neffect on the overall prediction. In particular, if you assume that your\\nhash function is pairwise independent (a common assumption of\\nhash functions), then the expected value of this quadratic term is zero,\\nand its variance decreases at a rate of O(P\\x002). In other words, if you\\nchoose P\\x19100, then the variance is on the order of 0.0001.\\n14.5Further Reading\\nTODO further reading15 | U NSUPERVISED LEARNING\\nDependencies:If you have access to labeled training data , you know what\\nto do. This is the “supervised” setting, in which you have a teacher\\ntelling you the right answers. Unfortunately, ﬁnding such a teacher\\nis often difﬁcult, expensive, or down right impossible. In those cases,\\nyou might still want to be able to analyze your data, even though you\\ndo not have labels.\\nUnsupervised learning is learning without a teacher. One basic\\nthing that you might want to do with data is to visualizeit. Sadly, it\\nis difﬁcult to visualize things in more than two (or three) dimensions,\\nand most data is in hundreds of dimensions (or more). Dimen sion -\\nalityreduction is the problem of taking high dimensional data and\\nembedding it in a lower dimension space. Another thing you might\\nwant to do is automatically derive a partitioning of the data into\\nclusters. You’ve already learned a basic approach for doing this: the\\nk-means algorithm (Chapter 3). Here you will analyze this algorithm\\nto see why it works. You will also learn more advanced clustering\\napproaches.\\n15.1K-Means Clustering, Revisited\\nThe K-means clustering algorithm is re-presented in Algorithm 15.1.\\nThere are two very basic questions about this algorithm: ( 1) does it\\nconverge (and if so, how quickly); ( 2) how sensitive it is to initializa-\\ntion? The answers to these questions, detailed below, are: ( 1) yes it\\nconverges, and it converges very quickly in practice (though slowly\\nin theory); ( 2) yes it is sensitive to initialization, but there are good\\nways to initialize it.\\nConsider the question of convergence. The following theorem\\nstates that the K-Means algorithm converges, though it does not say\\nhow quickly it happens. The method of proving the convergence is\\nto specify a clusteringqual ityobjective function, and then to show\\nthat the K-Means algorithm converges to a (local) optimum of that\\nobjective function. The particular objective function that K-MeansLearning Objectives:\\n• Explain the difference between\\nlinear and non-linear dimensionality\\nreduction.\\n• Relate the view of PCA as maximiz-\\ning variance with the view of it as\\nminimizing reconstruction error.\\n• Implement latent semantic analysis\\nfor text data.\\n• Motivate manifold learning from the\\nperspective of reconstruction error.\\n• Understand K-means clustering as\\ndistance minimization.\\n• Explain the importance of initial-\\nization in k-means and furthest-ﬁrst\\nheuristic.\\n• Implement agglomerative clustering.\\n• Argue whether spectral cluster-\\ning is a clustering algorithm or a\\ndimensionality reduction algorithm.[My father] advised me to sit every few months in my reading\\nchair for an entire evening, close my eyes and try to think of new\\nproblems to solve. I took his advice very seriously and have been\\nglad ever since that he did. – Luis Walter Alvarezunsupervised learning 179\\nAlgorithm 35K-Means (D,K)\\n1:fork=1toKdo\\n2:mk some random location // randomly initialize mean for kth cluster\\n3:end for\\n4:repeat\\n5:forn=1toNdo\\n6: zn argminkjjmk\\x00xnjj // assign example nto closest center\\n7:end for\\n8:fork=1toKdo\\n9: mk mean (fxn:zn=kg) // re-estimate mean of cluster k\\n10:end for\\n11:until converged\\n12:return z // return cluster assignments\\nis optimizing is the sum of squared distances from any data point to its\\nassigned center. This is a natural generalization of the deﬁnition of a\\nmean: the mean of a set of points is the single point that minimizes\\nthe sum of squared distances from the mean to every point in the\\ndata. Formally, the K-Means objective is:\\nL(z,m;D) =å\\nn\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cxn\\x00mzn\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n=å\\nkå\\nn:zn=kjjxn\\x00mkjj2(15.1)\\nTheorem 16(K-Means Convergence Theorem) .For any dataset Dand\\nany number of clusters K, the K-means algorithm converges in a ﬁnite num-\\nber of iterations, where convergence is measured by Lceasing the change.\\nProof of Theorem 16.The proof works as follows. There are only two\\npoints in which the K-means algorithm changes the values of morz:\\nlines 6and9. We will show that both of these operations can never\\nincrease the value of L. Assuming this is true, the rest of the argu-\\nment is as follows. After the ﬁrst pass through the data, there are\\nare only ﬁnitely many possible assignments to zandm, because zis\\ndiscrete and because mcan only take on a ﬁnite number of values:\\nmeans of some subset of the data. Furthermore, Lis lower-bounded\\nby zero. Together, this means that Lcannot decrease more than a\\nﬁnite number of times. Thus, it must stop decreasing at some point,\\nand at that point the algorithm has converged.\\nIt remains to show that lines 6and9decreaseL. For line 6, when\\nlooking at example n, suppose that the previous value of znisaand\\nthe new value is b. It must be the case that jjxn\\x00mbjj\\x14jjxn\\x00mbjj.\\nThus, changing from atobcan only decrease L. For line 9, consider\\nthe second form of L. Line 9computes mkas the mean of the data\\npoints for which zn=k, which is precisely the point that minimizes\\nsquared sitances. Thus, this update to mkcan only decrease L.\\nThere are several aspects of K-means that are unfortunate. First,\\nthe convergence is only to a local optimum of L. In practice, this180 a course in machine learning\\nmeans that you should usually run it 10 times with different initial-\\nizations and pick the one with minimal resulting L. Second, one\\ncan show that there are input datasets and initializations on which\\nit might take an exponential amount of time to converge. Fortu-\\nnately, these cases almost never happen in practice, and in fact it has\\nrecently been shown that (roughly) if you limit the ﬂoating point pre-\\ncision of your machine, K-means willconverge in polynomial time\\n(though still only to a local optimum), using techniques of smoothed\\nanal ysis.\\nThe biggest practical issue in K-means is initialization. If the clus-\\nter means are initialized poorly, you often get convergence to uninter-\\nesting solutions. A useful heuristic is the furthest -ﬁrst heuris tic. This\\ngives a way to perform a semi-random initialization that attempts to\\npick initial means as far from each other as possible. The heuristic is\\nsketched below:\\n1. Pick a random example mand set m1=xm.\\n2. For k=2 . . . K:\\n(a) Find the example mthat is as far as possible from allprevi-\\nously selected means; namely: m=arg max mmin k0<kjjxm\\x00mk0jj2\\nand set mk=xm\\nIn this heuristic, the only bit of randomness is the selection of the\\nﬁrst data point. After that, it is completely deterministic (except in\\nthe rare case that there are multiple equidistant points in step 2a). It\\nis extremely important that when selecting the 3rd mean, you select\\nthat point that maximizes theminimum distance to the closest other\\nmean. You want the point that’s as far away from allprevious means\\nas possible.\\nThe furthest-ﬁrst heuristic is just that: a heuristic. It works very\\nwell in practice, though can be somewhat sensitive to outliers (which\\nwill often get selected as some of the initial means). However, this\\noutlier sensitivity is usually reduced after one iteration through the\\nK-means algorithm. Despite being just a heuristic, it is quite useful in\\npractice.\\nYou can turn the heuristic into an algorithm by adding a bit more\\nrandomness. This is the idea of the K-means ++algorithm, which\\nis a simple randomized tweak on the furthest-ﬁrst heuristic. The\\nidea is that when you select the kth mean, instead of choosing the\\nabsolute furthest data point, you choose a data point at random, with\\nprobability proportional to its distance squared. This is made formal\\nin Algorithm 15.1.\\nIf you use K-means ++as an initialization for K-means, then you\\nare able to achieve an approximation guarantee on the ﬁnal valueunsupervised learning 181\\nAlgorithm 36K-Means ++(D,K)\\n1:m1 xmformchosen uniformly at random // randomly initialize ﬁrst point\\n2:fork=2toKdo\\n3:dn min k0<kjjxn\\x00mk0jj2,8n // compute distances\\n4:p 1\\nånndd // normalize to probability distribution\\n5:m random sample from p // pick an example at random\\n6:mk xm\\n7:end for\\n8:run K-M eans using mas initial centers\\nof the objective. This doesn’t tell you that you will reach the global\\noptimum, but it does tell you that you will get reasonably close. In\\nparticular, if ˆLis the value obtained by running K-means ++, then this\\nwill not be “too far” from L(opt), the true global minimum.\\nTheorem 17(K-means ++Approximation Guarantee) .The expected\\nvalue of the objective returned by K-means ++is never more than O(logK)\\nfrom optimal and can be as close as O(1)from optimal. Even in the former\\ncase, with 2Krandom restarts, one restart will be O(1)from optimal (with\\nhigh probability). Formally: E\\x02ˆL\\x03\\x148(logK+2)L(opt). Moreover, if the\\ndata is “well suited” for clustering, then E\\x02ˆL\\x03\\x14O(1)L(opt).\\nThe notion of “well suited” for clustering informally states that\\nthe advantage of going from K\\x001 clusters to Kclusters is “large.”\\nFormally, it means that LK(opt)\\x14e2LK\\x001(opt), whereLK(opt)is the\\noptimal value for clustering with Kclusters, and eis the desired\\ndegree of approximation. The idea is that if this condition does not\\nhold, then you shouldn’t bother clustering the data.\\nOne of the biggest practical issues with K-means clustering is\\n“choosing K.” Namely, if someone just hands you a dataset and\\nasks you to cluster it, how many clusters should you produce? This\\nis difﬁcult, because increasing Kwill always decrease LK(opt)(until\\nK>N), and so simply using Las a notion of goodness is insufﬁ-\\ncient (analogous to overﬁtting in a supervised setting). A number\\nof “information criteria” have been proposed to try to address this\\nproblem. They all effectively boil down to “regularizing” Kso that\\nthe model cannot grow to be too complicated. The two most popular\\nare the Bayes Information Criteria (BIC) and the Akaike Information\\nCriteria (AIC), deﬁned below in the context of K-means:\\nBIC: arg min\\nKˆLK+KlogD (15.2)\\nAIC: arg min\\nKˆLK+2KD (15.3)\\nThe informal intuition behind these criteria is that increasing Kis\\ngoing to makeLKgo down. However, if it doesn’t go down “by\\nenough” then it’s not worth doing. In the case of BIC, “by enough”182 a course in machine learning\\nmeans by an amount proportional to log D; in the case of AIC, it’s\\nproportional to 2 D. Thus, AIC provides a much stronger penalty for\\nmany clusters than does BIC, especially in high dimensions.\\nA more formal intuition for BIC is the following. You ask yourself\\nthe question “if I wanted to send this data across a network, how\\nmany bits would I need to send?” Clearly you could simply send\\nall of the Nexamples, each of which would take roughly log Dbits\\nto send. This gives NlogDto send all the data. Alternatively, you\\ncould ﬁrst cluster the data and send the cluster centers. This will take\\nKlogDbits. Then, for each data point, you send its center as well as\\nits deviation from that center. It turns out this will cost exactly ˆLK\\nbits. Therefore, the BIC is precisely measuring how many bits it will\\ntake to send your data using Kclusters. The Kthat minimizes this\\nnumber of bits is the optimal value.\\n15.2Linear Dimensionality Reduction\\nDimensionality reduction is the task of taking a dataset in high di-\\nmensions (say 10000) and reducing it to low dimensions (say 2) while\\nretaining the “important” characteristics of the data. Since this is\\nan unsupervised setting, the notion of important characteristics is\\ndifﬁcult to deﬁne.\\nFigure 15.1:unsup:pcadata : Data in\\ntwo dimensions\\nFigure 15.2:unsup:pcadata2 : Projection\\nof that data down to one dimension by\\nPCAConsider the dataset in Figure 15.1, which lives in high dimensions\\n(two) and you want to reduce to low dimensions (one). In the case\\nof linear dimensionality reduction, the only thing you can do is to\\nproject the data onto a vector and use the projected distances as the\\nembeddings. Figure 15.2shows a projection of this data onto the\\nvector that points in the direction of maximal variance of the original\\ndataset. Intuitively, this is a reasonable notion of importance, since\\nthis is the direction in which most information is encoded in the data.\\nFor the rest of this section, assume that the data is centered:\\nnamely, the mean of all the data is at the origin. (This will sim-\\nply make the math easier.) Suppose the two dimensional data is\\nx1, . . . , xNand you’re looking for a vector uthat points in the direc-\\ntion of maximal variance. You can compute this by projecting each\\npoint onto uand looking at the variance of the result. In order for\\nthe projection to make sense, you need to constrain jjujj2=1. In\\nthis case, the projections are x1\\x01u,x2\\x01u, . . . , xN\\x01u. Call these values\\np1, . . . , pN.\\nThe goal is to compute the variance of the fpngs and then choose\\nuto maximize this variance. To compute the variance, you ﬁrst need\\nto compute the mean. Because the mean of the xns was zero, theunsupervised learning 183\\ntodo the usual...MATHREVIEW | EIGENVALUES AND EIGENVECTORS\\nFigure 15.3:\\nmean of the ps is also zero. This can be seen as follows:\\nå\\nnpn=å\\nnxn\\x01u= \\nå\\nnxn!\\n\\x01u=0\\x01u=0 (15.4)\\nThe variance of the fpngis then just ånp2\\nn. Finding the optimal\\nu(from the perspective of variance maximization) reduces to the\\nfollowing optimization problem:\\nmaxuå\\nn(xn\\x01u)2subj. tojjujj2=1 ( 15.5)\\nIn this problem it becomes apparent why keeping uunit length is\\nimportant: if not, uwould simply stretch to have inﬁnite length to\\nmaximize the objective.\\nIt is now helpful to write the collection of datapoints xnas a N\\x02\\nDmatrix X. If you take this matrix Xand multiply it by u, which\\nhas dimensions D\\x021, you end up with a N\\x021 vector whose values\\nare exactly the values p. The objective in Eq ( 15.5) is then just the\\nsquared norm of p. This simpliﬁes Eq ( 15.5) to:\\nmaxujjXujj2subj. tojjujj2\\x001=0 ( 15.6)\\nwhere the constraint has been rewritten to make it amenable to con-\\nstructing the Lagrangian. Doing so and taking gradients yields:\\nL(u,l) =jjXujj2\\x00l\\x10\\njjujj2\\x001\\x11\\n(15.7)\\nruL=2X>Xu\\x002lu (15.8)\\n=) lu=\\x10\\nX>X\\x11\\nu (15.9)\\nYou can solve this expression ( lu=X>Xu) by computing the ﬁrst\\neigenvector and eigenvalue of the matrix X>X.\\nThis gives you the solution to a projection into a one-dimensional\\nspace. To get a second dimension, you want to ﬁnd a new vector von\\nwhich the data has maximal variance. However, to avoid redundancy,\\nyou want vto be orthogonal to u; namely u\\x01v=0. This gives:\\nmaxvjjXvjj2subj. tojjvjj2=1, and u\\x01v=0 ( 15.10)\\nFollowing the same procedure as before, you can construct a La-184 a course in machine learning\\nAlgorithm 37PCA( D,K)\\n1:m mean (X) // compute data mean for centering\\n2:D \\x10\\nX\\x00m1>\\x11\\n>\\x10\\nX\\x00m1>\\x11\\n// compute covariance, 1is a vector of ones\\n3:flk,ukg topKeigenvalues/eigenvectors of D\\n4:return (X\\x00m1)U // project data using U\\ngrangian and differentiate:\\nL(v,l1,l2) =jjXvjj2\\x00l1\\x10\\njjvjj2\\x001\\x11\\n\\x00l2u\\x01v (15.11)\\nruL=2X>Xv\\x002l1v\\x00l2u (15.12)\\n=) l1v=\\x10\\nX>X\\x11\\nv\\x00l2\\n2u (15.13)\\nHowever, you know that uis the ﬁrst eigenvector of X>X, so the\\nsolution to this problem for l1and vis given by the second eigen-\\nvalue/eigenvector pair of X>X.\\nRepeating this analysis inductively tells you that if you want to\\nproject onto Kmutually orthogonal dimensions, you simply need to\\ntake the ﬁrst Keigenvectors of the matrix X>X. This matrix is often\\ncalled the data covariance matrixbecause [X>X]i,j=ånåmxn,ixm,j,\\nwhich is the sample covariance between features iand j.\\nThis leads to the technique of prin ciplecom ponents anal ysis,\\norPCA . For completeness, the is depicted in Algorithm 15.2. The\\nimportant thing to note is that the eigenanalysis only gives you the\\nprojection directions. It does not give you the embedded data. To\\nembed a data point xyou need to compute its embedding as hx\\x01\\nu1,x\\x01u2, . . . , x\\x01uKi. If you write Ufor the D\\x02Kmatrix of us, then this\\nis just XU.\\nThere is an alternative derivation of PCA that can be informative,\\nbased on reconstruction error. Consider the one-dimensional case\\nagain, where you are looking for a single projection direction u. If\\nyou were to use this direction, your projected data would be Z=Xu.\\nEach Zngives the position of the nth datapoint along u. You can\\nproject this one-dimensional data back into the original space by\\nmultiplying it by u>. This gives you reconstructed values Zu>. Instead\\nof maximizing variance, you might instead want to minimize the\\nreconstruc tion error, deﬁned by:\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cX\\x00Zu>\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n=\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cX\\x00Xuu>\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\ndeﬁnition of Z\\n(15.14)\\n=jjXjj2+\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cXuu>\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n\\x002X>Xuu>quadratic rule\\n(15.15)unsupervised learning 185\\n=jjXjj2+\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cXuu>\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\n\\x002u>X>Xu quadratic rule\\n(15.16)\\n=jjXjj2+jjXjj2\\x002u>X>Xu u is a unit vector\\n(15.17)\\n=C\\x002jjXujj2join constants, rewrite last term\\n(15.18)\\nMinimizing this ﬁnal term is equivalent to maximizing jjXujj2, which\\nis exactly the form of the maximum variance derivation of PCA.\\nThus, you can see that maximizing variance is identical to minimiz-\\ning reconstruction error.\\nThe same question of “what should Kbe” arises in dimension-\\nality reduction as in clustering. If the purpose of dimensionality\\nreduction is to visualize, then Kshould be 2 or 3. However, an alter-\\nnative purpose of dimensionality reduction is to avoid the curse of\\ndimensionality. For instance, even if you have labeled data, it might\\nbe worthwhile to reduce the dimensionality before applying super-\\nvised learning, essentially as a form of regularization. In this case,\\nthe question of an optimal Kcomes up again. In this case, the same\\ncriteria (AIC and BIC) that can be used for clustering can be used for\\nPCA. The only difference is the quality measure changes from a sum\\nof squared distances to means (for clustering) to a sum of squared\\ndistances to original data points (for PCA). In particular, for BIC you\\nget the reconstruction error plus KlogD; for AIC, you get the recon-\\nstruction error plus 2 KD.\\n15.3Autoencoders\\nTODO\\n15.4Further Reading\\nTODO16 | E XPECTATION MAXIMIZATION\\nDependencies:Suppose you were building a naive Bayes model for a text cate-\\ngorization problem. After you were done, your boss told you that it\\nbecame prohibitively expensive to obtain labeled data. You now have\\na probabilistic model that assumes access to labels, but you don’t\\nhave any labels! Can you still do something?\\nAmazingly, you can. You can treat the labels as hidden variables ,\\nand attempt to learn them at the same time as you learn the param-\\neters of your model. A very broad family of algorithms for solving\\nproblems just like this is the expectation max imiza tion family. In this\\nchapter, you will derive expectation maximization (EM) algorithms\\nfor clustering and dimensionality reduction, and then see why EM\\nworks.\\n16.1Grading an Exam without an Answer Key\\nAlice’s machine learning professor Carlos gives out an exam that\\nconsists of 50true/false questions. Alice’s class of 100students takes\\nthe exam and Carlos goes to grade their solutions. If Carlos made\\nan answer key, this would be easy: he would just count the fraction\\nof correctly answered questions each student got, and that would be\\ntheir score. But, like many professors, Carlos was really busy and\\ndidn’t have time to make an answer key. Can he still grade the exam?\\nThere are two insights that suggest that he might be able to. Sup-\\npose he know ahead of time that Alice was an awesome student, and\\nis basically guaranteed to get 100% on the exam. In that case, Carlos\\ncan simply use Alice’s answers as the ground truth. More generally,\\nif Carlos assumes that on average students are better than random\\nguessing, he can hope that the majority answer for each question is\\nlikely to be correct. Combining this with the previous insight, when\\ndoing the “voting”, he might want to pay more attention to the an-\\nswers of the better students.\\nTo be a bit more pedantic, suppose there are N=100 students\\nand M=50 questions. Each student nhas a score sn, between 0 andLearning Objectives:\\n• Explain the relationship between\\nparameters and hidden variables.\\n• Construct generative stories for\\nclustering and dimensionality\\nreduction.\\n• Draw a graph explaining how EM\\nworks by constructing convex lower\\nbounds.\\n• Implement EM for clustering with\\nmixtures of Gaussians, and contrast-\\ning it with k-means.\\n• Evaluate the differences betweem\\nEM and gradient descent for hidden\\nvariable models.A hen is only an egg’s way of making another egg. – Samuel Butlerexpectation maximization 187\\n1 that denotes how well they do on the exam. The score is what we\\nreally want to compute. For each question mand each student n, the\\nstudent has provided an answer an,m, which is either zero or one.\\nThere is also an unknown ground truth answer for each question m,\\nwhich we’ll call tm, which is also either zero or one.\\nAs a starting point, let’s consider a simple heuristic and then com-\\nplexify it. The heuristic is the “majority vote” heuristic and works as\\nfollows. First, we estimate tmas the most common answer for ques-\\ntion m:tm=argmaxtån1[an,m=t]. Once we have a guess for each\\ntrue answer, we estimate each students’ score as how many answers\\nthey produced that match this guessed key: sn=1\\nMåm1[an,m=tm].\\nOnce we have these scores, however, we might want to trust some\\nof the students more than others. In particular, answers from stu-\\ndents with high scores are perhaps more likely to be correct, so we\\ncanrecompute the ground truth, according to weighted votes. The\\nweight of the votes will be precisely the score the corresponding each\\nstudent:\\ntm=argmax\\ntå\\nnsn1[an,m=t] (16.1)\\nYou can recognize this as a chicken and egg problem . If you knew the\\nstudent’s scores, you could estimate an answer key. If you had an\\nanswer key, you could compute student scores. A very common\\nstrategy in computer science for dealing with such chicken and egg\\nproblems is to iterate. Take a guess at the ﬁrst, compute the second,\\nrecompute the ﬁrst, and so on.\\nIn order to develop this idea formally, we have to case the prob-\\nlem in terms of a probabilistic model with a generative story. The\\ngenerative story we’ll use is:\\n1. For each question m, choose a true answer tm\\x18Ber(0.5)\\n2. For each student n, choose a score sn\\x18Uni(0, 1)\\n3. For each question mand each student n, choose an answer\\nan,m\\x18Ber(sn)tmBer(1\\x00sn)1\\x00tm\\nIn the ﬁrst step, we generate the true answers independently by\\nﬂipping a fair coin. In the second step, each students’ overall score\\nis determined to be a uniform random number between zero and\\none. The tricky step is step three, where each students’ answer is\\ngenerated for each question. Consider student nanswering question\\nm, and suppose that sn=0.9. If tm=1, then an,mshould be 1(i.e.,\\ncorrect) 90% of the time; this can be accomplished by drawing the an-\\nswer fromBer(0.9). On the other hand, if tm=0, then an,mshould 1\\n(i.e., incorrect) 10% of the time; this can be accomplished by drawing188 a course in machine learning\\nthe answer fromBer(0.1). The exponent in step 3selects which of two\\nBernoulli distributions to draw from, and then implements this rule.\\nThis can be translated into the following likelihood:\\np(a,t,s)\\n=\"\\nÕ\\nm0.5tm0.51\\x00tm#\\n\\x02\"\\nÕ\\nn1#\\n\\x02\"\\nÕ\\nnÕ\\nmsan,mtm\\nn(1\\x00sn)(1\\x00an,m)tm\\ns(1\\x00an,m)(1\\x00tm)\\nn (1\\x00sn)an,m(1\\x00tm)i\\n(16.2)\\n=0.5MÕ\\nnÕ\\nmsan,mtm\\nn(1\\x00sn)(1\\x00an,m)tms(1\\x00an,m)(1\\x00tm)\\nn (1\\x00sn)an,m(1\\x00tm)\\n(16.3)\\nSuppose we knew the true lables t. We can take the log of this\\nlikelihood and differentiate it with respect to the score snof some\\nstudent (note: we can drop the 0.5Mterm because it is just a con-\\nstant):\\nlogp(a,t,s) =å\\nnå\\nmh\\nan,mtmlogsn+ (1\\x00an,m)(1\\x00tm)log(sn)\\n+ (1\\x00an,m)tmlog(1\\x00sn) +an,m(1\\x00tm)log(1\\x00sn)i\\n(16.4)\\n¶logp(a,t,s)\\n¶sn=å\\nmhan,mtm+ (1\\x00an,m)(1\\x00tm)\\nsn\\x00(1\\x00an,m)tm+an,m(1\\x00tm)\\n1\\x00sni\\n(16.5)\\nThe derivative has the formA\\nsn\\x00B\\n1\\x00sn. If we set this equal to zero and\\nsolve for sn, we get an optimum of sn=A\\nA+B. In this case:\\nA=å\\nm\\x02\\nan,mtm+ (1\\x00an,m)(1\\x00tm)\\x03\\n(16.6)\\nB=å\\nm\\x02(1\\x00an,m)tm+an,m(1\\x00tm)\\x03\\n(16.7)\\nA+B=å\\nm\\x02\\n1\\x03=M (16.8)\\nPutting this together, we get:\\nsn=1\\nMå\\nm\\x02\\nan,mtm+ (1\\x00an,m)(1\\x00tm)\\x03\\n(16.9)\\nIn the case of known ts, this matches exactly what we had in the\\nheuristic.\\nHowever, we do not know t, so instead of using the “true” val-\\nues of t, we’re going to use their expectations . In particular, we will\\ncompute snby maximizing its likelihood under the expected valuesexpectation maximization 189\\noft, hence the name expectation max imiza tion. If we are going\\nto compute expectations of t, we have to say: expectations accord-\\ning to which probability distribution? We will use the distribution\\np(tmja,s). Let ˜tmdenote Etm\\x18p(tmja,s)[tm]. Because tmis a bi-\\nnary variable, its expectation is equal to it’s probability; namely:\\n˜tm=p(tmja,s).\\nHow can we compute this? We will compute C=p(tm=1,a,s)\\nand D=p(tm=0,a,s)and then compute ˜tm=C/(C+D). The\\ncomputation is straightforward:\\nC=0.5Õ\\nnsan,m\\nn(1\\x00sn)1\\x00an,m=0.5Õ\\nn:\\nan,m=1snÕ\\nn:\\nan,m=0(1\\x00sn)(16.10)\\nD=0.5Õ\\nns1\\x00an,m\\nn (1\\x00sn)an,m=0.5Õ\\nn:\\nan,m=1(1\\x00sn)Õ\\nn:\\nan,m=0sn(16.11)\\nIf you inspect the value of C, it is basically “voting” (in a product\\nform, not a sum form) the scores of those students who agree that the\\nanswer is 1 with one-minus-the-score of those students who do not.\\nThe value of Dis doing the reverse. This is a form of multiplicative\\nvoting, which has the effect that if a given student has a perfect score\\nof 1.0, their results will carry the vote completely.\\nWe now have a way to:\\n1. Compute expected ground truth values ˜tm, given scores.\\n2. Optimize scores sngiven expected ground truth values.\\nThe full solution is then to alternate between these two. You can\\nstart by initializing the ground truth values at the majority vote (this\\nseems like a safe initialization). Given those, compute new scores.\\nGiven those new scores, compute new ground truth values. And\\nrepeat until tired.\\nIn the next two sections, we will consider a more complex unsu-\\npervised learning model for clustering, and then a generic mathe-\\nmatical framework for expectation maximization, which will answer\\nquestions like: will this process converge, and, if so, to what?\\n16.2Clustering with a Mixture of Gaussians\\nIn Chapter 9, you learned about probabilitic models for classiﬁcation\\nbased on density estimation. Let’s start with a fairly simple classiﬁca-\\ntion model that assumes we have labeled data. We will shortly remove\\nthis assumption. Our model will state that we have Kclasses, and\\ndata from class kis drawn from a Gaussian with mean mkand vari-\\nance s2\\nk. The choice of classes is parameterized by q. The generative\\nstory for this model is:190 a course in machine learning\\n1. For each example n=1 . . . N:\\n(a) Choose a label yn\\x18Disc(q)\\n(b) Choose example xn\\x18N or(myn,s2\\nyn)\\nThis generative story can be directly translated into a likelihood as\\nbefore:\\np(D) =Õ\\nnMult(ynjq)Nor(xnjmyn,s2\\nyn) (16.12)\\n=for each examplez }| {\\nÕ\\nnqyn|{z}\\nchoose labelh\\n2ps2\\nyni\\x00D\\n2exp\"\\n\\x001\\n2s2yn\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cxn\\x00myn\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2#\\n| {z }\\nchoose feature values(16.13)\\nIf you had access to labels, this would be all well and good, and\\nyou could obtain closed form solutions for the maximum likelihood\\nestimates of all parameters by taking a log and then taking gradients\\nof the log likelihood:\\nqk=fraction of training examples in class k (16.14)\\n=1\\nNå\\nn[yn=k]\\nmk=mean of training examples in class k (16.15)\\n=ån[yn=k]xn\\nån[yn=k]\\ns2\\nk=variance of training examples in class k (16.16)\\n=ån[yn=k]jjxn\\x00mkjj\\nån[yn=k]\\nSuppose that you don’t have labels. Analogously to the K-means You should be able to derive the\\nmaximum likelihood solution re-\\nsults formally by now.? algorithm, one potential solution is to iterate. You can start off with\\nguesses for the values of the unknown variables, and then iteratively\\nimprove them over time. In K-means, the approach was the assign\\nexamples to labels (or clusters). This time, instead of making hard\\nassignments (“example 10 belongs to cluster 4”), we’ll make soft as-\\nsign ments (“example 10 belongs half to cluster 4, a quarter to cluster\\n2 and a quarter to cluster 5”). So as not to confuse ourselves too\\nmuch, we’ll introduce a new variable, zn=hzn,1, . . . , zn,K(that sums\\nto one), to denote a fractional assignment of examples to clusters.\\nFigure 16.1:em:piecharts : A ﬁgure\\nshowing pie chartsThis notion of soft-assignments is visualized in Figure 16.1. Here,\\nwe’ve depicted each example as a pie chart, and it’s coloring denotes\\nthe degree to which it’s been assigned to each (of three) clusters. The\\nsize of the pie pieces correspond to the znvalues.expectation maximization 191\\nFormally, zn,kdenotes the probability that example nis assigned to\\ncluster k:\\nzn,k=p(yn=kjxn) (16.17)\\n=p(yn=k,xn)\\np(xn)(16.18)\\n=1\\nZnMult(kjq)Nor(xnjmk,s2\\nk) (16.19)\\nHere, the normalizer Znis to ensure that znsums to one.\\nGiven a set of parameters (the qs,ms and s2s), the fractional as-\\nsign ments zn,kare easy to compute. Now, akin to K-means, given\\nfractional assignments, you need to recompute estimates of the\\nmodel parameters. In analogy to the maximum likelihood solution\\n(Eqs ( ??)-(??)), you can do this by counting fractional points rather\\nthan full points. This gives the following re-estimation updates:\\nqk=fraction of training examples in class k (16.20)\\n=1\\nNå\\nnzn,k\\nmk=mean of fractional examples in class k (16.21)\\n=ånzn,kxn\\nånzn,k\\ns2\\nk=variance of fractional examples in class k (16.22)\\n=ånzn,kjjxn\\x00mkjj\\nånzn,k\\nAll that has happened here is that the hard assignments “ [yn=k]”\\nhave been replaced with soft assignments “ zn,k”. As a bit of fore-\\nshadowing of what is to come, what we’ve done is essentially replace\\nknown labels with expected labels, hence the name “expectation maxi-\\nmization.”\\nPutting this together yields Algorithm 16.2. This is the GMM\\n(“Gaus sian Mixture Mod els”) algorithm, because the probabilitic\\nmodel being learned describes a dataset as being drawn from a mix-\\nture distribution, where each component of this distribution is a\\nGaussian. Aside from the fact that GMMs\\nuse soft assignments and K-means\\nuses hard assignments, there are\\nother differences between the two\\napproaches. What are they??Just as in the K-means algorithm, this approach is succeptible to\\nlocal optima and quality of initialization. The heuristics for comput-\\ning better initializers for K-means are also useful here.\\n16.3The Expectation Maximization Framework\\nAt this point, you’ve seen a method for learning in a particular prob-\\nabilistic model with hidden variables. Two questions remain: ( 1) can192 a course in machine learning\\nAlgorithm 38GMM( X,K)\\n1:fork=1toKdo\\n2:mk some random location // randomly initialize mean for kth cluster\\n3:s2\\nk 1 // initialize variances\\n4:qk 1/K // each cluster equally likely a priori\\n5:end for\\n6:repeat\\n7:forn=1toNdo\\n8: fork=1toKdo\\n9: zn,k qk\\x02\\n2ps2\\nk\\x03\\x00D\\n2exph\\n\\x001\\n2s2\\nkjjxn\\x00mkjj2i\\n// compute\\n(unnormalized) fractional assignments\\n10: end for\\n11: zn 1\\nåkzn,kzn // normalize fractional assignments\\n12:end for\\n13:fork=1toKdo\\n14: qk 1\\nNånzn,k // re-estimate prior probability of cluster k\\n15: mk ånzn,kxn\\nånzn,k// re-estimate mean of cluster k\\n16: s2\\nk ånzn,kjjxn\\x00mkjj\\nånzn,k// re-estimate variance of cluster k\\n17:end for\\n18:until converged\\n19:return z // return cluster assignments\\nyou apply this idea more generally and ( 2) why is it even a reason-\\nable thing to do? Expectation maximization is a family of algorithms\\nfor performing maximum likelihood estimation in probabilistic mod-\\nels with hidden variables.\\nFigure 16.2:em:lowerbound : A ﬁgure\\nshowing successive lower boundsThe general ﬂavor of how we will proceed is as follows. We want\\nto maximize the log likelihood L, but this will turn out to be difﬁ-\\ncult to do directly. Instead, we’ll pick a surrogate function ˜Lthat’s a\\nlower bound onL(i.e., ˜L\\x14L everywhere) that’s (hopefully) easier\\nto maximize. We’ll construct the surrogate in such a way that increas-\\ning it will force the true likelihood to also go up. After maximizing\\n˜L, we’ll construct a new lower bound and optimize that. This process\\nis shown pictorially in Figure 16.2.\\nTo proceed, consider an arbitrary probabilistic model p(x,yjq),\\nwhere xdenotes the observed data, ydenotes the hidden data and\\nqdenotes the parameters. In the case of Gaussian Mixture Models,\\nxwas the data points, ywas the (unknown) labels and qincluded\\nthe cluster prior probabilities, the cluster means and the cluster vari-\\nances. Now, given access only to a number of examples x1, . . . , xN,\\nyou would like to estimate the parameters ( q) of the model.\\nProbabilistically, this means that some of the variables are un-\\nknown and therefore you need to marginalize (or sum) over their\\npossible values. Now, your data consists only of X=hx1,x2, . . . , xNi,expectation maximization 193\\nnot the (x,y)pairs in D. You can then write the likelihood as:\\np(Xjq) =å\\ny1å\\ny2\\x01\\x01\\x01å\\nyNp(X,y1,y2, . . .yNjq) marginalization\\n(16.23)\\n=å\\ny1å\\ny2\\x01\\x01\\x01å\\nyNÕ\\nnp(xn,ynjq) examples are independent\\n(16.24)\\n=Õ\\nnå\\nynp(xn,ynjq) algebra\\n(16.25)\\nAt this point, the natural thing to do is to take logs and then start\\ntaking gradients. However, once you start taking logs, you run into a\\nproblem: the log cannot eat the sum!\\nL(Xjq) =å\\nnlogå\\nynp(xn,ynjq) (16.26)\\nNamely, the log gets “stuck” outside the sum and cannot move in to\\ndecompose the rest of the likelihood term!\\nThe next step is to apply the somewhat strange, but strangely\\nuseful, trick of multiplying by 1. In particular, let q(\\x01)be an arbitrary\\nprobability distribution. We will multiply the p(. . .)term above by\\nq(yn)/q(yn), a valid step so long as qis never zero. This leads to:\\nL(Xjq) =å\\nnlogå\\nynq(yn)p(xn,ynjq)\\nq(yn)(16.27)\\nWe will now construct a lower bound using Jensen’s inequal ity.\\nThis is a very useful (and easy to prove!) result that states that\\nf(åilixi)\\x15åilif(xi), so long as (a) li\\x150 for all i, (b)åili=1,\\nand (c) fis concave. If this looks familiar, that’s just because it’s a\\ndirect result of the deﬁnition of concavity. Recall that fis concave if\\nf(ax+by)\\x15a f(x) +b f(x)whenever a+b=1. Prove Jensen’s inequality using the\\ndeﬁnition of concavity and induc-\\ntion.? You can now apply Jensen’s inequality to the log likelihood by\\nidentifying the list of q(yn)s as the ls, log as f(which is, indeed,\\nconcave) and each “ x” as the p/qterm. This yields:\\nL(Xjq)\\x15å\\nnå\\nynq(yn)logp(xn,ynjq)\\nq(yn)(16.28)\\n=å\\nnå\\nynh\\nq(yn)logp(xn,ynjq)\\x00q(yn)logq(yn)i\\n(16.29)\\n,˜L(Xjq) (16.30)\\nNote that this inequality holds for anychoice of function q, so long as\\nits non-negative and sums to one. In particular, it needn’t even by the194 a course in machine learning\\nsame function qfor each n. We will need to take advantage of both of\\nthese properties.\\nWe have succeeded in our ﬁrst goal: constructing a lower bound\\nonL. When you go to optimize this lower bound for q, the only part\\nthat matters is the ﬁrst term. The second term, qlogq, drops out as a\\nfunction of q. This means that the the maximization you need to be\\nable to compute, for ﬁxed qns, is:\\nq(new) arg max\\nqå\\nnå\\nynqn(yn)logp(xn,ynjq) (16.31)\\nThis is exactly the sort of maximization done for Gaussian mixture\\nmodels when we recomputed new means, variances and cluster prior\\nprobabilities.\\nThe second question is: what should qn(\\x01)actually be? Any rea-\\nsonable qwill lead to a lower bound, so in order to choose one qover\\nanother, we need another criterion. Recall that we are hoping to max-\\nimizeLby instead maximizing a lower bound. In order to ensure\\nthat an increase in the lower bound implies an increase in L, we need\\nto ensure thatL(Xjq) = ˜L(Xjq). In words: ˜Lshould be a lower\\nbound onLthat makes contact at the current point, q.\\n16.4Further Reading\\nTODO further reading17 | S TRUCTURED PREDICTION\\nDependencies:It is often the case that instead of predicting a single output, you\\nneed to predict multiple, correlated outputs simultaneously. In nat-\\nural language processing, you might want to assign a syntactic label\\n(like noun, verb, adjective, etc.) to words in a sentence: there is clear\\ncorrelation among these labels. In computer vision, you might want\\nto label regions in an image with object categories; again, there is\\ncorrelation among these regions. The branch of machine learning that\\nstudies such questions is struc tured prediction.\\nIn this chapter, we will cover two of the most common algorithms\\nfor structured prediction: the structured perceptron and the struc-\\ntured support vector machine. We will consider two types of struc-\\nture. The ﬁrst is the “sequence labeling” problem, typiﬁed by the\\nnatural language processing example above, but also common in\\ncomputational biology (labeling amino acids in DNA) and robotics\\n(labeling actions in a sequence). For this, we will develop specialized\\nprediction algorithms that take advantage of the sequential nature\\nof the task. We will also consider more general structures beyond\\nsequences, and discuss how to cast them in a generic optimization\\nframework: integerlinearprogram ming (orILP).\\nThe general framework we will explore is that of jointly scoring\\ninput/output conﬁgurations . We will construct algorithms that learn a\\nfunction s(x, ˆy)(sfor “score”), where xis an input (like an image)\\nand ˆ yis some predicted output (like a segmentation of that image).\\nFor any given image, there are a lotof possible segmentations (i.e.,\\na lot of possible ˆ ys), and the goal of sis to rank them in order of\\n“how good” they are: how compatible they are with the input x. The\\nmost important thing is that the scoring function sranks the true\\nsegmentation yhigher than any other imposter segmentation ˆ y. That\\nis, we want to ensure that s(x,y)>s(x, ˆy)for all ˆ y6=y. The main\\nchallenge we will face is how to do this efﬁciently , given that there are\\nso many imposter ˆ ys.Learning Objectives:\\n• Recognize when a problem should\\nbe solved using a structured predic-\\ntion technique.\\n• Implement the structured perceptron\\nalgorithm for sequence labeling.\\n• Map “argmax” problems to integer\\nlinear programs.\\n• Augment the structured perceptron\\nwith losses to derive structured\\nSVMs.196 a course in machine learning\\n17.1Multiclass Perceptron\\nIn order to build up to structured problems, let’s begin with a simpli-\\nﬁed by pedagogically useful stepping stone: multiclass classiﬁcation\\nwith a perceptron. As discussed earlier, in multiclass classiﬁcation we\\nhave inputs x2RDand output labels y2f1, 2, . . . , Kg. Our goal\\nis to learn a scoring function sso that s(x,y)>s(x,ˆy)for all ˆy6=y,\\nwhere yis the true label and ˆyis an imposter label. The general form\\nof scoring function we consider is a linear function of a joint feature\\nvector f(x,y):\\ns(x,y) =w\\x01f(x,y) (17.1)\\nHere, the features f(x,y)should denote how “compatible” the input\\nxis with the label y. We keep track of a single weight vector wthat\\nlearns how to weigh these different “compatibility” features.\\nA natural way to represent f, if you know nothing else about\\nthe problem, is an outer product between xand the label space. This\\nyields the following representation:\\nf(x,k) =D\\n0, 0, . . . , 0|{z}\\nD(k\\x001)zeros,x|{z}\\n2RD, 0, 0, . . . , 0|{z}\\nD(K\\x00k)zerosE\\n2RDK(17.2)\\nIn this representation, weffectively encodes a separate weight for\\nevery feature/label pair.\\nHow are we going to learn w? We will start with w=0and then\\nprocess each input one at a time. Suppose we get an input xwith\\ngold standard label y. We will use the current scoring function to\\npredict a label. In particular, we will predict the label ˆythat maxi-\\nmizes the score:\\nˆy=argmax\\nˆy2[1,K]s(x,ˆy) (17.3)\\n=argmax\\nˆy2[1,K]w\\x01f(x,ˆy) (17.4)\\nIf this predicted output is correct (i.e., ˆy=y), then, per the normal\\nperceptron, we will do nothing. Suppose that ˆy6=y. This means that\\nthe score of ˆyis greater than the score of y, so we want to update w\\nso that the score of ˆyis decreased and the score of yis increased. We\\ndo this by:\\nw w+f(x,y)\\x00f(x,ˆy) (17.5)\\nTo make sure this is doing what we expect, let’s consider what would\\nhappen if we computed scores under the updated value of w. To make\\nthe notation clear, let’s say w(old)are the weights before update, andstructured prediction 197\\nAlgorithm 39Multiclass Perceptron Train (D,MaxIter )\\n1:w 0 // initialize weights\\n2:foriter=1. . .MaxIter do\\n3:for all (x,y)2Ddo\\n4: ˆy argmaxkw\\x01f(x,k) // compute prediction\\n5: ifˆy6=ythen\\n6: w w+f(x,y)\\x00f(x,ˆy) // update weights\\n7: end if\\n8:end for\\n9:end for\\n10:return w // return learned weights\\nw(new)are the weights after update. Then:\\nw(new)\\x01f(x,y) (17.6)\\n=\\x10\\nw(old)+f(x,y)\\x00f(x,ˆy)\\x11\\n\\x01f(x,y) (17.7)\\n=w(old)\\x01f(x,y)|{z}\\nold prediction+f(x,y)\\x01f(x,y)|{z}\\n\\x150\\x00f(x,ˆy)\\x01f(x,y)|{z}\\n=0(17.8)\\nHere, the ﬁrst term is the old prediction. The second term is of the\\nform a\\x01awhich is non-negative (and, unless f(x,y)is the zero vec-\\ntor, positive). The third term is the dot product between ffor two\\ndifferent labels, which by deﬁnition of fis zero (see Eq ( 17.2)).Verify the score of ˆy,w(new)\\x01f(x,ˆy),\\ndecreases after an update, as we\\nwould want.? This gives rise to the updated multiclass perceptron speciﬁed in\\nAlgorithm 17.1. As with the normal perceptron, the generalization\\nof the multiclass perceptron increases dramatically if you do weight\\naveraging.\\nAn important note is that MulticlassPerceptronTrain is actually\\nmore powerful than suggested so far. For instance, suppose that you\\nhave three categories, but believe that two of them are tightly related,\\nwhile the third is very different. For instance, the categories might\\nbefmusic, movies, oncology g. You can encode this relatedness by\\ndeﬁning a feature expansion fthat reﬂects this:\\nf(x, music ) =hx,0,0,xi (17.9)\\nf(x, movies ) =h0,x,0,xi (17.10)\\nf(x, oncology ) =h0,0,x,0i (17.11)\\nThis encoding is identical to the normal encoding in the ﬁrst three\\npositions, but includes an extra copy of the features at the end,\\nshared between music and movies. By doing so, if the perceptron\\nwants to learn something common to music and movies, it can use\\nthis ﬁnal shared position.Suppose you have a hierarchy\\nof classes arranged in a tree.\\nHow could you use that to\\nconstruct a feature representa-\\ntion. You can think of the mu-\\nsic/movies/oncology example as\\na binary tree: the left branch of the\\nroot splits into music and movies;\\nthe right branch of the root is just\\noncology.?198 a course in machine learning\\n17.2Structured Perceptron\\nLet us now consider the sequence labeling task. In sequence labeling,\\nthe outputs are themselves variable-length vectors. An input/output\\npair (which must have the same length) might look like:\\nx=“ monsters eat tasty bunnies “ ( 17.12)\\ny= noun verb adj noun ( 17.13)\\nTo set terminology, we will refer to the entire sequence yas the “out-\\nput” and a single label within yas a “label”. As before, our goal is to\\nlearn a scoring function that scores the true output sequence yhigher\\nthan any imposter output sequence.\\nAs before, despite the fact that yis now a vector, we can stillde-\\nﬁne feature functions over the entire input/output pair. For instance,\\nwe might want to count the number of times “monsters” has been\\ntagged as “noun” in a given output. Or the number of times “verb”\\nis followed by “noun” in an output. Both of these are features that\\nare likely indicative of a correct output. We might also count the num-\\nber of times “tasty” has been tagged as a verb (probably a negative\\nfeature) and the number of times two verbs are adjacent (again, prob-\\nably a negative feature).\\nMore generally, a very standard set of features would be:\\n• the number of times word whas been labeled with tag l, for all\\nwords wand all syntactic tags l\\n• the number of times tag lis adjacent to tag l0in the output, for all\\ntags land l0\\nThe ﬁrst set of features are often called unary features , because they\\ntalk only about the relationship between the input (sentence) and a\\nsingle (unit) label in the output sequence. The second set of features\\nare often called Markov features , because they talk about adjacent la-\\nbels in the output sequence, which is reminiscent of Markov models\\nwhich only have short term memory.\\nNote that for a given input xof length L(in the example, L=\\n4), the number of possible outputs is KL, where Kis the number of\\nsyntactic tags. This means that the number of possible outputs grows\\nexponentially in the length of the input. In general, we write Y(x)to\\nmean “the set of all possible structured outputs for the input x”. We\\nhave just seen that jY(x)j=Klen(x).\\nDespite the fact that the inputs and outputs have variable length,\\nthe size of the feature representation is constant. If there are Vwords\\nin your vocabulary and Klabels for a given word, the the number of\\nunary features is VKand the number of Markov features is K2, sostructured prediction 199\\nAlgorithm 40Structured Perceptron Train (D,MaxIter )\\n1:w 0 // initialize weights\\n2:foriter=1. . .MaxIter do\\n3:for all (x,y)2Ddo\\n4: ˆy argmaxˆy2Y(x)w\\x01f(x, ˆy) // compute prediction\\n5: ifˆy6=ythen\\n6: w w+f(x,y)\\x00f(x, ˆy) // update weights\\n7: end if\\n8:end for\\n9:end for\\n10:return w // return learned weights\\nthe total number of features is K(V+K). Of course, more complex\\nfeature representations are possible and, in general, are a good idea.\\nFor example, it is often useful to have unary features of neighboring\\nwords like “the number of times the word immediately preceding a\\nverb was ’monsters’.”\\nNow that we have a ﬁxed size feature representation, we can de-\\nvelop a perceptron-style algorithm for sequence labeling. The core\\nidea is the same as before. We will maintain a single weight vector w.\\nWe will make predictions by choosing the (entire) output sequence\\nˆythat maximizes a score given by w\\x01f(x, ˆy). And if this output se-\\nquence is incorrect, we will adjust the weights word the correct output\\nsequence yand away from the incorrect output sequence ˆ y. This is\\nsummarized in Algorithm 17.2\\nYou may have noticed that Algorithm 17.2for the structured per-\\nceptron is identical to Algorithm 17.1, aside from the fact that in the\\nmulticlass perceptron the argmax is over the Kpossible classes, while\\nin the structured perceptron, the argmax is over the KLpossible out-\\nput sequences!\\nThe only difﬁculty in this algorithm is in line 4:\\nˆy argmax\\nˆy2Y(x)w\\x01f(x, ˆy) (17.14)\\nIn principle, this requires you to search over KLpossible output se-\\nquences ˆ yto ﬁnd the one that maximizes the dot product. Except for\\nvery small Kor very small L, this is computationally infeasible. Be-\\ncause of its difﬁculty, this is often refered to as the argmax prob lem\\nin structured prediction. Below, we consider how to solve the argmax\\nproblem for sequences.\\n17.3Argmax for Sequences\\nWe now face an algorithmic question, not a machine learning ques-\\ntion: how to compute the argmax in Eq 17.14efﬁciently. In general,200 a course in machine learning\\nthis is not possible. However, under somewhat restrictive assump-\\ntions about the form of our features f, we can solve this problem efﬁ-\\nciently, by casting it as the problem of computing a maximum weight\\npath through a speciﬁcally constructed lattice. This is a variant of the\\nViterbi algorithm for hidden Markov models, a classic example of dy-\\nnamic programming. (Later, in Section 17.6, we will consider argmax\\nfor more general problems.)\\nThe key observation for sequences is that—so long as we restrict\\nour attention to unary features and Markov features—the feature\\nfunction fdecomposes over the input. This is easiest to see with\\nan example. Consider the input/output sequence from before: x=\\n“monsters eat tasty bunnies” and y=[noun verb adj noun]. If we\\nwant to compute the number of times “bunnies” is tagged as “noun”\\nin this pair, we can do this by:\\n1. count the number of times “bunnies” is tagged as “noun” in the\\nﬁrst three words of the sentence\\n2. add to that the number of times “bunnies” is tagged as “noun” in\\nthe ﬁnal word\\nWe can do a similar exercise for Markov features, like the number of\\ntimes “adj” is followed by “noun”.\\nHowever, we don’t actually need these counts. All we need for\\ncomputing the argmax sequence is the dot product between the\\nweights wand these counts. In particular, we can compute w\\x01f(x,y)\\nas the dot product on all-but-the-last word plus the dot product on\\nthe last word: w\\x01f1:3(x,y) +w\\x01f4(x,y). Here, f1:3means “fea-\\ntures for everything up to and including position 3” and f4means\\n“features for position 4.”\\nMore generally, we can write f(x,y) =åL\\nl=1fl(x,y), where\\nfl(x,y)only includes features about position l.1In particular, we’re1In the case of Markov features, we\\nthink of them as pairs that endat\\nposition l, so “verb adj” would be the\\nactive feature for f3.taking advantage of the associative law for addition:\\nw\\x01f(x,y) =w\\x01L\\nå\\nl=1fl(x,y) decomposition of structure (17.15)\\n=L\\nå\\nl=1w\\x01fl(x,y) associative law (17.16)\\nWhat this means is that we can build a graph like that in Figure ??,\\nwith one verticle slice per time step ( l1 . . . L).2Each edge in this2A graph of this sort is called a trel-\\nlis, and sometimes a lattice in the\\nliterature.graph will receive a weight, constructed in such a way that if you\\ntake a complete path through the lattice, and add up all the weights,\\nthis will correspond exactly to w\\x01f(x,y).\\nTo complete the construction, let fl(x,\\x01\\x01\\x01\\x0e y\\x0ey0)denote the unary\\nfeatures at position ltogether with the Markov features that end atstructured prediction 201\\ntastyN\\nV\\nA\\nbunniesN\\nV\\nA\\neatN\\nV\\nA\\nmonstersN\\nV\\nA\\nFigure 17.1: A picture of a trellis se-\\nquence labeling. At each time step l\\nthe corresponding word can have any\\nof the three possible labels. Any path\\nthrough this trellis corresponds to a\\nunique labeling of this sentence. The\\ngold standard path is drawn with bold\\nred arrows. The highlighted edge cor-\\nresponds to the edge between l=2\\nand l=3 for verb/adj as described\\nin the text. That edge has weight\\nw\\x01f3(x,\\x01\\x01\\x01\\x0e verb\\x0eadj).\\nposition l. These features depend only onx,yand y0, and notany of\\nthe previous parts of the output.\\nFor example, in the running example “monsters/noun eat/verb\\ntasty/adj bunnies/noun”, consider the edge between l=2 and\\nl=3 going from “verb” to “adj”. (Note: this is a “correct” edge, in\\nthe sense that it belongs to the ground truth output.) The features\\nassociated with this edge will be unary features about “tasty/adj”\\nas well as Markov features about “verb/adj”. The weight of this edge\\nwill be exactly the total score (according to w) of those features.\\nFormally, consider an edge in the trellis that goes from time l\\x00\\n1 to l, and transitions from ytoy0. Set the weight of this edge to\\nexactly w\\x01fl(x,\\x01\\x01\\x01\\x0e y\\x0ey0). By doing so, we guarantee that the\\nsum of weights along any path through this lattice is exactly equal\\nto the score of that path. Once we have constructed the graph as\\nsuch, we can run any max-weight path algorithm to compute the\\nhighest scoring output. For trellises, this can be computed by the\\nViterbi algorithm, or by applying any of a number of path ﬁnding\\nalgorithms for more general graphs. A complete derivation of the\\ndynamic program in this case is given in Section 17.7for those who\\nwant to implement it directly.\\nThe main beneﬁt of this construction is that it is guaranteed to\\nexactly compute the argmax output for sequences required in the\\nstructured perceptron algorithm, efﬁciently . In particular, it’s run-\\ntime is O(LK2), which is an exponential improvement on the naive\\nO(KL)runtime if one were to enumerate every possible output se-\\nquence. The algorithm can be naturally extended to handle “higher\\norder” Markov assumptions, where features depend on triples or\\nquadruples of the output. The trellis becomes larger, but the algo-\\nrithm remains essentially the same. In order to handle a length M\\nMarkov features, the resulting algorithm will take O(LKM)time. In\\npractice, it’s rare that M>3 is necessary or useful.202 a course in machine learning\\n17.4Structured Support Vector Machines\\nIn Section 7.7we saw the support vector machine as a very useful\\ngeneral framework for binary classiﬁcation. In this section, we will\\ndevelop a related framework for structured support vector machines.\\nThe two main advantages of structured SVMs over the structured\\nperceptron are ( 1) it is regularized (though averaging in structured\\nperceptron achieves a similar effect) and ( 2) we can incorporate more\\ncomplex loss functions.\\nIn particular, one suboptimal thing about the structured percep-\\ntron is that all errors are consider equally bad. For structured prob-\\nlems, we often have much more nuanced and elaborate loss functions\\nthat we want to optimize. Even for sequence labeling, it is typically\\nfar worse to label every word incorrectly than to just label one word\\nincorrectly. It is very common to use Ham ming loss as a general loss\\nfunction for structured prediction. Hamming loss simply counts:\\nof all the predictions you made, how many were incorrect? For se-\\nquence labeling, it is:\\n`(Ham)(y, ˆy) =L\\nå\\nl=11[yl6=ˆyl] (17.17)\\nIn order to build up to structured SVMs, recall that SVMs began with\\nthe following optimization problem:\\nmin\\nw,x1\\n2jjwjj2\\n|{z}\\nlarge margin+Cå\\nnxn\\n|{z}\\nsmall slack(17.18)\\nsubj. to yn(w\\x01xn+b)\\x151\\x00xn (8n)\\nxn\\x150 (8n)\\nAfter a bit of work, we were able to reformulate this in terms of a\\nstandard loss optimization algorithm with hinge loss:\\nminw1\\n2jjwjj2\\n|{z}\\nlarge margin+Cå\\nn`(hin)(yn,w\\x01xn+b)\\n|{z}\\nsmall slack(17.19)\\nWe can do a similar derivation in the structured case. The question\\nis: exactly what should the slack be measuring? Our goal is for the\\nscore of the true output yto beat the score of any imposter output\\nˆy. To incorporate loss, we will say that we want the score of the true\\noutput to beat the score of any imposter output by at least the loss\\nthat would be suffered if we were to predict that imposter output. An\\nalternative view is the ranking view: we want the true output to be\\nranked above any imposter by an amount at least equal to the loss.structured prediction 203\\nTo keep notation simple, we will write sw(x,y)to denote the score\\nof the pair x,y, namely w\\x01f(x,y). This suggests a set of constraints\\nof the form:\\nsw(x,y)\\x00sw(x, ˆy)\\x15`(Ham)(y, ˆy)\\x00xˆy(8n,8ˆy2Y(x)) (17.20)\\nThe rest of the optimization problem remains the same, yielding:\\nmin\\nw,x1\\n2jjwjj2+Cå\\nnå\\nˆy2Yxnxn,ˆy (17.21)\\nsubj. to sw(x,y)\\x00sw(x, ˆy)\\n\\x15`(Ham)(yn, ˆy)\\x00xn,ˆy (8n,8ˆy2Y(xn))\\nxn,ˆy\\x150 (8n,8ˆy2Y(xn))\\nThis optimization problem asks for a large margin and small slack,\\nwhere there is a slack very for every training example and every\\npossible incorrect output associated with that training example. In\\ngeneral, this is way too many slack variables and way too many con-\\nstraints!\\nThere is a very useful, general trick we can apply. If you focus on\\nthe ﬁrst constraint, it roughly says (letting s()denote score): s(y)\\x15\\x02\\ns(ˆy) +`(y, ˆy)\\x03\\nfor all ˆ y, modulo slack. We’ll refer to the thing in\\nbrackets as the “loss-augmented score.” But if we want to guarantee\\nthat the score of the true ybeats the loss-augmented score of allˆy, it’s\\nenough to ensure that it beats the loss-augmented score of the most\\nconfusing imposter. Namely, it is sufﬁcient to require that s(y)\\x15\\nmax ˆy\\x02\\ns(ˆy) +`(y, ˆy)\\x03\\n, modulo slack. Expanding out the deﬁnition\\nofs()and adding slack back in, we can replace the exponentially\\nlarge number of constraints in Eq ( 17.21) with the simpler set of\\nconstraints:\\nsw(xn,yn)\\x15max\\nˆy2Y(xn)h\\nsw(xn, ˆy) +`(Ham)(yn, ˆy)i\\n\\x00xn (8n)\\nWe can now apply the same trick as before to remove xnfrom the\\nanalysis. In particular, because xnis constrained to be \\x150 and be-\\ncause we are trying to minimize it’s sum, we can ﬁgure out that out\\nthe optimum, it will be the case that:\\nxn=max(\\n0, max\\nˆy2Y(xn)h\\nsw(xn, ˆy) +`(Ham)(yn, ˆy)i\\n\\x00sw(xn,yn))\\n(17.22)\\n=`(s-h)(yn,xn,w) (17.23)\\nThis value is referred to as the struc tured hinge loss, which we have\\ndenoted as `(s-h)(yn,xn,w). This is because, although it is more com-\\nplex, it bears a striking resemlance to the hinge loss from Chapter 7.204 a course in machine learning\\nIn particular, if the score of the true output beats the score of every\\nthe best imposter by at least its loss, then xnwill be zero. On the\\nother hand, if some imposter (plus its loss) beats the true output, the\\nloss scales linearly as a function of the difference. At this point, there\\nis nothing special about Hamming loss, so we will replace it with\\nsome arbitrary structured loss `.\\nPlugging this back into the objective function of Eq ( 17.21), we can\\nwrite the structured SVM as an unconstrained optimization problem,\\nakin to Eq ( 17.19), as:\\nminw1\\n2jjwjj2+Cå\\nn`(s-h)(yn,xn,w) (17.24)\\nThis is now in a form that we can optimize using subgradient descent\\n(Chapter 7) or stochastic subgradient descent (Chapter 14).\\nIn order to compute subgradients of Eq ( 17.24), we need to be able\\nto compute subgradients of the structured hinge loss. Mathematically\\nthis is straightforward. If the structured hinge loss on an example\\n(x,vy)is zero, then the gradient with respect to wis also zero. If the\\nstructured hinge loss is positive, then the gradient is:\\nrw`(s-h)(y,x,w)ifthe loss is >0 ( 17.25)\\nexpand deﬁnition using arbitrary structured loss `\\n=rw(\\nmax\\nˆy2Y(xn)h\\nw\\x01f(xn, ˆy) +`(yn, ˆy)i\\n\\x00w\\x01f(xn,yn))\\n(17.26)\\ndeﬁne ˆynto be the output that attains the maximum above, rearrange\\n=rwn\\nw\\x01f(xn, ˆy)\\x00w\\x01f(xn,yn) +`(yn, ˆy)o\\n(17.27)\\ntake gradient\\n=f(xn, ˆy)\\x00f(xn,yn) (17.28)\\nPutting this together, we get the full gradient as:\\nrw`(s-h)(yn,xn,w) =(\\n0 if`(s-h)(yn,xn,w) =0\\nf(xn, ˆyn)\\x00f(xn,yn)otherwise\\nwhere ˆ yn=argmax\\nˆyn2Y(xn)h\\nw\\x01f(xn, ˆyn) +`(yn, ˆyn)i\\n(17.29)\\nThe form of this gradient is very simple: it is equal to the features\\nof the worst imposter minus the features of the truth, unless the\\ntruth beats all imposters, in which case it’s zero. When plugged into\\nstochastic subgradient descent, you end up with an update that looks\\nvery much like the structured perceptron: if the current prediction\\n(ˆyn) is correct, there is no gradient step. But if the current prediction\\nis incorrect, you step wtoward the truth and away from the imposter.structured prediction 205\\nAlgorithm 41Stoch SubGrad Struct SVM( D,MaxIter ,l,`)\\n1:w 0 // initialize weights\\n2:foriter=1. . .MaxIter do\\n3:for all (x,y)2Ddo\\n4: ˆy argmaxˆy2Y(x)w\\x01f(x, ˆy) +`(y, ˆy) // loss-augmented prediction\\n5: ifˆy6=ythen\\n6: w w+f(x,y)\\x00f(x, ˆy) // update weights\\n7: end if\\n8: w w\\x00l\\nNw // shrink weights due to regularizer\\n9:end for\\n10:end for\\n11:return w // return learned weights\\nWe will consider how to compute the loss-augmented argmax in\\nthe next section, but before that we summarize an algorithm for opti-\\nmizing structured SVMs using stochastic subgradient descent: Algo-\\nrithm 17.4. Of course there are other possible optimization strategies;\\nwe are highlighting this one because it is nearly identical to the struc-\\ntured perceptron. The only differences are: ( 1) on line 4you use loss-\\naugmented argmax instead of argmax; and ( 2) on line 8the weights\\nare shrunk slightly corresponding to the `2regularizer on w. (Note:\\nwe have used l=1/(2C)to make the connection to linear models\\nclearer.)\\n17.5Loss-Augmented Argmax\\nThe challenge that arises is that we now have a more complicated\\nargmax problem that before. In structured perceptron, we only\\nneeded to compute ˆ ynas the output that maximized its score (see\\nEq17.14). Here, we need to ﬁnd the output that maximizes it score\\nplus it’s loss (Eq ( 17.29)). This optimization problem is refered to as\\nloss-augmented search orloss-augmented inference .\\nBefore solving the loss-augmented inference problem, it’s worth\\nthinking about why it makes sense. What is ˆ yn? It’s the output that\\nhas the highest score among all outputs, after adding the output’s\\ncorresponding loss to that score. In other words, every incorrect\\noutput gets an artiﬁcial boost to its score, equal to its loss. The loss is\\nserving to make imposters look even better than they really are, so if\\nthe truth is to beat an imposter, it has to beat it by a lot. In fact, this\\nloss augmentation is essentially playing the role of a margin, where\\nthe required margin scales according to the loss.\\nThe algorithmic question, then, is how to compute ˆ yn. In the fully\\ngeneral case, this is at least as hard as the normal argmax problem, so\\nwe cannot expect a general solution. Moreover, even in cases where\\nthe argmax problem is easy (like for sequences), the loss-augmented206 a course in machine learning\\nargmax problem can still be difﬁcult. In order to make it easier, we\\nneed to assume that the loss decomposes of the input in a way that’s\\nconsistent with the features. In particular, if the structured loss func-\\ntion is Hamming loss, this is often straightforward.\\nAs a concrete example, let’s consider loss-augmented argmax for\\nsequences under Hamming loss. In comparison to the trellis problem\\nsolved in Section 17.7, the only difference is that we want to reward\\npaths that go through incorrect nodes in the trellis! In particular, in\\nFigure 17.1, all of the edges that are not part of the gold standard\\npath—those that are thinner and grey—get a free “ +1” added to their\\nweights. Since Hamming loss adds one to the score for any word\\nthat’s predicted incorrectly, this means that every edge in the trellis\\nthat leads to an incorrect node (i.e., one that does not match the gold\\ntruth label) gets a “ +1” added to its weight.\\nAgain, consider an edge in the trellis that goes from time l\\x001 to\\nl, and transitions from ytoy0. In the non-loss-augmented, the weight\\nof this edge was exactly w\\x01fl(x,\\x01\\x01\\x01\\x0e y\\x0ey0). In the loss-augmented\\ncases, the weight of this edge becomes:\\nw\\x01fl(x,\\x01\\x01\\x01\\x0e y\\x0ey0)|{z}\\nedge score, as before+ 1[y06=yl]|{z}\\n+1 for mispredictions(17.30)\\nOnce this loss-augmented graph has been constructed, the same max-\\nweight path algorithm can be run to ﬁnd the loss-augmented argmax\\nsequence.\\n17.6Argmax in General\\nThe general argmax problem for structured perceptron is the algo-\\nrithmic question of whether the following can be efﬁciently com-\\nputed:\\nˆy argmax\\nˆy2Y(x)w\\x01f(x, ˆy) (17.31)\\nWe have seen that ifthe output spaceY(x)is sequences andthe\\nonly types of features are unary features and Markov features, then\\nthis can be computed efﬁciently. There are a small number of other\\nstructured output spaces and feature restrictions for which efﬁcient\\nproblem-speciﬁc algorithms exist:\\n• Binary trees, with context-free features: use the CKY algorithm\\n•2d image segmentation, with adjacent-pixel features: use a form of\\ngraph cuts\\n• Spanning trees, with edge-based features: use Kruskal’s algorithm\\n(or for directed spanning trees, use Chu-Liu/Edmonds algorithm)structured prediction 207\\nThese special cases are often very useful, and many problems can be\\ncast in one of these frameworks. However, it is often the case that you\\nneed a more general solution.\\nOne of the most generally useful solutions is to cast the argmax\\nproblem as an integerlinearprogram , orILP. ILPs are a speciﬁc\\ntype of mathematical program/optimization problem, in which the\\nobjective function being optimized is linear and the constraints are\\nlinear. However, unlike “normal” linear programs, in an ILP you are\\nallowed to have integer constraints and disallow fractional values.\\nThe general form of an ILP is, for a ﬁxed vector a:\\nmaxza\\x01zsubj. to linear constraints on z (17.32)\\nThe main point is that the constraints on zare allowed to include\\nconstraints like z32f0, 1g, which is considered an integer constraint.\\nBeing able to cast your argmax problem as an ILP has the advan-\\ntage that there are very good, efﬁciently, well-engineered ILP solvers\\nout there in the world.3ILPs are not a panacea though: in the worst3I like Gurobi best, and it’s free for\\nacademic use. It also has a really nice\\nPython interface.case, the ILP solver will be horribly inefﬁcient. But for prototyping,\\nor if there are no better options, it’s a very handy technique.\\nFiguring out how exactly to cast your argmax problem as an ILP\\ncan be a bit challenging. Let’s start with an example of encoding\\nsequence labeling with Markov features as an ILP . We ﬁrst need\\nto decide what the variables will be. Because we need to encode\\npairwise features, we will let our variables be of the form:\\nzl,k0,k=1[label liskand label l\\x001 isk0] (17.33)\\nThese zs will all be binary indicator variables.\\nOur next task is to construct the linear objective function. To do\\nso, we need to assign a value to al,k0,kin such a way that a\\x01zwill be\\nexactly equal to w\\x01f(x,y(z)), where y(z)denotes the sequence that\\nwe can read off of the variables z. With a little thought, we arrive at:\\nal,k0,k=w\\x01fl(x,h. . . ,k0,ki) (17.34)\\nFinally, we need to construct constaints. There are a few things that\\nthese constraints need to enforce:\\n1. That all the zs are binary. That’s easy: just say zl,k0,k2f0, 1g, for\\nalll,k0,k.\\n2. That for a given position l, there is exactly one active z. We can do\\nthis with an equality constraint: åkåk0zl,k0,k=1 for all l.\\n3. That the zs are internally consistent: if the label at position 5 is\\nsupposed to be “noun” then both z5,.,.and z 6,.,.need to agree on208 a course in machine learning\\nthis. We can do this as: åk0zl,k0,k=åk00zl+1,k,k00for all l,k. Effec-\\ntively what this is saying is that z5,?,verb =z6,verb,? where the “?”\\nmeans “sum over all possibilities.”\\nThis fully speciﬁes an ILP that you can relatively easily implement\\n(arguably more easily than the dynamic program in Algorithm 17.7)\\nand which will solve the argmax problem for you. Will it be efﬁcient?\\nIn this case, probably yes. Will it be as efﬁcient as the dynamic pro-\\ngram? Probably not.\\nIt takes a bit of effort and time to get used to casting optimization\\nproblems as ILPs, and certainly not all can be, but most can and it’s a\\nvery nice alternative.\\nIn the case of loss-augmented search for structured SVMs (as\\nopposed to structured perceptron), the objective function of the ILP\\nwill need to be modiﬁed to include terms corresponding to the loss.\\n17.7Dynamic Programming for Sequences\\nRecall the decomposition we derived earlier:\\nw\\x01f(x,y) =w\\x01L\\nå\\nl=1fl(x,y) decomposition of structure (17.35)\\n=L\\nå\\nl=1w\\x01fl(x,y) associative law (17.36)\\nThis decomposition allows us to construct the following dynamic\\nprogram. We will compute al,kas the score of the best possible output\\npreﬁx up to and including position lthat labels the lth word with\\nlabel k. More formally:\\nal,k=max\\nˆy1:l\\x001w\\x01f1:l(x, ˆy\\x0ek) (17.37)\\nHere, ˆ yis a sequence of length l\\x001, and ˆ y\\x0ekdenotes the sequence\\nof length lobtained by adding konto the end. The max denotes the\\nfact that we are seeking the bestpossible preﬁx up to position l\\x001,\\nand the forcing the label for position lto be k.\\nBefore working through the details, let’s consider an example.\\nSuppose that we’ve computing the as up to l=2, and have: a2,noun =\\n2,a2,verb =9,a2,adj=\\x001 (recall: position l=2 is “eat”). We want\\nto extend this to position 3; for example, we want to compute a3,adj.\\nLet’s assume there’s a single unary feature here, “tasty/adj” and\\nthree possible Markov features of the form “?:adj”. Assume these\\nweights are as given to the right.4Now, the question for a3,adj is:4w“tasty/adj” =1.2\\nw“noun:adj” =\\x005\\nw“verb:adj” =2.5\\nw“adj:adj” =2.2what’s the score of the best preﬁx that labels “tasty” as “adj”? We can\\nobtain this by taking the best preﬁx up to “eat” and then appendingstructured prediction 209\\neach possible label. Whichever combination is best is the winner. The\\nrelevant computation is:\\na3,adj=maxn\\na2,noun +w“tasty/adj” +w“noun:adj”\\na2,verb +w“tasty/adj” +w“verb:adj”\\na2,adj+w“tasty/adj” +w“adj:adj”o\\n(17.38)\\n=maxn\\n2+1.2\\x005, 9 +1.2+2.5,\\x001+1.2+2.2o\\n(17.39)\\n=maxn\\n\\x001.8, 12.7, 2.4o\\n=12.7 ( 17.40)\\nThis means that (a) the score for the preﬁx ending at position 3la-\\nbeled as adjective is 12.7, and (b) the “winning” previous label was\\n“verb”. We will need to record these winning previous labels so that\\nwe can extract the best path at the end. Let’s denote by zl,kthe label\\nat position l\\x001 that achieves the max.\\nFrom here, we can formally compute the as recursively. The\\nmain observation that will be necessary is that, because we have\\nlimited ourselves to Markov features, fl+1(x,hy1,y2, . . . , yl,yl+1i)\\ndepends only on the last two terms of y, and does notdepend on\\ny1,y2, . . . , yl\\x001. The full recursion is derived as:\\na0,k=08k (17.41)\\nz0,k=Æ8k (17.42)\\nthe score for any empty sequence is zero\\nal+1,k=max\\nˆy1:lw\\x01f1:l+1(x, ˆy\\x0ek) (17.43)\\nseparate score of preﬁx from score of position l+1\\n=max\\nˆy1:lw\\x01\\x10\\nf1:l(x, ˆy) +fl+1(x, ˆy\\x0ek)\\x11\\n(17.44)\\ndistributive law over dot products\\n=max\\nˆy1:lh\\nw\\x01f1:l(x, ˆy) +w\\x01fl+1(x, ˆy\\x0ek)i\\n(17.45)\\nseparate out ﬁnal label from preﬁx, call it k’\\n=max\\nˆy1:l\\x001max\\nk0h\\nw\\x01f1:l(x, ˆy\\x0ek0) +w\\x01fl+1(x, ˆy\\x0ek0\\x0ek)i\\n(17.46)\\nswap order of maxes, and last term doesn’t depend on preﬁx\\n=max\\nk0\\x14h\\nmax\\nˆy1:l\\x001w\\x01f1:l(x, ˆy\\x0ek0)i\\n+w\\x01fl+1(x,h. . . ,k0,ki)i\\n(17.47)\\napply recursive deﬁnition\\n=max\\nk0h\\nal,k0+w\\x01fl+1(x,h. . . ,k0,ki)i\\n(17.48)210 a course in machine learning\\nAlgorithm 42Argmax ForSequences (x,w)\\n1:L len(x)\\n2:al,k 0,zk,l 0,8k=1 . . . K,8l=0 . . . L // initialize variables\\n3:forl=0. . .L-1do\\n4:fork=1. . .Kdo\\n5: al+1,k max k0\\x02\\nal,k0+w\\x01fl+1(x,h. . . ,k0,ki)\\x03\\n// recursion:\\n// here, fl+1(. . .k0,k. . .)is the set of features associated with\\n// output position l+1and two adjacent labels k0andkat that position\\n6: zl+1,k thek’ that achieves the maximum above // store backpointer\\n7:end for\\n8:end for\\n9:y h0,0, . . . , 0i // initialize predicted output to L-many zeros\\n10:yL argmaxkaL,k // extract highest scoring ﬁnal label\\n11:forl=L-1. . .1do\\n12:yl zl,yl+1// traceback zbased on yl+1\\n13:end for\\n14:return y // return predicted output\\nand record a backpointer to the k’ that achieves the max\\nzl+1,k=argmax\\nk0h\\nal,k0+w\\x01fl+1(x,h. . . ,k0,ki)i\\n(17.49)\\nAt the end, we can take max kaL,kas the score of the best output\\nsequence. To extract the ﬁnal sequence, we know that the best label\\nfor the last word is argmax aL,k. Let’s call this ˆyLOnce we know that,\\nthe best previous label is zL\\x001,ˆyL. We can then follow a path through z\\nback to the beginning. Putting this all together gives Algorithm 17.7.\\nThe main beneﬁt of Algorithm 17.7is that it is guaranteed to ex-\\nactly compute the argmax output for sequences required in the struc-\\ntured perceptron algorithm, efﬁciently . In particular, it’s runtime is\\nO(LK2), which is an exponential improvement on the naive O(KL)\\nruntime if one were to enumerate every possible output sequence.\\nThe algorithm can be naturally extended to handle “higher order”\\nMarkov assumptions, where features depend on triples or quadru-\\nples of the output. The memoization becomes notationally cumber-\\nsome, but the algorithm remains essentially the same. In order to\\nhandle a length MMarkov features, the resulting algorithm will take\\nO(LKM)time. In practice, it’s rare that M>3 is necessary or useful.\\nIn the case of loss-augmented search for structured SVMs (as\\nopposed to structured perceptron), we need to include the scores\\ncoming from the loss augmentation in the dynamic program. The\\nonly thing that changes between the standard argmax solution (Al-\\ngorithm 17.7, and derivation in Eq ( 17.48)) is that the any time an\\nincorrect label is used, the (loss-augmented) score increases by one.\\nRecall that in the non-loss-augmented case, we have the arecursionstructured prediction 211\\nas:\\nal+1,k=max\\nˆy1:lw\\x01f1:l+1(x, ˆy\\x0ek) (17.50)\\n=max\\nk0h\\nal,k0+w\\x01fl+1(x,h. . . ,k0,ki)i\\n(17.51)\\nIf we deﬁne ˜ato be the loss-augmented score, the corresponding\\nrecursion is (differences highlighted in blue):\\n˜al+1,k=max\\nˆy1:lw\\x01f1:l+1(x, ˆy\\x0ek)+`(Ham)\\n1:l+1(y, ˆy\\x0ek) (17.52)\\n=max\\nk0h\\n˜al,k0+w\\x01fl+1(x,h. . . ,k0,ki)i\\n+1[k6=yl+1] (17.53)\\nIn other words, when computing ˜ain the loss-augmented case,\\nwhenever the output prediction is forced to pass through an incorrect\\nlabel, the score for that cell in the dynamic program gets increased\\nby one. The resulting algorithm is identical to Algorithm 17.7, except\\nthat Eq ( 17.53) is used for computing as.\\n17.8Further Reading\\nTODO18 | I MITATION LEARNING\\nDependencies:So far ,we have largely considered machine learning\\nproblems in which the goal of the learning algorithm is to make\\nasingle prediction. In many real world problems, however, an algo-\\nrithm must make a sequence of decisions, with the world possibly\\nchanging during that sequence. Such problems are often called se-\\nquen tialdecision mak ingproblems. A straightforward example—\\nwhich will be the running example for this chapter—is that of self-\\ndriving cars. We want to train a machine learning algorithm to drive\\na car. But driving a car is not a single prediction: it’s a sequence of\\npredictions over time. And as the machine is driving the car, the\\nworld around it is changing, often based on its own behavior. This\\ncreates complicated feedback loops, and one of the major challenges\\nwe will face is how to deal with these feedback loops.\\nTo make this discussion more concrete, let’s consider the case of a\\nself-driving car. And let’s consider a very simplistic car, in which the\\nonly decision that has to be made is how to steer, and that’s between\\none of three options: fleft, right, noneg. In the imitation learning\\nsetting, we assume that we have access to an expert ororaclethat al-\\nready knows how to drive. We want to watch the expert driving, and\\nlearn to imitate their behavior. Hence: imitation learn ing(sometimes\\ncalled learn ingbydemon stration orprogram ming byexample, in\\nthe sense that programs are learned, and not implemented).\\nAt each point in time t=1 . . . T, the car recieves sensor informa-\\ntion xt(for instance, a camera photo ahead of the car, or radar read-\\nings). It then has to take an action, at; in the case of the car, this is\\none of the three available steering actions. The car then suffers some\\nloss`t; this might be zero in the case that it’s driving well, or large in\\nthe case that it crashes. The world then changes, moves to time step\\nt+1, sensor readings xt+1are observed, action at+1is taken, loss `t+1\\nis suffered, and the process continues.\\nThe goal is to learn a function fthat maps from sensor readings xt\\nto actions. Because of close connections to the ﬁeld of reinforce ment\\nlearn ing, this function is typically called a policy. The measure ofLearning Objectives:\\n• Be able to formulate imitation\\nlearning problems.\\n• Understand the failure cases of\\nsimple classiﬁcation approaches to\\nimitation learning.\\n• Implement solutions to those prob-\\nlems based on either classiﬁcation or\\ndataset aggregation.\\n• Relate structured prediction and\\nimitation learning.Programming is a skill best acquired by practice and example\\nrather than from books. – Alan Turingimitation learning 213\\nsuccess of a policy is: if we were to run this policy, how much total\\nloss would be suffered. In particular, suppose that the trajectory\\n(denoted t) of observation/action/reward triples encountered by\\nyour policy is:\\nt=x1,a1|{z}\\n=f(x1),`1,x2,a2|{z}\\n=f(x2),`2, . . . , xT,aT|{z}\\n=f(xT),`T (18.1)\\nThe losses `tdepend implicitly on the state of the world and the\\nactions of the policy. The goal of fis to minimize the expected loss\\nEt\\x18fh\\nåT\\nt=1`ti\\n, where the expectation is taken over all randomness in\\nthe world, and the sequence of actions taken is according to f.1 1It’s completely okay for fto look\\nat more than just xtwhen making\\npredictions; for instance, it might want\\nto look at xt\\x001, orat\\x001and at\\x002. As\\nlong as it only references information\\nfrom the past, this is ﬁne. For notational\\nsimplicity, we will assume that all of\\nthis relevant information is summarized\\ninxt.18.1Imitation Learning by Classiﬁcation\\nexpertexpert\\nFigure 18.1: A single expert trajectory in\\na self-driving car.We will begin with a straightforward, but brittle, approach to imita-\\ntion learning. We assume access to a set of training trajectories taken\\nby an expert. For example, consider a self-driving car, like that in Fig-\\nure18.1. A single trajectory tconsists of a sequence of observations\\n(what is seen from the car’s sensors) and a sequence of actions (what\\naction did the expect take at that point in time). The idea in imitation\\nlearning by classiﬁcation is to learn a classiﬁer that attempts to mimic\\nthe expert’s action based on the observations at that time.\\nIn particular, we have t1,t2, . . . ,tN. Each of the Ntrajectories\\ncomprises a sequence of T-many observation/action/loss triples,\\nwhere the action is the action taken by the expert. T, the length of\\nthe trajectory is typically called the time hori zon (or just hori zon).\\nFor instance, we may ask an expert human driver to drive N=20\\ndifferent routes, and record the observations and actions that driver\\nsaw and took during those routes. These are our training trajectories.\\nWe assume for simplicity that each of these trajectories is of ﬁxed\\nlength T, though this is mostly for notational convenience.\\nThe most straightforward thing we can do is convert this expert\\ndata into a big multiclass classiﬁcation problem. We take our favorite\\nmulticlass classiﬁcation algorithm, and use it to learn a mapping\\nfrom xtoa. The data on which it is trained is the set of all observa-\\ntion/action pairs visited during any of the Ntrajectories. In total,\\nthis would be NTexamples. This approach is summarized in Algo-\\nrithm 18.1for training and Algorithm 18.1for prediction.\\nHow well does this approach work?\\nThe ﬁrst question is: how good is the expert? If we learn to mimic\\nan expert, but the expert is no good, we lose. In general, it also seems\\nunrealistic to believe this algorithm should be able to improve on\\nthe expert. Similarly, if our multiclass classiﬁcation algorithm A\\nis crummy, we also lose. So part of the question “how well does214 a course in machine learning\\nAlgorithm 43Supervised Imitation Train (A,t1,t2, . . . ,tN)\\n1:D \\n(x,a):8n,8(x,a,`)2tn\\x0b\\n// collect all observation/action pairs\\n2:returnA(D) // train multiclass classiﬁer on D\\nAlgorithm 44Supervised Imitation Test(f)\\n1:fort=1. . .Tdo\\n2:xt current observation\\n3:at f(xt) // ask policy to choose an action\\n4:take action at\\n5:`t observe instantaneous loss\\n6:end for\\n7:return åT\\nt=1`t // return total loss\\nthis work” is the more basic question of: what are we even trying to\\nmeasure?\\nThere is a nice theorem2that gives an upper bound on the loss2Ross et al. 2011\\nsuffered by the SupervisedIL algorithm (Algorithm 18.1) as a func-\\ntion of (a) the quality of the expert, and (b) the error rate of the\\nlearned classiﬁer. To be clear, we need to distinguish between the\\nloss of the policy when run for Tsteps to form a full trajectory, and\\nthe error rate of the learned classiﬁer, which is just it’s average mul-\\nticlass classiﬁcation error. The theorem states, roughly, that the loss\\nof the learned policy is at most the loss of the expert plus T2times the\\nerror rate of the classiﬁer.\\nTheorem 18(Loss of SupervisedIL) .Suppose that one runs Algo-\\nrithm 18.1using a multiclass classiﬁer that optimizes the 0-1loss (or an\\nupperbound thereof). Let ebe the error rate of the underlying classiﬁer\\n(in expectation) and assume that all instantaneous losses are in the range\\n[0,`(max)]. Let f be the learned policy; then:\\nEt\\x18f\"\\nå\\nt`t#\\n|{z}\\nloss of learned policy\\x14Et\\x18expert\"\\nå\\nt`t#\\n|{z}\\nloss of expert+`(max)T2e (18.2)\\nIntuitively, this bound on the loss is about a factor of Taway from\\nwhat we might hope for. In particular, the multiclass classiﬁer makes\\nerrors on an efraction of it’s actions, measured by zero/one loss.\\nIn the worst case, this will lead to a loss of `(max)efor a single step.\\nSumming all these errors over the entire trajectory would lead to\\na loss on the order of `(max)Te, which is a factor Tbetter than this\\ntheorem provides. A natural question (addressed in the next section)\\nis whether this is analysis is tight. A related question (addressed in\\nthe section after that) is whether we can do better. Before getting\\nthere, though, it’s worth highlighting that an extra factor of Tisreallyimitation learning 215\\nbad. It can cause even very small multiclass error rates to blow up; in\\nparticular, if e\\x151/T, we lose, and Tcan be in the hundreds or more.\\n18.2Failure Analysis\\nThe biggest single issue with the supervised learning approach to\\nimitation learning is that it cannot learn to recover from failures. That\\nis: it has only been trained based on expert trajectories. This means\\nthat the only training data it has seen is that of an expert driver. If\\nitever veers from that state distribution, it may have no idea how\\nto recover. As a concrete example, perhaps the expert driver never\\never gets themselves into a state where they are directly facing a\\nwall. Moreover, the expert driver probably tends to drive forward\\nmore than backward. If the imperfect learner manages to make a few\\nerrors and get stuck next to a wall, it’s likely to resort to the general\\n“drive forward” rule and stay there forever. This is the problem of\\ncom pound ingerror; and yes, it does happen in practice.\\nIt turns out that it’s possible to construct an imitation learning\\nproblem on which the T2compounding error is unavoidable. Con-\\nsider the following somewhat artiﬁcial problem. At time t=1 you’re\\nshown a picture of either a zero or a one. You have two possible ac-\\ntions: press a button marked “zero” or press a button marked “one.”\\nThe “correct” thing to do at t=1 is to press the button that corre-\\nsponds to the image you’ve been shown. Pressing the correct button\\nleads to `1=0; the incorrect leads to `1=1. Now, at time t=2 you\\nare shown another image, again of a zero or one. The correct thing to\\ndo in this time step is the xor of (a) the number written on the picture\\nyou see right now, and (b) the correct answer from the previous time\\nstep. This holds in general for t>1.\\nThere are two important things about this construction. The ﬁrst\\nis that the expert can easily get zero loss. The second is that once the\\nlearned policy makes a single mistake, this can cause it to make all\\nfuture decisions incorrectly. (At least until it “luckily” makes another\\n“mistake” to get it back on track.)\\nBased on this construction, you can show the following theorem3.3Kääriäinen 2006\\nTheorem 19(Lower Bound for SupervisedIL) .There exist imitation\\nlearning problems on which Algorithm 18.1is able to achieve small classiﬁ-\\ncation error e2[0, 1/ T]under an optimal expert, but for which the test loss\\nis lower bounded as:\\nEt\\x18f\"\\nå\\nt`t#\\n|{z}\\nloss of learned policy\\x15T+1\\n2\\x001\\n4eh\\n1\\x00(1\\x002e)T+1i\\n(18.3)\\nwhich is bounded by T2eand, for small e, grows like T2e.216 a course in machine learning\\nUp to constants, this gives matching upper and lower bounds for\\nthe loss of a policy learned by supervised imitation learning that is\\npretty far (a factor of T) from what we might hope for.\\n18.3Dataset Aggregation\\nSupervised imitation learning fails because once it gets “off the ex-\\npert path,” things can go really badly. Ideally, we might want to train\\nour policy to deal with anypossible situation it could encounter.\\nUnfortunately, this is unrealistic: we cannot hope to be able to train\\non every possible conﬁguration of the world; and if we could, we\\nwouldn’t really need to learn anyway, we could just memorize. So\\nwe want to train fon a subset of world conﬁgurations, but using\\n“conﬁgurations visited by the expert” fails because fcannot learn to\\nrecover from its own errors. Somehow what we’d like to do is train f\\nto do well on the conﬁgurations that it, itself, encounters!\\nThis is a classic chicken-and-egg problem. We want a policy fthat\\ndoes well in a bunch of world conﬁgurations. What set of conﬁgura-\\ntions? The conﬁgurations that fencounters! A very classic approach\\nto solving chicken-and-egg problems is iteration. Start with some\\npolicy f. Run fand see what conﬁgurations is visits. Train a new f\\nto do well there. Repeat.\\nThis is exactly what the Dataset Aggregation algorithm (“Dagger”)\\ndoes. Continuing with the self-driving car analogy, we ﬁrst let a\\nhuman expert drive a car for a while, and learn an initial policy f0by\\nrunning standard supervised imitation learning (Algorithm 18.1) on\\nthe trajectories visited by the human. We then do something unusual.\\nWe put the human expert in the car, and record their actions, but the\\ncar behaves not according to the expert’s behavior, but according to\\nf0. That is, f0is in control of the car, and the expert is trying to steer,\\nbut the car is ignoring them4and simply recording their actions as4This is possibly terrifying for the\\nexpert!training data. This is shown in Figure 18.2.\\nexpertexpertff00\\nFigure 18.2: In DAgger, the trajectory\\n(red) is generated according to the\\npreviously learned policy, f0, but the\\ngold standard actions are given by the\\nexpert.Based on trajectories generated by f0but actions given by the\\nexpert, we generate a new dataset that contains information about\\nhow to recover from the errors of f0. We now will train a new policy,\\nf1. Because we don’t want f1to “forget” what f0already knows, f1\\nis trained on the union of the initial expert-only trajectories together\\nwith the new trajectories generated by f0. We repeat this process a\\nnumber of times MaxIter, yielding Algorithm 18.3.\\nThis algorithm returns the list of allpolicies generated during its\\nrun. A very practical question is: which one should you use? There\\nare essentially two choices. The ﬁrst choice is just to use the ﬁnal\\npolicy learned. The problem with this approach is that Dagger can\\nbe somewhat unstable in practice, and policies do not monotonicallyimitation learning 217\\nAlgorithm 45Dagger Train (A, MaxIter, N, expert)\\n1:ht(0)\\nniN\\nn=1 run the expert Nmany times\\n2:D0 \\n(x,a):8n,8(x,a,`)2t(0)\\nn\\x0b\\n// collect all pairs (same as supervised)\\n3:f0 A(D0) // train initial policy (multiclass classiﬁer) on D0\\n4:fori=1. . .MaxIter do\\n5:ht(i)\\nniN\\nn=1 run policy fi\\x001N-many times // trajectories by fi\\x001\\n6:Di \\n(x, expert (x)):8n,8(x,a,`)2t(i)\\nn\\x0b\\n// collect data set\\n// observations xvisited by fi\\x001\\n// but actions according to the expert\\n7: fi A\\x10Si\\nj=0Dj\\x11\\n// train policy fion union of all data so far\\n8:end for\\n9:returnhf0,f1, . . . , fMaxIteri // return collection of all learned policies\\nimprove. A safer alternative (as we’ll see by theory below) is to test\\nall of them on some held-out “development” tasks, and pick the one\\nthat does best there. This requires a bit more computation, but is a\\nmuch better approach in general.\\nOne major difference in requirements between Dagger (Algo-\\nrithm 18.3) and SupervisedIL (Algorithm 18.1) is the requirement\\nof interaction with the expert. In SupervisedIL, you only need access\\nto a bunch of trajectories taken by the expert, passively . In Dagger,\\nyou need access to them expert themselves, so you can ask questions\\nlike “if you saw conﬁguration x, what would you do?” This puts\\nmuch more demand on the expert.\\nAnother question that arises is: what should N, the number of\\ntrajectories generated in each round, be? In practice, the initial N\\nshould probably be reasonably large, so that the initial policy f0\\nis pretty good. The number of trajectories generated by iteration\\nsubsequently can be much smaller, perhaps even just one.\\nIntuitively, Dagger should be less sensitive to compounding error\\nthan SupervisedIL, precisely because it gets trained on observations\\nthat it is likely to see at test time. This is formalized in the following\\ntheorem:\\nTheorem 20(Loss of Dagger) .Suppose that one runs Algorithm 18.3\\nusing a multiclass classiﬁer that optimizes the 0-1loss (or an upperbound\\nthereof). Let ebe the error rate of the underlying classiﬁer (in expectation)\\nand assume that all instantaneous losses are in the range [0,`(max)]. Let f be\\nthe learned policy; then:\\nEt\\x18f\"\\nå\\nt`t#\\n|{z}\\nloss of learned policy\\x14Et\\x18expert\"\\nå\\nt`t#\\n|{z}\\nloss of expert+`(max)Te+O\\x12`(max)TlogT\\nMaxIter\\x13\\n(18.4)\\nFurthermore, if the loss function is strongly convex in f , and MaxIter is218 a course in machine learning\\n˜O(T/e), then:\\nEt\\x18f\"\\nå\\nt`t#\\n|{z}\\nloss of learned policy\\x14Et\\x18expert\"\\nå\\nt`t#\\n|{z}\\nloss of expert+`(max)Te+O(e) (18.5)\\nBoth of these results show that, assuming MaxIter is large enough,\\nthe loss of the learned policy f(here, taken to be the best on of all\\nthe MaxIter policies learned) grows like Te, which is what we hope\\nfor. Note that the ﬁnal term in the ﬁrst bound gets small so long as\\nMaxIter is at least TlogT.\\n18.4Expensive Algorithms as Experts\\nBecause of the strong requirement on the expert in Dagger (i.e., that\\nyou need to be able to query it many times during training), one of\\nthe most substantial use cases for Dagger is to learn to (quickly) imi-\\ntate otherwise slow algorithms. Here are two prototypical examples:\\n1. Game playing. When a game (like chess or minecraft) can be run\\nin simulation, you can often explicitly compute a semi-optimal\\nexpert behavior with brute-force search. But this search might be\\ntoo computationally expensive to play in real time, so you can\\nuse it during training time, learning a fast policy that attempts\\nto mimic the expensive search. This learned policy can then be\\napplied at test time.\\n2. Discrete optimizers. Many discrete optimization problems can be\\ncomputationally expensive to run in real time; for instance, even\\nshortest path search on a large graph can be too slow for real time\\nuse. We can compute shortest paths ofﬂine as “training data” and\\nthen use imitation learning to try to build shortest path optimizers\\nthat will run sufﬁciently efﬁciently in real time.\\nConsider the game playing example, and for concreteness, sup-\\npose you are trying to learn to play solitaire (this is an easier exam-\\nple because it’s a single player game). When running DaggerTrain\\n(Algorithm 18.3to learn a chess-playing policy, the algorithm will\\nrepeatedly ask for expert (x), where xis the current state of the game.\\nWhat should this function return? Ideally, it should return the/an ac-\\ntion asuch that, if ais taken, and then the rest of the game is played\\noptimally, the player wins. Computing this exactly is going to be very\\ndifﬁcult for anything except the simplest games, so we need to restort\\nto an approxiamtion.imitation learning 219\\nAlgorithm 46Depth Limited DFS( x,h, MaxDepth)\\n1:ifxis a terminal state or MaxDepth \\x140then\\n2:return (?,h(x)) // if we cannot search deeper\\n// return “no action” ( ?) and the current heuristic score\\n3:else\\n4:BestAction, BestScore  ? ,\\x00¥ // keep track of best action & its score\\n5:for all actions afrom xdo\\n6: (_, score) Depth Limited DFS(x\\x0ea,h, MaxDepth\\x001)// get score\\n// for action a, depth reduced by one by appending atox\\n7: ifscore>BestScore then\\n8: BestAction, BestScore  a,score // update tracked best action & score\\n9: end if\\n10:end for\\n11:end if\\n12:return (BestAction, BestScore ) // return best found action and its score\\nFigure 18.3:imit:dldfs : Depth limited\\ndepth-ﬁrst searchA common strategy is to run a depth-limited depth ﬁrst search,\\nstarting at state x, and terminating after at most three of four moves\\n(see Figure 18.3). This will generate a search tree. Unless you are\\nvery near the end of the game, none of the leaves of this tree will\\ncorrespond to the end of the game. So you’ll need some heuristic, h,\\nfor evaluating states that are non-terminals. You can propagate this\\nheuristic score up to the root, and choose the action that looks best\\nwith this depth four search. This is not necessarily going to be the\\noptimal action, and there’s a speed/accuracy trade-off for searching\\ndeeper, but this is typically effective. This approach summarized in\\nAlgorithm 18.4.\\n18.5Structured Prediction via Imitation Learning\\nA ﬁnal case where an expert can often be computed algorithmically\\narises when one solves structured prediction (see Chapter 17) via\\nimitation learning. It is clearest how this can work in the case of\\nsequence labeling. Recall there that predicted outputs should be\\nsequences of labels. The running example from the earlier chapter\\nwas:\\nx=“ monsters eat tasty bunnies “ ( 18.6)\\ny= noun verb adj noun ( 18.7)\\nOne can easily cast the prediction of yas a sequential decision mak-\\ning problem, by treating the production of yin a left-to-right manner.\\nIn this case, we have a time horizon T=4. We want to learn a policy\\nfthat ﬁrst predicts “noun” then “verb” then “adj” then “noun” on\\nthis input.220 a course in machine learning\\nLet’s suppose that the input to fconsists of features extracted both\\nfrom the input ( x)and the current predicted output preﬁx ˆ y, denoted\\nf(x, ˆy). For instance, f(x, ˆy)might represent a similar set of features\\nto those use in Chapter 17. It is perhaps easiest to think of fas just\\na classiﬁer: given some features of the input sentence x(“monsters\\neat tasty bunnies”), and some features about previous predictions in\\nthe output preﬁx (so far, produced “noun verb”), the goal of fis to\\npredict the tag for the next word (“tasty”) in this context.\\nAn important question is: what is the “expert” in this case? In-\\ntuitively, the expert should provide the correct next label, but what\\ndoes this mean? That depends on the loss function being optimized.\\nUnder Hamming loss (sum zero/one loss over each individual pre-\\ndiction), the expert is straightforward. When the expert is asked to\\nproduce an action for the third word, the expert’s response is always\\n“adj” (or whatever happens to be the correct label for the third word\\nin the sentence it is currently training on).\\nMore generally, the expert gets to look at x,yand a preﬁx ˆ yof the\\noutput. Note, importantly , that the preﬁx might be wrong! In particular,\\nafter the ﬁrst iteration of Dagger, the preﬁx will be predicted by\\nthe learned policy, which may make mistakes! The expert also has\\nsome structured loss function `that it is trying to minimize. Like\\nin the previous section, the expert’s goal is to choose the action that\\nminimizes the long-term loss according to `on this example.\\nTo be more formal, we need a bit of notation. Let best (`,y, ˆy)\\ndenote the loss (according to `and the ground truth y) of the best\\nreachable output starting at ˆ y. For instance, if yis “noun verb adj\\nnoun” and ˆ yis “noun noun”, and the loss is Hamming loss, then the\\nbest achievable output (predicting left-to-right) is “noun noun adj\\nnoun” which has a loss of 1. Thus, best for this situation is 1.\\nGiven that notion of best, the expert is easy to deﬁne:\\nexpert (`,y, ˆy) =argmin\\nabest(`,y, ˆy\\x0ea) (18.8)\\nNamely, it is the action that leads to the best possible completion\\nafter taking that action. So in the example above, the expert action\\nis “adj”. For some problems and some loss functions, computing\\nthe expert is easy. In particular, for sequence labeling under Ham-\\nming loss, it’s trivial. In the case that you can compute the expert\\nexactly, it is often called an oracle.5For some other problems, exactly5Some literature calls it a “dynamic\\noracle”, though the extra word is\\nunnecessary.computing an oracle is computationally expensive or intractable. In\\nthose cases, one can often resort to depth limited depth-ﬁrst-search\\n(Algorithm 18.4) to compute an approximate oracle as an expert.\\nTo be very concrete, a typical implementation of Dagger applied\\nto sequence labeling would go as follows. Each structured training\\nexample (a pair of sentence and tag-sequence) gives rise to one trajec-imitation learning 221\\ntory. At training time, a predict tag seqence is generated left-to-right,\\nstarting with the empty sequence. At any given time step, you are\\nattempting to predict the label of the tth word in the input. You de-\\nﬁne a feature vector f(x, ˆy), which will typically consist of: (a) the tth\\nword, (b) left and right neighbors of the tth word, (c) the last few pre-\\ndictions in ˆ y, and (d) anything else you can think of. In particular, the\\nfeatures are notlimited to Markov style features, because we’re not\\nlonger trying to do dynamic programming. The expert label for the\\ntth word is just the corresponding label in the ground truth y. Given\\nall this, one can run Dagger (Algorithm 18.4) exactly as speciﬁed.\\nMoving to structured prediction problems other than sequence\\nlabeling problems is beyond the scope of this book. The general\\nframework is to cast your structured prediction problem as a sequen-\\ntial decision making problem. Once you’ve done that, you need to\\ndecide on features (this is the easy part) and an expert (this is often\\nthe harder part). However, once you’ve done so, there are generic\\nlibraries for “compiling” your speciﬁcation down to code.\\n18.6Further Reading\\nTODO further readingCODE AND DATASETS\\nRating Easy? AI? Sys? Thy? Morning?\\n+2 y y n y n\\n+2 y y n y n\\n+2 n y n n n\\n+2 n n n y n\\n+2 n y y n y\\n+1 y y n n n\\n+1 y y n y n\\n+1 n y n y n\\n0 n n n n y\\n0 y n n y y\\n0 n y n y n\\n0 y y y y y\\n-1 y y y n y\\n-1 n n y y n\\n-1 n n y n y\\n-1 y n y n y\\n-2 n n y y n\\n-2 n y y n y\\n-2 y n y n n\\n-2 y n y n y Table 1: Course rating data setBIBLIOGRAPHY\\nShai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira.\\nAnalysis of representations for domain adaptation. Advances in\\nneural information processing systems ,19:137,2007 .\\nSteffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative\\nlearning for differing training and test distributions. In Proceedings\\nof the International Conference on Machine Learning (ICML) ,2007 .\\nSergey Brin. Near neighbor search in large metric spaces. In Confer-\\nence on Very Large Databases (VLDB) ,1995 .\\nHal Daumé III. Frustratingly easy domain adaptation. In Conference\\nof the Association for Computational Linguistics (ACL) , Prague, Czech\\nRepublic, 2007 .\\nSorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasub-\\nramanian. On the (im)possibility of fairness. arXiv preprint\\narXiv: 1609 .07236 ,2016 .\\nMoritz Hardt, Eric Price, and Nathan Srebro. Equality of oppor-\\ntunity in supervised learning. In Advances in Neural Information\\nProcessing Systems , pages 3315 –3323 ,2016 .\\nMatti Kääriäinen. Lower bounds for reductions. Talk at the Atomic\\nLearning Workshop (TTI-C), March 2006 .\\nTom M. Mitchell. Machine Learning . McGraw Hill, 1997 .\\nJ. Ross Quinlan. Induction of decision trees. Machine learning ,1(1):\\n81–106,1986 .\\nFrank Rosenblatt. The perceptron: A probabilistic model for infor-\\nmation storage and organization in the brain. Psychological Review ,\\n65:386–408,1958 . Reprinted in Neurocomputing (MIT Press, 1998 ).\\nStéphane Ross, Geoff J. Gordon, and J. Andrew Bagnell. A reduction\\nof imitation learning and structured prediction to no-regret online\\nlearning. In Proceedings of the Workshop on Artiﬁcial Intelligence and\\nStatistics (AIStats) ,2011 .224 a course in machine learningINDEX\\nK-nearest neighbors, 58\\ndA-distance, 113\\np-norms, 92\\n0/1loss, 88\\n80% rule, 111\\nabsolute loss, 14\\nactivation function, 130\\nactivations, 41\\nAdaBoost, 166\\nadaptation, 105\\nalgorithm, 87\\nall pairs, 80\\nall versus all, 80\\napproximation error, 71\\narchitecture selection, 139\\narea under the curve, 64,84\\nargmax problem, 199\\nAUC, 64,83,84\\nAVA, 80\\naveraged perceptron, 52\\nback-propagation, 134,137\\nbag of words, 56\\nbagging, 165\\nbase learner, 164\\nbatch, 173\\nBayes error rate, 20\\nBayes optimal classiﬁer, 19\\nBayes optimal error rate, 20\\nBayes rule, 117\\nBernouilli distribution, 121\\nbias, 42\\nbias/variance trade-off, 72\\nbinary features, 30\\nbipartite ranking problems, 83\\nboosting, 155,164\\nbootstrap resampling, 165\\nbootstrapping, 67,69categorical features, 30\\nchain rule, 117,120\\nchord, 90\\ncircuit complexity, 138\\nclustering, 35,178\\nclustering quality, 178\\ncomplexity, 34\\ncompounding error, 215\\nconcave, 90\\nconcavity, 193\\nconcept, 157\\nconﬁdence intervals, 68\\nconstrained optimization problem,\\n100\\ncontour, 92\\nconvergence rate, 95\\nconvex, 87,89\\ncovariate shift, 105\\ncross validation, 65,68\\ncubic feature map, 144\\ncurvature, 95\\ndata covariance matrix, 184\\ndata generating distribution, 15\\ndecision boundary, 34\\ndecision stump, 168\\ndecision tree, 8,10\\ndecision trees, 57\\ndensity estimation, 107\\ndevelopment data, 26\\ndimensionality reduction, 178\\ndiscrepancy, 113\\ndiscrete distribution, 121\\ndisparate impact, 111\\ndistance, 31\\ndomain adaptation, 105\\ndominates, 63\\ndot product, 45\\ndual problem, 151dual variables, 151\\nearly stopping, 53,132\\nembedding, 178\\nensemble, 164\\nerror driven, 43\\nerror rate, 88\\nestimation error, 71\\nEuclidean distance, 31\\nevidence, 127\\nexample normalization, 59,60\\nexamples, 9\\nexpectation maximization, 186,189\\nexpected loss, 16\\nexpert, 212\\nexponential loss, 90,169\\nfeasible region, 101\\nfeature augmentation, 109\\nfeature combinations, 54\\nfeature mapping, 54\\nfeature normalization, 59\\nfeature scale, 33\\nfeature space, 31\\nfeature values, 11,29\\nfeature vector, 29,31\\nfeatures, 11,29\\nforward-propagation, 137\\nfractional assignments, 191\\nfurthest-ﬁrst heuristic, 180\\nGaussian distribution, 121\\nGaussian kernel, 147\\nGaussian Mixture Models, 191\\ngeneralize, 9,17\\ngenerative story, 123\\ngeometric view, 29\\nglobal minimum, 94\\nGMM, 191226 a course in machine learning\\ngradient, 93\\ngradient ascent, 93\\ngradient descent, 93\\nHamming loss, 202\\nhard-margin SVM, 101\\nhash kernel, 177\\nheld-out data, 26\\nhidden units, 129\\nhidden variables, 186\\nhinge loss, 90,203\\nhistogram, 12\\nhorizon, 213\\nhyperbolic tangent, 130\\nhypercube, 38\\nhyperparameter, 26,44,89\\nhyperplane, 41\\nhyperspheres, 38\\nhypothesis, 71,157\\nhypothesis class, 71,160\\nhypothesis testing, 67\\ni.i.d. assumption, 117\\nidentically distributed, 24\\nILP ,195,207\\nimbalanced data, 73\\nimitation learning, 212\\nimportance sampling, 106\\nimportance weight, 74\\nindependent, 24\\nindependently, 117\\nindependently and identically dis-\\ntributed, 117\\nindicator function, 88\\ninduce, 16\\ninduced distribution, 76\\ninduction, 9\\ninductive bias, 20,31,33,91,121\\ninteger linear program, 207\\ninteger linear programming, 195\\niteration, 36\\njack-kniﬁng, 69\\nJensen’s inequality, 193\\njoint, 124\\nK-nearest neighbors, 32\\nKarush-Kuhn-Tucker conditions, 152\\nkernel, 141,145\\nkernel trick, 146\\nkernels, 54KKT conditions, 152\\nlabel, 11\\nLagrange multipliers, 119\\nLagrange variable, 119\\nLagrangian, 119\\nlattice, 200\\nlayer-wise, 139\\nlearning by demonstration, 212\\nleave-one-out cross validation, 65\\nlevel-set, 92\\nlicense, 2\\nlikelihood, 127\\nlinear classiﬁer, 169\\nlinear classiﬁers, 169\\nlinear decision boundary, 41\\nlinear regression, 98\\nlinearly separable, 48\\nlink function, 130\\nlog likelihood, 118\\nlog posterior, 127\\nlog probability, 118\\nlog-likelihood ratio, 122\\nlogarithmic transformation, 61\\nlogistic loss, 90\\nlogistic regression, 126\\nLOO cross validation, 65\\nloss function, 14\\nloss-augmented inference, 205\\nloss-augmented search, 205\\nmargin, 49,100\\nmargin of a data set, 49\\nmarginal likelihood, 127\\nmarginalization, 117\\nMarkov features, 198\\nmaximum a posteriori, 127\\nmaximum depth, 26\\nmaximum likelihood estimation, 118\\nmean, 59\\nMercer’s condition, 146\\nmodel, 87\\nmodeling, 25\\nmulti-layer network, 129\\nnaive Bayes assumption, 120\\nnearest neighbor, 29,31\\nneural network, 169\\nneural networks, 54,129\\nneurons, 41\\nnoise, 21non-convex, 135\\nnon-linear, 129\\nNormal distribution, 121\\nnormalize, 46,59\\nnull hypothesis, 67\\nobjective function, 88\\none versus all, 78\\none versus rest, 78\\nonline, 42\\noptimization problem, 88\\noracle, 212,220\\noracle experiment, 28\\noutput unit, 129\\nOVA, 78\\noverﬁtting, 23\\noversample, 76\\np-value, 67\\nPAC, 156,166\\npaired t-test, 67\\nparametric test, 67\\nparity, 21\\nparity function, 138\\npatch representation, 56\\nPCA, 184\\nperceptron, 41,42,58\\nperpendicular, 45\\npixel representation, 55\\npolicy, 212\\npolynomial kernels, 146\\npositive semi-deﬁnite, 146\\nposterior, 127\\nprecision, 62\\nprecision/recall curves, 63\\npredict, 9\\npreference function, 82\\nprimal variables, 151\\nprinciple components analysis, 184\\nprior, 127\\nprobabilistic modeling, 116\\nProbably Approximately Correct, 156\\nprogramming by example, 212\\nprojected gradient, 151\\nprojection, 46\\npsd, 146\\nradial basis function, 139\\nrandom forests, 169\\nrandom variable, 117\\nRBF kernel, 147index 227\\nRBF network, 139\\nrecall, 62\\nreceiver operating characteristic, 64\\nreconstruction error, 184\\nreductions, 76\\nredundant features, 56\\nregularized objective, 89\\nregularizer, 88,91\\nreinforcement learning, 212\\nrepresenter theorem, 143,145\\nROC curve, 64\\nsample complexity, 157,158,160\\nsample mean, 59\\nsample selection bias, 105\\nsample variance, 59\\nsemi-supervised adaptation, 106\\nsensitivity, 64\\nseparating hyperplane, 87\\nsequential decision making, 212\\nSGD, 173\\nshallow decision tree, 21,168\\nshape representation, 56\\nsigmoid, 130\\nsigmoid function, 126\\nsigmoid network, 139\\nsign, 130\\nsingle-layer network, 129\\nsingular, 98\\nslack, 148\\nslack parameters, 101\\nsmoothed analysis, 180\\nsoft assignments, 190\\nsoft-margin SVM, 101span, 143\\nsparse, 92\\nspeciﬁcity, 64\\nsquared loss, 14,90\\nstatistical inference, 116\\nstatistically signiﬁcant, 67\\nsteepest ascent, 93\\nstochastic gradient descent, 173\\nstochastic optimization, 172\\nstrong law of large numbers, 24\\nstrong learner, 166\\nstrong learning algorithm, 166\\nstrongly convex, 95\\nstructural risk minimization, 87\\nstructured hinge loss, 203\\nstructured prediction, 195\\nsub-sampling, 75\\nsubderivative, 96\\nsubgradient, 96\\nsubgradient descent, 97\\nsum-to-one, 117\\nsupport vector machine, 100\\nsupport vectors, 153\\nsurrogate loss, 90\\nsymmetric modes, 135\\nt-test, 67\\ntest data, 25\\ntest error, 25\\ntest set, 9\\ntext categorization, 56\\nthe curse of dimensionality, 37\\nthreshold, 42\\nTikhonov regularization, 87time horizon, 213\\ntotal variation distance, 113\\ntrain/test mismatch, 105\\ntraining data, 9,16,24\\ntraining error, 16\\ntrajectory, 213\\ntrellis, 200\\ntrucated gradients, 175\\ntwo-layer network, 129\\nunary features, 198\\nunbiased, 47\\nunderﬁtting, 23\\nunit hypercube, 39\\nunit vector, 46\\nunsupervised adaptation, 106\\nunsupervised learning, 35\\nvalidation data, 26\\nVapnik-Chernovenkis dimension, 162\\nvariance, 59,165\\nvariational distance, 113\\nVC dimension, 162\\nvector, 31\\nvisualize, 178\\nvote, 32\\nvoted perceptron, 52\\nvoting, 52\\nweak learner, 166\\nweak learning algorithm, 166\\nweights, 41\\nzero/one loss, 14'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing cassion using the ASTRA DB(cassandra) credentials\n",
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
      ],
      "metadata": {
        "id": "TIGC1K_5sBz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a LLM and embeddings\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
        "             model=\"gpt-3.5-turbo\",\n",
        "             temperature=0,\n",
        "             max_tokens=256,\n",
        "             top_p=1\n",
        "             )\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "nWhX-jaZsTvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CReating a vector store to store the pdf file\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_ml1\",\n",
        "    keyspace=None,\n",
        "    session=None\n",
        ")"
      ],
      "metadata": {
        "id": "THbTWJIes2y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating small chuncks of the pdf\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "Mjaf5aLYtqkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f_Eo7v3quHGM",
        "outputId": "3f46d500-2409-4eec-db54-79ab9104cb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A Course in\\nMachine Learning\\nHal Daumé IIICopyright © 2013 –2017 Hal Daumé III\\nSelf-published\\nhttp://ciml.info/\\nTODO. . . .\\nSecond printing, January 2017For my students and teachers.\\nOften the same.TABLE OF CONTENTS\\nAbout this Book 6\\n1 Decision Trees 8\\n2 Limits of Learning 19\\n3 Geometry and Nearest Neighbors 29\\n4 The Perceptron 41\\n5 Practical Issues 55\\n6 Beyond Binary Classification 73\\n7 Linear Models 87\\n8 Bias and Fairness 104\\n9 Probabilistic Modeling 116\\n10 Neural Networks 1295\\n11 Kernel Methods 141\\n12 Learning Theory 154\\n13 Ensemble Methods 164\\n14 Efficient Learning 171\\n15 Unsupervised Learning 178\\n16 Expectation Maximization 186\\n17 Structured Prediction 195\\n18 Imitation Learning 212\\nCode and Datasets 222\\nBibliography 223\\nIndex 225ABOUT THIS BOOK\\nMachine learning is a broad and fascinating field . Even\\ntoday, machine learning technology runs a substantial part of your\\nlife, often without you knowing it. Any plausible approach to artiﬁ-',\n",
              " 'Machine learning is a broad and fascinating field . Even\\ntoday, machine learning technology runs a substantial part of your\\nlife, often without you knowing it. Any plausible approach to artiﬁ-\\ncial intelligence must involve learning, at some level, if for no other\\nreason than it’s hard to call a system intelligent if it cannot learn.\\nMachine learning is also fascinating in its own right for the philo-\\nsophical questions it raises about what it means to learn and succeed\\nat tasks.\\nMachine learning is also a very broad ﬁeld, and attempting to\\ncover everything would be a pedagogical disaster. It is also so quickly\\nmoving that any book that attempts to cover the latest developments\\nwill be outdated before it gets online. Thus, this book has two goals.\\nFirst, to be a gentle introduction to what is a very deep ﬁeld. Second,\\nto provide readers with the skills necessary to pick up new technol-\\nogy as it is developed.\\n0.1How to Use this Book',\n",
              " 'First, to be a gentle introduction to what is a very deep ﬁeld. Second,\\nto provide readers with the skills necessary to pick up new technol-\\nogy as it is developed.\\n0.1How to Use this Book\\nThis book is designed to be read linearly, since it’s goal is not to be\\na generic reference. That said, once you get through chapter 5, you\\ncan pretty much jump anywhere. When I teach a one-semester un-\\ndergraduate course, I typically cover the chapter 1-13, sometimes\\nskipping 7or9or10or12depending on time and interest. For a\\ngraduate course for students with no prior machine learning back-\\nground, I would very quickly blaze through 1-4, then cover the rest,\\naugmented with some additional reading.\\n0.2Why Another Textbook?\\nThe purpose of this book is to provide a gentle and pedagogically orga-\\nnized introduction to the ﬁeld. This is in contrast to most existing ma-\\nchine learning texts, which tend to organize things topically, rather7',\n",
              " 'nized introduction to the ﬁeld. This is in contrast to most existing ma-\\nchine learning texts, which tend to organize things topically, rather7\\nthan pedagogically (an exception is Mitchell’s book1, but unfortu-1Mitchell 1997\\nnately that is getting more and more outdated). This makes sense for\\nresearchers in the ﬁeld, but less sense for learners. A second goal of\\nthis book is to provide a view of machine learning that focuses on\\nideas and models, not on math. It is not possible (or even advisable)\\nto avoid math. But math should be there to aidunderstanding, not\\nhinder it. Finally, this book attempts to have minimal dependencies,\\nso that one can fairly easily pick and choose chapters to read. When\\ndependencies exist, they are listed at the start of the chapter.\\nThe audience of this book is anyone who knows differential calcu-\\nlus and discrete math, and can program reasonably well. (A little bit\\nof linear algebra and probability will not hurt.) An undergraduate in',\n",
              " 'lus and discrete math, and can program reasonably well. (A little bit\\nof linear algebra and probability will not hurt.) An undergraduate in\\ntheir fourth or ﬁfth semester should be fully capable of understand-\\ning this material. However, it should also be suitable for ﬁrst year\\ngraduate students, perhaps at a slightly faster pace.\\n0.3Organization and Auxilary Material\\nThere is an associated web page, http://ciml.info/ , which contains\\nan online copy of this book, as well as associated code and data. It\\nalso contains errata. Please submit bug reports on github: github.com/\\nhal3/ciml .\\n0.4Acknowledgements\\nAcknowledgements: I am indebted to many people for this book.\\nMy teachers, especially Rami Grossberg (from whom the title of this\\nbook was borrowed) and Stefan Schaal. Students who have taken\\nmachine learning from me over the past ten years, including those\\nwho suffered through the initial versions of the class before I ﬁgured',\n",
              " 'book was borrowed) and Stefan Schaal. Students who have taken\\nmachine learning from me over the past ten years, including those\\nwho suffered through the initial versions of the class before I ﬁgured\\nout how to teach it. Especially Scott Alfeld, Josh de Bever, Cecily\\nHeiner, Jeffrey Ferraro, Seth Juarez, John Moeller, JT Olds, Piyush\\nRai. People who have helped me edit, and who have submitted bug\\nreports, including TODO. . . , but also check github for the latest list\\nof contributors!1 | D ECISION TREES\\nDependencies: None.At a basic level , machine learning is about predicting the future\\nbased on the past. For instance, you might wish to predict how much\\na user Alice will like a movie that she hasn’t seen, based on her rat-\\nings of movies that she has seen. This prediction could be based on\\nmany factors of the movies: their category (drama, documentary,\\netc.), the language, the director and actors, the production company,',\n",
              " 'many factors of the movies: their category (drama, documentary,\\netc.), the language, the director and actors, the production company,\\netc. In general, this means making informed guesses about some un-\\nobserved property of some object, based on observed properties of\\nthat object.\\nThe ﬁrst question we’ll ask is: what does it mean to learn? In\\norder to develop learning machines, we must know what learning\\nactually means, and how to determine success (or failure). You’ll see\\nthis question answered in a very limited learning setting, which will\\nbe progressively loosened and adapted throughout the rest of this\\nbook. For concreteness, our focus will be on a very simple model of\\nlearning called a decision tree.\\n1.1What Does it Mean to Learn?\\nAlice has just begun taking a course on machine learning. She knows\\nthat at the end of the course, she will be expected to have “learned”\\nall about this topic. A common way of gauging whether or not she',\n",
              " 'that at the end of the course, she will be expected to have “learned”\\nall about this topic. A common way of gauging whether or not she\\nhas learned is for her teacher, Bob, to give her a exam. She has done\\nwell at learning if she does well on the exam.\\nBut what makes a reasonable exam? If Bob spends the entire\\nsemester talking about machine learning, and then gives Alice an\\nexam on History of Pottery, then Alice’s performance on this exam\\nwill notbe representative of her learning. On the other hand, if the\\nexam only asks questions that Bob has answered exactly during lec-\\ntures, then this is also a bad test of Alice’s learning, especially if it’s\\nan “open notes” exam. What is desired is that Alice observes speciﬁc\\nexamples from the course, and then has to answer new, but related\\nquestions on the exam. This tests whether Alice has the ability toLearning Objectives:\\n• Explain the difference between\\nmemorization and generalization.\\n• Implement a decision tree classiﬁer.',\n",
              " 'questions on the exam. This tests whether Alice has the ability toLearning Objectives:\\n• Explain the difference between\\nmemorization and generalization.\\n• Implement a decision tree classiﬁer.\\n• Take a concrete task and cast it as a\\nlearning problem, with a formal no-\\ntion of input space, features, output\\nspace, generating distribution and\\nloss function.The words printed here are concepts.\\nY ou must go through the experiences. – Carl Frederickdecision trees 9\\ngeneralize. Generalization is perhaps the most central concept in\\nmachine learning.\\nAs a concrete example, consider a course recommendation system\\nfor undergraduate computer science students. We have a collection\\nof students and a collection of courses. Each student has taken, and\\nevaluated, a subset of the courses. The evaluation is simply a score\\nfrom\\x002 (terrible) to +2 (awesome). The job of the recommender\\nsystem is to predict how much a particular student (say, Alice) will\\nlike a particular course (say, Algorithms).',\n",
              " 'from\\x002 (terrible) to +2 (awesome). The job of the recommender\\nsystem is to predict how much a particular student (say, Alice) will\\nlike a particular course (say, Algorithms).\\nGiven historical data from course ratings (i.e., the past) we are\\ntrying to predict unseen ratings (i.e., the future). Now, we could\\nbe unfair to this system as well. We could ask it whether Alice is\\nlikely to enjoy the History of Pottery course. This is unfair because\\nthe system has no idea what History of Pottery even is, and has no\\nprior experience with this course. On the other hand, we could ask\\nit how much Alice will like Artiﬁcial Intelligence, which she took\\nlast year and rated as +2 (awesome). We would expect the system to\\npredict that she would really like it, but this isn’t demonstrating that\\nthe system has learned: it’s simply recalling its past experience. In\\nthe former case, we’re expecting the system to generalize beyond its\\nexperience, which is unfair. In the latter case, we’re not expecting it',\n",
              " 'the former case, we’re expecting the system to generalize beyond its\\nexperience, which is unfair. In the latter case, we’re not expecting it\\nto generalize at all.\\nThis general set up of predicting the future based on the past is\\nat the core of most machine learning. The objects that our algorithm\\nwill make predictions about are examples . In the recommender sys-\\ntem setting, an example would be some particular Student/Course\\npair (such as Alice/Algorithms). The desired prediction would be the\\nrating that Alice would give to Algorithms.\\nknown labelstraining\\ndatalearning\\nalgorithm\\nf ?test\\nexample\\npredicted\\nlabel\\nFigure 1.1: The general supervised ap-\\nproach to machine learning: a learning\\nalgorithm reads in training data and\\ncomputes a learned function f. This\\nfunction can then automatically label\\nfuture text examples.To make this concrete, Figure 1.1shows the general framework of\\ninduction. We are given train ingdata on which our algorithm is ex-',\n",
              " 'function can then automatically label\\nfuture text examples.To make this concrete, Figure 1.1shows the general framework of\\ninduction. We are given train ingdata on which our algorithm is ex-\\npected to learn. This training data is the examples that Alice observes\\nin her machine learning course, or the historical ratings data for\\nthe recommender system. Based on this training data, our learning\\nalgorithm induces a function fthat will map a new example to a cor-\\nresponding prediction. For example, our function might guess that\\nf(Alice/Machine Learning )might be high because our training data\\nsaid that Alice liked Artiﬁcial Intelligence. We want our algorithm\\nto be able to make lots of predictions, so we refer to the collection\\nof examples on which we will evaluate our algorithm as the test set.\\nThe test set is a closely guarded secret: it is the ﬁnal exam on which\\nour learning algorithm is being tested. If our algorithm gets to peek',\n",
              " 'The test set is a closely guarded secret: it is the ﬁnal exam on which\\nour learning algorithm is being tested. If our algorithm gets to peek\\nat it ahead of time, it’s going to cheat and do better than it should. Why is it bad if the learning algo-\\nrithm gets to peek at the test data??The goal of inductive machine learning is to take some training\\ndata and use it to induce a function f. This function fwill be evalu-10 a course in machine learning\\nated on the test data. The machine learning algorithm has succeeded\\nif its performance on the test data is high.\\n1.2Some Canonical Learning Problems\\nThere are a large number of typical inductive learning problems.\\nThe primary difference between them is in what type of thing they’re\\ntrying to predict. Here are some examples:\\nRegression: trying to predict a real value. For instance, predict the\\nvalue of a stock tomorrow given its past performance. Or predict\\nAlice’s score on the machine learning ﬁnal exam based on her\\nhomework scores.',\n",
              " 'value of a stock tomorrow given its past performance. Or predict\\nAlice’s score on the machine learning ﬁnal exam based on her\\nhomework scores.\\nBinary Classiﬁcation: trying to predict a simple yes/no response.\\nFor instance, predict whether Alice will enjoy a course or not.\\nOr predict whether a user review of the newest Apple product is\\npositive or negative about the product.\\nMulticlass Classiﬁcation: trying to put an example into one of a num-\\nber of classes. For instance, predict whether a news story is about\\nentertainment, sports, politics, religion, etc. Or predict whether a\\nCS course is Systems, Theory, AI or Other.\\nRanking: trying to put a set of objects in order of relevance. For in-\\nstance, predicting what order to put web pages in, in response to a\\nuser query. Or predict Alice’s ranked preferences over courses she\\nhasn’t taken.\\nFor each of these types of canon-\\nical machine learning problems,\\ncome up with one or two concrete',\n",
              " 'user query. Or predict Alice’s ranked preferences over courses she\\nhasn’t taken.\\nFor each of these types of canon-\\nical machine learning problems,\\ncome up with one or two concrete\\nexamples.?The reason that it is convenient to break machine learning prob-\\nlems down by the type of object that they’re trying to predict has to\\ndo with measuring error. Recall that our goal is to build a system\\nthat can make “good predictions.” This begs the question: what does\\nit mean for a prediction to be “good?” The different types of learning\\nproblems differ in how they deﬁne goodness. For instance, in regres-\\nsion, predicting a stock price that is off by $0.05 is perhaps much\\nbetter than being off by $200.00. The same does not hold of multi-\\nclass classiﬁcation. There, accidentally predicting “entertainment”\\ninstead of “sports” is no better or worse than predicting “politics.”\\n1.3The Decision Tree Model of Learning\\nThe decision tree is a classic and natural model of learning. It is',\n",
              " 'instead of “sports” is no better or worse than predicting “politics.”\\n1.3The Decision Tree Model of Learning\\nThe decision tree is a classic and natural model of learning. It is\\nclosely related to the fundamental computer science notion of “di-\\nvide and conquer.” Although decision trees can be applied to manydecision trees 11\\nlearning problems, we will begin with the simplest case: binary clas-\\nsiﬁcation.\\nSuppose that your goal is to predict whether some unknown user\\nwill enjoy some unknown course. You must simply answer “yes” or\\n“no.” In order to make a guess, you’re allowed to ask binary ques-\\ntions about the user/course under consideration. For example:\\nYou: Is the course under consideration in Systems?\\nMe: Yes\\nYou: Has this student taken any other Systems courses?\\nMe: Yes\\nYou: Has this student liked most previous Systems courses?\\nMe: No\\nYou: I predict this student will not like this course.\\nThe goal in learning is to ﬁgure out what questions to ask, in what',\n",
              " 'Me: Yes\\nYou: Has this student liked most previous Systems courses?\\nMe: No\\nYou: I predict this student will not like this course.\\nThe goal in learning is to ﬁgure out what questions to ask, in what\\norder to ask them, and what answer to predict once you have asked\\nenough questions.\\nisSystems?\\ntakenOtherSys?\\nmorning? likedOtherSys?\\nlike nah nah likelikeno\\nno\\nno noyes\\nyes\\nyes yesFigure 1.2: A decision tree for a course\\nrecommender system, from which the\\nin-text “dialog” is drawn.\\nThe decision tree is so-called because we can write our set of ques-\\ntions and guesses in a tree format, such as that in Figure 1.2. In this\\nﬁgure, the questions are written in the internal tree nodes (rectangles)\\nand the guesses are written in the leaves (ovals). Each non-terminal\\nnode has two children: the left child speciﬁes what to do if the an-\\nswer to the question is “no” and the right child speciﬁes what to do if\\nit is “yes.”\\nIn order to learn, I will give you training data. This data consists',\n",
              " 'swer to the question is “no” and the right child speciﬁes what to do if\\nit is “yes.”\\nIn order to learn, I will give you training data. This data consists\\nof a set of user/course examples, paired with the correct answer for\\nthese examples (did the given user enjoy the given course?). From\\nthis, you must construct your questions. For concreteness, there is a\\nsmall data set in Table 1in the Appendix of this book. This training\\ndata consists of 20course rating examples, with course ratings and\\nanswers to questions that you might ask about this pair. We will\\ninterpret ratings of 0, +1 and +2 as “liked” and ratings of \\x002 and\\x001\\nas “hated.”\\nIn what follows, we will refer to the questions that you can ask as\\nfeatures and the responses to these questions as feature values. The\\nrating is called the label. An example is just a set of feature values.\\nAnd our training data is a set of examples, paired with labels.\\nThere are a lot of logically possible trees that you could build,',\n",
              " 'And our training data is a set of examples, paired with labels.\\nThere are a lot of logically possible trees that you could build,\\neven over just this small number of features (the number is in the\\nmillions). It is computationally infeasible to consider all of these to\\ntry to choose the “best” one. Instead, we will build our decision tree\\ngreedily. We will begin by asking:\\nIf I could only ask one question, what question would I ask?\\noverall:\\neasy:\\nAI:\\nsystems:\\ntheory:60%\\n40%like\\nnah\\n60%\\n40%\\n60%\\n40%yes\\nno\\n82%\\n18%\\n33%\\n67%yes\\nno\\n20%\\n80%\\n100%\\n0%yes\\nno\\n80%\\n20%\\n40%\\n60%yes\\nno\\nFigure 1.3: A histogram of labels for (a)\\nthe entire data set; (b-e) the examples\\nin the data set for each value of the ﬁrst\\nfour features.You want to ﬁnd a feature that is most useful in helping you guess\\nwhether this student will enjoy this course. A useful way to think12 a course in machine learning\\nabout this is to look at the histogram of labels for each feature.1 1A colleague related the story of',\n",
              " 'whether this student will enjoy this course. A useful way to think12 a course in machine learning\\nabout this is to look at the histogram of labels for each feature.1 1A colleague related the story of\\ngetting his 8-year old nephew to\\nguess a number between 1and100.\\nHis nephew’s ﬁrst four questions\\nwere: Is it bigger than 20? (YES) Is\\nit even? (YES) Does it have a 7in it?\\n(NO) Is it 80? (NO). It took 20more\\nquestions to get it, even though 10\\nshould have been sufﬁcient. At 8,\\nthe nephew hadn’t quite ﬁgured out\\nhow to divide and conquer. http:\\n//blog.computationalcomplexity.\\norg/2007/04/\\ngetting-8-year-old-interested-in.\\nhtml .This is shown for the ﬁrst four features in Figure 1.3. Each histogram\\nshows the frequency of “like”/“hate” labels for each possible value\\nof an associated feature. From this ﬁgure, you can see that asking\\nthe ﬁrst feature is not useful: if the value is “no” then it’s hard to\\nguess the label; similarly if the answer is “yes.” On the other hand,',\n",
              " 'the ﬁrst feature is not useful: if the value is “no” then it’s hard to\\nguess the label; similarly if the answer is “yes.” On the other hand,\\nasking the second feature isuseful: if the value is “no,” you can be\\npretty conﬁdent that this student will hate this course; if the answer\\nis “yes,” you can be pretty conﬁdent that this student will like this\\ncourse.\\nMore formally, you will consider each feature in turn. You might\\nconsider the feature “Is this a System’s course?” This feature has two\\npossible value: no and yes. Some of the training examples have an\\nanswer of “no” – let’s call that the “NO” set. Some of the training\\nexamples have an answer of “yes” – let’s call that the “YES” set. For\\neach set (NO and YES) we will build a histogram over the labels.\\nThis is the second histogram in Figure 1.3. Now, suppose you were\\nto ask this question on a random example and observe a value of\\n“no.” Further suppose that you must immediately guess the label for',\n",
              " 'This is the second histogram in Figure 1.3. Now, suppose you were\\nto ask this question on a random example and observe a value of\\n“no.” Further suppose that you must immediately guess the label for\\nthis example. You will guess “like,” because that’s the more preva-\\nlent label in the NO set (actually, it’s the only label in the NO set).\\nAlternatively, if you receive an answer of “yes,” you will guess “hate”\\nbecause that is more prevalent in the YES set.\\nSo, for this single feature, you know what you would guess if you\\nhad to. Now you can ask yourself: if I made that guess on the train-\\ning data, how well would I have done? In particular, how many ex-\\namples would I classify correctly? In the NO set (where you guessed\\n“like”) you would classify all 10 of them correctly. In the YES set\\n(where you guessed “hate”) you would classify 8 (out of 10) of them\\ncorrectly. So overall you would classify 18 (out of 20) correctly. Thus,',\n",
              " '(where you guessed “hate”) you would classify 8 (out of 10) of them\\ncorrectly. So overall you would classify 18 (out of 20) correctly. Thus,\\nwe’ll say that the score of the “Is this a System’s course?” question is\\n18/20. How many training examples\\nwould you classify correctly for\\neach of the other three features\\nfrom Figure 1.3??You will then repeat this computation for each of the available\\nfeatures to us, compute the scores for each of them. When you must\\nchoose which feature consider ﬁrst, you will want to choose the one\\nwith the highest score.\\nBut this only lets you choose the ﬁrst feature to ask about. This\\nis the feature that goes at the root of the decision tree. How do we\\nchoose subsequent features? This is where the notion of divide and\\nconquer comes in. You’ve already decided on your ﬁrst feature: “Is\\nthis a Systems course?” You can now partition the data into two parts:\\nthe NO part and the YES part. The NO part is the subset of the data',\n",
              " 'this a Systems course?” You can now partition the data into two parts:\\nthe NO part and the YES part. The NO part is the subset of the data\\non which value for this feature is “no”; the YES half is the rest. This\\nis the divide step.decision trees 13\\nAlgorithm 1Decision TreeTrain (data,remaining features )\\n1:guess most frequent answer in data // default answer for this data\\n2:ifthe labels in data are unambiguous then\\n3:return Leaf(guess ) // base case: no need to split further\\n4:else if remaining features is empty then\\n5:return Leaf(guess ) // base case: cannot split further\\n6:else // we need to query more features\\n7:for all f2remaining features do\\n8: NO the subset of data on which f=no\\n9: YES the subset of data on which f=yes\\n10: score [f] # of majority vote answers in NO\\n11: + # of majority vote answers in YES\\n// the accuracy we would get if we only queried on f\\n12:end for\\n13:f the feature with maximal score (f)\\n14:NO the subset of data on which f=no',\n",
              " '11: + # of majority vote answers in YES\\n// the accuracy we would get if we only queried on f\\n12:end for\\n13:f the feature with maximal score (f)\\n14:NO the subset of data on which f=no\\n15:YES the subset of data on which f=yes\\n16:left Decision TreeTrain (NO,remaining features nffg)\\n17:right Decision TreeTrain (YES,remaining features nffg)\\n18:return Node(f,left,right )\\n19:end if\\nAlgorithm 2Decision TreeTest(tree,test point )\\n1:iftreeis of the form L eaf(guess )then\\n2:return guess\\n3:else if treeis of the form N ode(f,left,right )then\\n4:iff=nointest point then\\n5: return Decision TreeTest(left,test point )\\n6:else\\n7: return Decision TreeTest(right ,test point )\\n8:end if\\n9:end if\\nThe conquer step is to recurse, and run the same routine (choosing\\nthe feature with the highest score) on the NO set (to get the left half\\nof the tree) and then separately on the YES set (to get the right half of\\nthe tree).\\nAt some point it will become useless to query on additional fea-',\n",
              " 'of the tree) and then separately on the YES set (to get the right half of\\nthe tree).\\nAt some point it will become useless to query on additional fea-\\ntures. For instance, once you know that this is a Systems course,\\nyou know that everyone will hate it. So you can immediately predict\\n“hate” without asking any additional questions. Similarly, at some\\npoint you might have already queried every available feature and still\\nnot whittled down to a single answer. In both cases, you will need to\\ncreate a leaf node and guess the most prevalent answer in the current\\npiece of the training data that you are looking at.\\nPutting this all together, we arrive at the algorithm shown in Al-\\ngorithm 1.3.2This function, D ecision TreeTrain takes two argu-2There are more nuanced algorithms\\nfor building decision trees, some of\\nwhich are discussed in later chapters of\\nthis book. They primarily differ in how\\nthey compute the score function.14 a course in machine learning',\n",
              " 'for building decision trees, some of\\nwhich are discussed in later chapters of\\nthis book. They primarily differ in how\\nthey compute the score function.14 a course in machine learning\\nments: our data, and the set of as-yet unused features. It has two\\nbase cases: either the data is unambiguous, or there are no remaining\\nfeatures. In either case, it returns a L eaf node containing the most\\nlikely guess at this point. Otherwise, it loops over all remaining fea-\\ntures to ﬁnd the one with the highest score. It then partitions the data\\ninto a NO/YES split based on the best feature. It constructs its left\\nand right subtrees by recursing on itself. In each recursive call, it uses\\none of the partitions of the data, and removes the just-selected feature\\nfrom consideration. Is Algorithm 1.3guaranteed to\\nterminate??The corresponding prediction algorithm is shown in Algorithm 1.3.\\nThis function recurses down the decision tree, following the edges',\n",
              " 'from consideration. Is Algorithm 1.3guaranteed to\\nterminate??The corresponding prediction algorithm is shown in Algorithm 1.3.\\nThis function recurses down the decision tree, following the edges\\nspeciﬁed by the feature values in some test point . When it reaches a\\nleaf, it returns the guess associated with that leaf.\\n1.4Formalizing the Learning Problem\\nAs you’ve seen, there are several issues that we must take into ac-\\ncount when formalizing the notion of learning.\\n• The performance of the learning algorithm should be measured on\\nunseen “test” data.\\n• The way in which we measure performance should depend on the\\nproblem we are trying to solve.\\n• There should be a strong relationship between the data that our\\nalgorithm sees at training time and the data it sees at test time.\\nIn order to accomplish this, let’s assume that someone gives us a\\nloss func tion,`(\\x01,\\x01), of two arguments. The job of `is to tell us how\\n“bad” a system’s prediction is in comparison to the truth. In particu-',\n",
              " 'loss func tion,`(\\x01,\\x01), of two arguments. The job of `is to tell us how\\n“bad” a system’s prediction is in comparison to the truth. In particu-\\nlar, if yis the truth and ˆyis the system’s prediction, then `(y,ˆy)is a\\nmeasure of error.\\nFor three of the canonical tasks discussed above, we might use the\\nfollowing loss functions:\\nRegression: squared loss`(y,ˆy) = ( y\\x00ˆy)2\\norabsolute loss`(y,ˆy) =jy\\x00ˆyj.\\nBinary Classiﬁcation: zero/one loss`(y,ˆy) =(\\n0 if y=ˆy\\n1 otherwiseThis notation means that the loss is zero\\nif the prediction is correct and is one\\notherwise.\\nMulticlass Classiﬁcation: also zero/one loss.\\nWhy might it be a bad idea to use\\nzero/one loss to measure perfor-\\nmance for a regression problem??Note that the loss function is something that youmust decide on\\nbased on the goals of learning.\\nNow that we have deﬁned our loss function, we need to consider\\nwhere the data (training andtest) comes from. The model that wedecision trees 15',\n",
              " 'based on the goals of learning.\\nNow that we have deﬁned our loss function, we need to consider\\nwhere the data (training andtest) comes from. The model that wedecision trees 15\\nWe write E(x,y)\\x18D[`(y,f(x))]for the expected loss. Expectation means “average.” This is saying “if you\\ndrew a bunch of (x,y)pairs independently at random from D, what would your average loss be?More\\nformally, ifDis a discrete probability distribution, then this expectation can be expanded as:\\nE(x,y)\\x18D[`(y,f(x))] =å\\n(x,y)2D[D(x,y)`(y,f(x))] (1.1)\\nThis is exactly the weighted average loss over the all (x,y)pairs inD, weighted by their probability,\\nD(x,y). IfDis aﬁnite discrete distribution , for instance deﬁned by a ﬁnite data set f(x1,y1), . . . ,(xN,yN)\\nthat puts equal weight on each example (probability 1/ N), then we get:\\nE(x,y)\\x18D[`(y,f(x))] =å\\n(x,y)2D[D(x,y)`(y,f(x))] deﬁnition of expectation (1.2)\\n=N\\nå\\nn=1[D(xn,yn)`(yn,f(xn))] Dis discrete and ﬁnite (1.3)\\n=N\\nå\\nn=1[1\\nN`(yn,f(xn))] deﬁnition of D (1.4)\\n=1\\nNN\\nå',\n",
              " 'E(x,y)\\x18D[`(y,f(x))] =å\\n(x,y)2D[D(x,y)`(y,f(x))] deﬁnition of expectation (1.2)\\n=N\\nå\\nn=1[D(xn,yn)`(yn,f(xn))] Dis discrete and ﬁnite (1.3)\\n=N\\nå\\nn=1[1\\nN`(yn,f(xn))] deﬁnition of D (1.4)\\n=1\\nNN\\nå\\nn=1[`(yn,f(xn))] rearranging terms (1.5)\\nWhich is exactly the average loss on that dataset.\\nThe most important thing to remember is that there are two equivalent ways to think about expections:\\n(1) The expectation of some function gis the weighted average value of g , where the weights are given by\\nthe underlying probability distribution. ( 2) The expectation of some function gis your best guess of the\\nvalue of g if you were to draw a single item from the underlying probability distribution.MATHREVIEW | EXPECTATED VALUES\\nFigure 1.4:\\nwill use is the probabilistic model of learning. Namely, there is a prob-\\nability distribution Dover input/output pairs. This is often called\\nthedata generatingdistribution. If we write xfor the input (the',\n",
              " 'ability distribution Dover input/output pairs. This is often called\\nthedata generatingdistribution. If we write xfor the input (the\\nuser/course pair) and yfor the output (the rating), then Dis a distri-\\nbution over (x,y)pairs.\\nA useful way to think about Dis that it gives high probability to\\nreasonable (x,y)pairs, and low probability to unreasonable (x,y)\\npairs. A (x,y)pair can be unreasonable in two ways. First, xmight\\nbe an unusual input. For example, a xrelated to an “Intro to Java”\\ncourse might be highly probable; a xrelated to a “Geometric and\\nSolid Modeling” course might be less probable. Second, ymight\\nbe an unusual rating for the paired x. For instance, if Alice were to\\ntake AI 100 times (without remembering that she took it before!),\\nshe would give the course a +2 almost every time. Perhaps some16 a course in machine learning\\nsemesters she might give a slightly lower score, but it would be un-\\nlikely to see x=Alice/AI paired with y=\\x002.',\n",
              " 'semesters she might give a slightly lower score, but it would be un-\\nlikely to see x=Alice/AI paired with y=\\x002.\\nIt is important to remember that we are not making anyassump-\\ntions about what the distribution Dlooks like. (For instance, we’re\\nnot assuming it looks like a Gaussian or some other, common distri-\\nbution.) We are also not assuming that we know what Dis. In fact,\\nif you know a priori what your data generating distribution is, your\\nlearning problem becomes signiﬁcantly easier. Perhaps the hardest\\nthing about machine learning is that we don’t know whatDis: all we\\nget is a random sample from it. This random sample is our training\\ndata.\\nOur learning problem, then, is deﬁned by two quantities: Consider the following prediction\\ntask. Given a paragraph written\\nabout a course, we have to predict\\nwhether the paragraph is a positive\\nornegative review of the course.\\n(This is the sentiment analysis prob-\\nlem.) What is a reasonable loss\\nfunction? How would you deﬁne',\n",
              " 'whether the paragraph is a positive\\nornegative review of the course.\\n(This is the sentiment analysis prob-\\nlem.) What is a reasonable loss\\nfunction? How would you deﬁne\\nthe data generating distribution??1. The loss function `, which captures our notion of what is important\\nto learn.\\n2. The data generating distribution D, which deﬁnes what sort of\\ndata we expect to see.\\nWe are given access to train ingdata , which is a random sample of\\ninput/output pairs drawn from D. Based on this training data, we\\nneed to induce a function fthat maps new inputs ˆ xto corresponding\\nprediction ˆy. The key property that fshould obey is that it should do\\nwell (as measured by `) on future examples that are alsodrawn from\\nD. Formally, it’s expected loss eoverDwith repsect to `should be\\nas small as possible:\\ne,E(x,y)\\x18D\\x02`(y,f(x))\\x03=å\\n(x,y)D(x,y)`(y,f(x)) (1.6)\\nThe difﬁculty in minimizing our expected loss from Eq ( 1.6) is\\nthat we don’t know whatDis!All we have access to is some training',\n",
              " 'e,E(x,y)\\x18D\\x02`(y,f(x))\\x03=å\\n(x,y)D(x,y)`(y,f(x)) (1.6)\\nThe difﬁculty in minimizing our expected loss from Eq ( 1.6) is\\nthat we don’t know whatDis!All we have access to is some training\\ndata sampled from it! Suppose that we denote our training data\\nset by D. The training data consists of N-many input/output pairs,\\n(x1,y1),(x2,y2), . . . ,(xN,yN). Given a learned function f, we can\\ncompute our train ingerror,ˆe:\\nˆe,1\\nNN\\nå\\nn=1`(yn,f(xn)) (1.7)\\nThat is, our training error is simply our average error over the train-\\ning data. Verify by calculation that we\\ncan write our training error as\\nE(x,y)\\x18D\\x02`(y,f(x))\\x03\\n, by thinking\\nofDas a distribution that places\\nprobability 1/ Nto each example in\\nDand probability 0 on everything\\nelse.?Of course, we can drive ˆeto zero by simply memorizing our train-\\ning data. But as Alice might ﬁnd in memorizing past exams, this\\nmight not generalize well to a new exam!\\nThis is the fundamental difﬁculty in machine learning: the thing',\n",
              " 'ing data. But as Alice might ﬁnd in memorizing past exams, this\\nmight not generalize well to a new exam!\\nThis is the fundamental difﬁculty in machine learning: the thing\\nwe have access to is our training error, ˆe. But the thing we care aboutdecision trees 17\\nminimizing is our expected error e. In order to get the expected error\\ndown, our learned function needs to generalizebeyond the training\\ndata to some future data that it might not have seen yet!\\nSo, putting it all together, we get a formal deﬁnition of induction\\nmachine learning: Given (i) a loss function `and (ii) a sample D\\nfrom some unknown distribution D, you must compute a function\\nfthat has low expected error eoverDwith respect to `.\\nA very important comment is that we should never expect a ma-\\nchine learning algorithm to generalize beyond the data distribution\\nit has seen at training time. In a famous—if posssibly apocryphal—\\nexample from the 1970 s, the US Government wanted to train a clas-',\n",
              " 'it has seen at training time. In a famous—if posssibly apocryphal—\\nexample from the 1970 s, the US Government wanted to train a clas-\\nsiﬁer to distinguish between US tanks and Russian tanks. They col-\\nlected a training and test set, and managed to build a classiﬁer with\\nnearly 100% accuracy on that data. But when this classiﬁer was run\\nin the “real world”, it failed miserably. It had not, in fact, learned\\nto distinguish between US tanks and Russian tanks, but rather just\\nbetween clear photos and blurry photos. In this case, there was a bias\\nin the training data (due to how the training data was collected) that\\ncaused the learning algorithm to learn something other than what we\\nwere hoping for. We will return to this issue in Chapter 8; for now,\\nsimply remember that the distribution Dfor training data must match\\nthe distribution Dfor the test data.\\n1.5Chapter Summary and Outlook\\nAt this point, you should be able to use decision trees to do machine',\n",
              " 'the distribution Dfor the test data.\\n1.5Chapter Summary and Outlook\\nAt this point, you should be able to use decision trees to do machine\\nlearning. Someone will give you data. You’ll split it into training,\\ndevelopment and test portions. Using the training and development\\ndata, you’ll ﬁnd a good value for maximum depth that trades off\\nbetween underﬁtting and overﬁtting. You’ll then run the resulting\\ndecision tree model on the test data to get an estimate of how well\\nyou are likely to do in the future.\\nYou might think: why should I read the rest of this book? Aside\\nfrom the fact that machine learning is just an awesome fun ﬁeld to\\nlearn about, there’s a lot left to cover. In the next two chapters, you’ll\\nlearn about two models that have very different inductive biases than\\ndecision trees. You’ll also get to see a very useful way of thinking\\nabout learning: the geometric view of data. This will guide much of\\nwhat follows. After that, you’ll learn how to solve problems more',\n",
              " 'about learning: the geometric view of data. This will guide much of\\nwhat follows. After that, you’ll learn how to solve problems more\\ncomplicated that simple binary classiﬁcation. (Machine learning\\npeople like binary classiﬁcation a lot because it’s one of the simplest\\nnon-trivial problems that we can work on.) After that, things will\\ndiverge: you’ll learn about ways to think about learning as a formal\\noptimization problem, ways to speed up learning, ways to learn18 a course in machine learning\\nwithout labeled data (or with very little labeled data) and all sorts of\\nother fun topics.\\nBut throughout, we will focus on the view of machine learning\\nthat you’ve seen here. You select a model (and its associated induc-\\ntive biases). You use data to ﬁnd parameters of that model that work\\nwell on the training data. You use development data to avoid under-\\nﬁtting and overﬁtting. And you use test data (which you’ll never look\\nat or touch, right?) to estimate future model performance. Then you',\n",
              " 'ﬁtting and overﬁtting. And you use test data (which you’ll never look\\nat or touch, right?) to estimate future model performance. Then you\\nconquer the world.\\n1.6Further Reading\\nIn our discussion of decision trees, we used misclassiﬁcation rate for\\nselecting features. While simple and intuitive, misclassiﬁcation rate\\nhas problems. There has been a signiﬁcant amount of work that\\nconsiders more advanced splitting criteria; the most popular is ID 3,\\nbased on the mutual information quantity from information the-\\nory. We have also only considered a very simple mechanism for\\ncontrolling inductive bias: limiting the depth of the decision tree.\\nAgain, there are more advanced “tree pruning” techniques that typ-\\nically operate by growing deep trees and then pruning back some\\nof the branches. These approaches have the advantage that differ-\\nent branches can have different depths, accounting for the fact that\\nthe amount of data that gets passed down each branch might differ',\n",
              " 'of the branches. These approaches have the advantage that differ-\\nent branches can have different depths, accounting for the fact that\\nthe amount of data that gets passed down each branch might differ\\ndramatically3.3Quinlan 19862 | L IMITS OF LEARNING\\nDependencies: None.Machine learning is a very general and useful framework,\\nbut it is not “magic” and will not always work. In order to better\\nunderstand when it will and when it will not work, it is useful to\\nformalize the learning problem more. This will also help us develop\\ndebugging strategies for learning algorithms.\\n2.1Data Generating Distributions\\nOur underlying assumption for the majority of this book is that\\nlearning problems are characterized by some unknown probability\\ndistributionDover input/output pairs (x,y)2X\\x02Y . Suppose that\\nsomeone toldyou whatDwas. In particular, they gave you a Python\\nfunction compute D that took two inputs, xand y, and returned the\\nprobability of that x,ypair underD. If you had access to such a func-',\n",
              " 'someone toldyou whatDwas. In particular, they gave you a Python\\nfunction compute D that took two inputs, xand y, and returned the\\nprobability of that x,ypair underD. If you had access to such a func-\\ntion, classiﬁcation becomes simple. We can deﬁne the Bayes optimal\\nclassiﬁeras the classiﬁer that, for any test input ˆx, simply returns the\\nˆythat maximizes compute D(ˆx,ˆy), or, more formally:\\nf(BO)(ˆx) =arg max\\nˆy2YD(ˆx,ˆy) (2.1)\\nThis classiﬁer is optimal in one speciﬁc sense: of all possible classiﬁers,\\nit achieves the smallest zero/one error.\\nTheorem 1(Bayes Optimal Classiﬁer) .The Bayes Optimal Classiﬁer\\nf(BO)achieves minimal zero/one error of any deterministic classiﬁer.\\nThis theorem assumes that you are comparing against deterministic\\nclassiﬁers. You can actually prove a stronger result that f(BO)is opti-\\nmal for randomized classiﬁers as well, but the proof is a bit messier.\\nHowever, the intuition is the same: for a given x,f(BO)chooses the',\n",
              " 'mal for randomized classiﬁers as well, but the proof is a bit messier.\\nHowever, the intuition is the same: for a given x,f(BO)chooses the\\nlabel with highest probability, thus minimizing the probability that it\\nmakes an error.\\nProof of Theorem 1.Consider some other classiﬁer gthat claims to\\nbe better than f(BO). Then, there must be some xon which g(x)6=Learning Objectives:\\n• Deﬁne “inductive bias” and recog-\\nnize the role of inductive bias in\\nlearning.\\n• Illustrate how regularization trades\\noff between underﬁtting and overﬁt-\\nting.\\n• Evaluate whether a use of test data\\nis “cheating” or not.Our lives sometimes depend on computers performing as pre-\\ndicted. – Philip Emeagwali20 a course in machine learning\\nf(BO)(x). Fix such an x. Now, the probability that f(BO)makes an error\\non this particular xis 1\\x00D(x,f(BO)(x))and the probability that g\\nmakes an error on this xis 1\\x00D(x,g(x)). But f(BO)was chosen in\\nsuch a way to maximizeD(x,f(BO)(x)), so this must be greater than',\n",
              " 'on this particular xis 1\\x00D(x,f(BO)(x))and the probability that g\\nmakes an error on this xis 1\\x00D(x,g(x)). But f(BO)was chosen in\\nsuch a way to maximizeD(x,f(BO)(x)), so this must be greater than\\nD(x,g(x)). Thus, the probability that f(BO)errs on this particular xis\\nsmaller than the probability that gerrs on it. This applies to any xfor\\nwhich f(BO)(x)6=g(x)and therefore f(BO)achieves smaller zero/one\\nerror than any g.\\nThe Bayes errorrate (orBayes optimal errorrate) is the error\\nrate of the Bayes optimal classiﬁer. It is the best error rate you can\\never hope to achieve on this classiﬁcation problem (under zero/one\\nloss). The take-home message is that if someone gave you access to\\nthe data distribution, forming an optimal classiﬁer would be trivial.\\nUnfortunately, no one gave you this distribution, so we need to ﬁgure\\nout ways of learning the mapping from xtoygiven only access to a\\ntraining set sampled fromD, rather thanDitself.\\n2.2Inductive Bias: What We Know Before the Data Arrives',\n",
              " 'out ways of learning the mapping from xtoygiven only access to a\\ntraining set sampled fromD, rather thanDitself.\\n2.2Inductive Bias: What We Know Before the Data Arrives\\nclass A\\nclass B\\nFigure 2.1: Training data for a binary\\nclassiﬁcation problem.\\nFigure 2.2: Test data for the same\\nclassiﬁcation problem.In Figure 2.1you’ll ﬁnd training data for a binary classiﬁcation prob-\\nlem. The two labels are “A” and “B” and you can see four examples\\nfor each label. Below, in Figure 2.2, you will see some test data. These\\nimages are left unlabeled. Go through quickly and, based on the\\ntraining data, label these images. (Really do it before you read fur-\\nther! I’ll wait!)\\nMost likely you produced one of two labelings: either ABBA or\\nAABB. Which of these solutions is right? The answer is that you can-\\nnot tell based on the training data. If you give this same example\\nto 100 people, 60\\x0070 of them come up with the ABBA prediction\\nand 30\\x0040 come up with the AABB prediction. Why? Presumably',\n",
              " 'not tell based on the training data. If you give this same example\\nto 100 people, 60\\x0070 of them come up with the ABBA prediction\\nand 30\\x0040 come up with the AABB prediction. Why? Presumably\\nbecause the ﬁrst group believes that the relevant distinction is be-\\ntween “bird” and “non-bird” while the second group believes that\\nthe relevant distinction is between “ﬂy” and “no-ﬂy.”\\nThis preference for one distinction (bird/non-bird) over another\\n(ﬂy/no-ﬂy) is a bias that different human learners have. In the con-\\ntext of machine learning, it is called inductive bias : in the absense of\\ndata that narrow down the relevant concept, what type of solutions\\nare we more likely to prefer? Two thirds of people seem to have an\\ninductive bias in favor of bird/non-bird, and one third seem to have\\nan inductive bias in favor of ﬂy/no-ﬂy.\\nIt is also possible that the correct\\nclassiﬁcation on the test data is\\nABAB. This corresponds to the bias\\n“is the background in focus.” Some-',\n",
              " 'an inductive bias in favor of ﬂy/no-ﬂy.\\nIt is also possible that the correct\\nclassiﬁcation on the test data is\\nABAB. This corresponds to the bias\\n“is the background in focus.” Some-\\nhow no one seems to come up with\\nthis classiﬁcation rule.?Throughout this book you will learn about several approaches to\\nmachine learning. The decision tree model is the ﬁrst such approach.\\nThese approaches differ primarily in the sort of inductive bias thatlimits of learning 21\\nthey exhibit.\\nConsider a variant of the decision tree learning algorithm. In this\\nvariant, we will not allow the trees to grow beyond some pre-deﬁned\\nmaximum depth, d. That is, once we have queried on d-many fea-\\ntures, we cannot query on any more and must just make the best\\nguess we can at that point. This variant is called a shal low decision\\ntree.\\nThe key question is: What is the inductive bias of shallow decision\\ntrees? Roughly, their bias is that decisions can be made by only look-',\n",
              " 'tree.\\nThe key question is: What is the inductive bias of shallow decision\\ntrees? Roughly, their bias is that decisions can be made by only look-\\ning at a small number of features. For instance, a shallow decision\\ntree would be very good at learning a function like “students only\\nlike AI courses.” It would be very bad at learning a function like “if\\nthis student has liked an odd number of their past courses, they will\\nlike the next one; otherwise they will not.” This latter is the parity\\nfunction, which requires you to inspect every feature to make a pre-\\ndiction. The inductive bias of a decision tree is that the sorts of things\\nwe want to learn to predict are more like the ﬁrst example and less\\nlike the second example.\\n2.3Not Everything is Learnable\\nAlthough machine learning works well—perhaps astonishingly\\nwell—in many cases, it is important to keep in mind that it is not\\nmagical. There are many reasons why a machine learning algorithm\\nmight fail on some learning task.',\n",
              " 'well—in many cases, it is important to keep in mind that it is not\\nmagical. There are many reasons why a machine learning algorithm\\nmight fail on some learning task.\\nThere could be noise in the training data. Noise can occur both\\nat the feature level and at the label level. Some features might corre-\\nspond to measurements taken by sensors. For instance, a robot might\\nuse a laser range ﬁnder to compute its distance to a wall. However,\\nthis sensor might fail and return an incorrect value. In a sentiment\\nclassiﬁcation problem, someone might have a typo in their review of\\na course. These would lead to noise at the feature level. There might\\nalso be noise at the label level. A student might write a scathingly\\nnegative review of a course, but then accidentally click the wrong\\nbutton for the course rating.\\nThe features available for learning might simply be insufﬁcient.\\nFor example, in a medical context, you might wish to diagnose',\n",
              " 'button for the course rating.\\nThe features available for learning might simply be insufﬁcient.\\nFor example, in a medical context, you might wish to diagnose\\nwhether a patient has cancer or not. You may be able to collect a\\nlarge amount of data about this patient, such as gene expressions,\\nX-rays, family histories, etc. But, even knowing all of this information\\nexactly, it might still be impossible to judge for sure whether this pa-\\ntient has cancer or not. As a more contrived example, you might try\\nto classify course reviews as positive or negative. But you may have\\nerred when downloading the data and only gotten the ﬁrst ﬁve char-22 a course in machine learning\\nacters of each review. If you had the rest of the features you might\\nbe able to do well. But with this limited feature set, there’s not much\\nyou can do.\\nSome examples may not have a single correct answer. You might\\nbe building a system for “safe web search,” which removes offen-']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the embedded chuncks of data to the vector store\n",
        "astra_vector_store.add_texts(texts)\n",
        "print(\"Inserted %i chuncks.\" % len(texts))\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4xCPJpeuLd8",
        "outputId": "145cefd2-87f3-431b-f4df-310fbee8e179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 642 chuncks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chatbot to fetch the responses from the pdf for a user query\n",
        "first_question = True\n",
        "while True:\n",
        "  if first_question:\n",
        "    query = input(\"Ask a question about the book (Or type 'quit' to exit):\").strip()\n",
        "  else:\n",
        "    query = input(\"Ask a follow up question:\").strip()\n",
        "  if query.lower() == \"quit\":\n",
        "    break\n",
        "  if query == \"\":\n",
        "    continue\n",
        "  first_question = False\n",
        "  print(\"\\nQuestion: %s\" % query)\n",
        "  answer = astra_vector_index.query(query, llm=llm)\n",
        "  print(\"\\nAnswer: %s\" % answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okZClR7IuyoJ",
        "outputId": "770655f6-b718-4749-8555-1c0f2f0012af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question about the book (Or type 'quit' to exit):machine learning\n",
            "\n",
            "Question: machine learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: Machine learning is a broad and fascinating field that involves using algorithms and statistical models to enable computers to learn from and make predictions or decisions based on data without being explicitly programmed. It plays a significant role in various aspects of our lives, often without us realizing it. The field raises philosophical questions about learning and success in tasks, and it is essential for any approach to artificial intelligence.\n",
            "Ask a follow up question:random forest\n",
            "\n",
            "Question: random forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer: Random forests are a type of ensemble learning method that consists of multiple decision trees. Each tree is built independently with random features, and the final prediction is made by aggregating the predictions of all the trees (voting). This method is efficient and effective, especially for classification and regression tasks.\n",
            "Ask a follow up question:quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuHOnx-olM-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}